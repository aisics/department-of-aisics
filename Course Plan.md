# Структура курсу лекцій з вивчення штучного інтелекту

## Частина 1: Математичні основи штучного інтелекту

### Лекція 1: [Вступ до курсу та основи лінійної алгебри - Вектори](Module 01/Вступ до курсу та основи лінійної алгебри - Вектори.md)
* Що таке штучний інтелект (ШІ)? Історія розвитку ШІ.
* Поняття вектора, його геометрична інтерпретація.
* Операції над векторами: додавання, віднімання, скалярне множення.
* Довжина (норма) вектора.

### Лекція 2: [Основи лінійної алгебри - Матриці та визначники](Module 02/Основи лінійної алгебри - Матриці та визначники.md)
* Поняття матриці, її розмірність.
* Операції над матрицями: додавання, віднімання, множення на скаляр, матричне множення.
* Транспонування матриці.
* Спеціальні типи матриць (квадратні, діагональні, одиничні, симетричні).
* Визначник матриці (для 2x2 та 3x3 матриць).
* Властивості визначників.

### Лекція 3: [Основи лінійної алгебри - Обернені матриці та системи рівнянь](Module 03/Основи лінійної алгебри - Обернені матриці та системи рівнянь.md)
* Обернена матриця та умови її існування.
* Розв'язування систем лінійних рівнянь за допомогою матриць (метод Крамера, матричний метод).
* Лінійна залежність та незалежність векторів.
* Власні значення та власні вектори (основи).

### Лекція 4: [Основи математичного аналізу - Функції та похідні](Module 04/Основи математичного аналізу - Функції та похідні.md)
* Поняття функції однієї та багатьох змінних, область визначення та область значень.
* Графіки функцій.
* Поняття похідної, її геометричний та фізичний зміст.
* Правила диференціювання (степенева, суми, добутку, частки, складної функції).

### Лекція 5: [Основи математичного аналізу - Частинні похідні та оптимізація](Module 05/Основи математичного аналізу - Частинні похідні та оптимізація.md)
* Частинні похідні першого та вищих порядків.
* Градієнт функції, його геометричний зміст (напрямок найшвидшого зростання).
* Матриця Гессе.
* Знаходження екстремумів функцій однієї та багатьох змінних (основи).

### Лекція 6: [Основи теорії ймовірностей](Module 06/Основи теорії ймовірностей.md)
* Основні поняття теорії ймовірностей (випадкова подія, ймовірність, простір подій).
* Аксіоми ймовірності.
* Умовна ймовірність.
* Формула Баєса.
* Незалежні події.

### Лекція 7: [Випадкові величини та основні розподіли](Module 07/Випадкові величини та основні розподіли.md)
* Випадкові величини (дискретні та неперервні).
* Функція розподілу ймовірностей, щільність ймовірності.
* Математичне сподівання та дисперсія.
* Основні розподіли (Бернуллі, біноміальний, нормальний).

## Частина 2: Основи машинного навчання та глибокого навчання

### Лекція 8: [Вступ до машинного навчання та лінійні моделі (регресія)](Module 08/Вступ до машинного навчання та лінійні моделі (регресія).md)
* Основні парадигми машинного навчання: навчання з учителем, без учителя, з підкріпленням.
* Процес розробки моделі машинного навчання.
* Лінійна регресія: гіпотеза, функція втрат (MSE), метод найменших квадратів.
* Градієнтний спуск як метод оптимізації (основи).

### Лекція 8.1: [Навчання без учителя (детальніше)](Module 08/Навчання без учителя (детальніше).md)
* Кластеризація (K-Means), Зменшення розмірності (PCA - Principal Component Analysis - детальніше застосування та інтерпретація).

### Лекція 9: [Лінійні моделі (класифікація) та оцінка моделей](Module 09/Лінійні моделі (класифікація) та оцінка моделей.md)
* Логістична регресія: бінарна та мультикласова класифікація, сигмоїдна функція, функція втрат (cross-entropy).
* Оцінка моделей класифікації (accuracy, precision, recall, F1-score, AUC).
* Проблема перенавчання та недонавчання.
* Розділення даних на навчальну, валідаційну та тестову вибірки.

### Лекція 9.1: [Класичні алгоритми машинного навчання](Module 09/Класичні алгоритми машинного навчання.md)
* Дерева рішень (Decision Trees), Випадкові ліси (Random Forests), Градієнтний бустинг (Gradient Boosting - оглядово)

### Лекція 9.2: [Інші методи навчання з учителем](Module 09/Інші методи навчання з учителем.md)
* Метод K-найближчих сусідів (K-Nearest Neighbors - KNN), Метод опорних векторів (Support Vector Machines - SVM) - основи.

### Лекція 9.3: [Практичні аспекти побудови моделей](Module 09/Практичні аспекти побудови моделей.md)
* Інженерія ознак (Feature Engineering), Відбір ознак (Feature Selection), Крос-валідація (Cross-Validation), Налаштування гіперпараметрів (Grid Search, Random Search).

### Лекція 10: [Нейронні мережі - Основи перцептрону та багатошарової мережі](Module 10/Нейронні мережі - Основи перцептрону та багатошарової мережі.md)
* Біологічний нейрон та штучний нейрон (перцептрон).
* Функції активації (сигмоїдна, ReLU, tanh та інші).
* Архітектура багатошарової нейронної мережі (MLP).
* Пряме поширення (forward propagation).

### Лекція 11: [Нейронні мережі - Навчання (Backpropagation) та оптимізатори](Module 11/Нейронні мережі - Навчання (Backpropagation) та оптимізатори.md)
* Функція втрат для нейронних мереж.
* Алгоритм зворотного поширення помилки (backpropagation) - інтуїтивне пояснення.
* Оптимізатори (SGD, Adam, RMSprop) - основні ідеї.
* Регуляризація в нейронних мережах (L1, L2, dropout).

### Лекція 12: [Глибоке навчання - Згорткові нейронні мережі (CNN) - Основи](Module 12/Глибоке навчання - Згорткові нейронні мережі (CNN) - Основи.md)
* Згорткові шари (convolutional layers), фільтри, stride, padding.
* Операція згортки та її інтуїтивне розуміння.
* Шари пулінгу (pooling layers).

### Лекція 13: [Глибоке навчання - Згорткові нейронні мережі (CNN) - Архітектури та застосування](Module 13/Глибоке навчання - Згорткові нейронні мережі (CNN) - Архітектури та застосування.md)
* Архітектура CNN для задач комп'ютерного зору (огляд LeNet, AlexNet, VGG).
* Застосування CNN для класифікації та розпізнавання образів.
* Трансферне навчання (transfer learning) у комп'ютерному зорі (основи).

### Лекція 14: [Глибоке навчання - Рекурентні нейронні мережі (RNN) - Основи](Module 14/Глибоке навчання - Рекурентні нейронні мережі (RNN) - Основи.md)
* Послідовні дані та їх особливості.
* Архітектура RNN, проблема зникаючого та зростаючого градієнта.
* Основні типи RNN.

### Лекція 15: [Глибоке навчання - Рекурентні нейронні мережі (RNN) - LSTM та GRU](Module 15/Глибоке навчання - Рекурентні нейронні мережі (RNN) - LSTM та GRU.md)
* Довга короткочасна пам'ять (LSTM) - архітектура та принцип роботи.
* Вентильований рекурентний блок (GRU) - архітектура та принцип роботи.
* Застосування RNN для обробки послідовностей (основи).

## Частина 3: Застосування штучного інтелекту

### Лекція 16: [Комп'ютерний зір (Computer Vision) - Задачі та сучасні архітектури](Module 16/Комп'ютерний зір (Computer Vision) - Задачі та сучасні архітектури.md)
* Основні задачі комп'ютерного зору (класифікація, локалізація, детекція об'єктів, сегментація).
* Сучасні архітектури CNN для комп'ютерного зору (огляд ResNet, Inception).
* Обгорткові мережі для детекції об'єктів (основи YOLO, Faster R-CNN).

### Лекція 17: [Обробка природної мови (Natural Language Processing - NLP) - Векторне представлення слів](Module 17/Обробка природної мови (Natural Language Processing - NLP) - Векторне представлення слів.md)
* Основні задачі NLP (класифікація тексту, аналіз тональності, машинний переклад, генерування тексту, відповіді на запитання).
* Векторне представлення слів (word embeddings: Word2Vec, GloVe) - принципи та застосування.

### Лекція 18: [Обробка природної мови (Natural Language Processing - NLP) - Трансформери та увага](Module 18/Обробка природної мови (Natural Language Processing - NLP) - Трансформери та увага.md)
* Трансформерні мережі (Transformer networks) - основна архітектура.
* Механізм уваги (attention mechanism) - принцип роботи та значення.
* Застосування трансформерів в NLP.

### Лекція 19: [Великі мовні моделі (Large Language Models - LLM)](Module 19/Великі мовні моделі (Large Language Models - LLM).md)
* Архітектура Transformer для LLM.
* Попереднє навчання (pre-training) та донавчання (fine-tuning) LLM.
* Приклади використання LLM (чат-боти, написання текстів, переклад).

### Лекція 19.1: [Вступ до генеративних моделей (окрім LLM)](Module 19/Вступ до генеративних моделей (окрім LLM).md)
* Генеративні змагальні мережі (Generative Adversarial Networks - GANs) - основи, Варіаційні автокодувальники (Variational Autoencoders - VAEs) - основи.

### Лекція 20: [Етичні та соціальні аспекти штучного інтелекту та майбутнє ШІ (огляд)](Module 20/Етичні та соціальні аспекти штучного інтелекту та майбутнє ШІ (огляд).md)
* Упередження в даних та моделях.
* Питання справедливості та прозорості ШІ.
* Поточні тенденції та майбутні напрямки розвитку ШІ.
* Ресурси для подальшого вивчення ШІ.