# Структура курсу лекцій з вивчення штучного інтелекту

## Частина 1: Математичні основи штучного інтелекту

### Лекція 1: Вступ до курсу та основи лінійної алгебри - Вектори
* Що таке штучний інтелект (ШІ)? Історія розвитку ШІ.
* Поняття вектора, його геометрична інтерпретація.
* Операції над векторами: додавання, віднімання, скалярне множення.
* Довжина (норма) вектора.

### Лекція 2: Основи лінійної алгебри - Матриці та визначники
* Поняття матриці, її розмірність.
* Операції над матрицями: додавання, віднімання, множення на скаляр, матричне множення.
* Транспонування матриці.
* Спеціальні типи матриць (квадратні, діагональні, одиничні, симетричні).
* Визначник матриці (для 2x2 та 3x3 матриць).
* Властивості визначників.

### Лекція 3: Основи лінійної алгебри - Обернені матриці та системи рівнянь
* Обернена матриця та умови її існування.
* Розв'язування систем лінійних рівнянь за допомогою матриць (метод Крамера, матричний метод).
* Лінійна залежність та незалежність векторів.
* Власні значення та власні вектори (основи).

### Лекція 4: Основи математичного аналізу - Функції та похідні
* Поняття функції однієї та багатьох змінних, область визначення та область значень.
* Графіки функцій.
* Поняття похідної, її геометричний та фізичний зміст.
* Правила диференціювання (степенева, суми, добутку, частки, складної функції).

### Лекція 5: Основи математичного аналізу - Частинні похідні та оптимізація
* Частинні похідні першого та вищих порядків.
* Градієнт функції, його геометричний зміст (напрямок найшвидшого зростання).
* Матриця Гессе.
* Знаходження екстремумів функцій однієї та багатьох змінних (основи).

### Лекція 6: Основи теорії ймовірностей
* Основні поняття теорії ймовірностей (випадкова подія, ймовірність, простір подій).
* Аксіоми ймовірності.
* Умовна ймовірність.
* Формула Баєса.
* Незалежні події.

### Лекція 7: Випадкові величини та основні розподіли
* Випадкові величини (дискретні та неперервні).
* Функція розподілу ймовірностей, щільність ймовірності.
* Математичне сподівання та дисперсія.
* Основні розподіли (Бернуллі, біноміальний, нормальний).

## Частина 2: Основи машинного навчання та глибокого навчання

### Лекція 8: Вступ до машинного навчання та лінійні моделі (регресія)
* Основні парадигми машинного навчання: навчання з учителем, без учителя, з підкріпленням.
* Процес розробки моделі машинного навчання.
* Лінійна регресія: гіпотеза, функція втрат (MSE), метод найменших квадратів.
* Градієнтний спуск як метод оптимізації (основи).

### Лекція 8.1: Навчання без учителя (детальніше)
* Кластеризація (K-Means), Зменшення розмірності (PCA - Principal Component Analysis - детальніше застосування та інтерпретація).

### Лекція 9: Лінійні моделі (класифікація) та оцінка моделей
* Логістична регресія: бінарна та мультикласова класифікація, сигмоїдна функція, функція втрат (cross-entropy).
* Оцінка моделей класифікації (accuracy, precision, recall, F1-score, AUC).
* Проблема перенавчання та недонавчання.
* Розділення даних на навчальну, валідаційну та тестову вибірки.

### Лекція 9.1: Класичні алгоритми машинного навчання
* Дерева рішень (Decision Trees), Випадкові ліси (Random Forests), Градієнтний бустинг (Gradient Boosting - оглядово)

### Лекція 9.2: Інші методи навчання з учителем
* Метод K-найближчих сусідів (K-Nearest Neighbors - KNN), Метод опорних векторів (Support Vector Machines - SVM) - основи.

### Лекція 9.3: Практичні аспекти побудови моделей
* Інженерія ознак (Feature Engineering), Відбір ознак (Feature Selection), Крос-валідація (Cross-Validation), Налаштування гіперпараметрів (Grid Search, Random Search).

### Лекція 10: Нейронні мережі - Основи перцептрону та багатошарової мережі
* Біологічний нейрон та штучний нейрон (перцептрон).
* Функції активації (сигмоїдна, ReLU, tanh та інші).
* Архітектура багатошарової нейронної мережі (MLP).
* Пряме поширення (forward propagation).

### Лекція 11: Нейронні мережі - Навчання (Backpropagation) та оптимізатори
* Функція втрат для нейронних мереж.
* Алгоритм зворотного поширення помилки (backpropagation) - інтуїтивне пояснення.
* Оптимізатори (SGD, Adam, RMSprop) - основні ідеї.
* Регуляризація в нейронних мережах (L1, L2, dropout).

### Лекція 12: Глибоке навчання - Згорткові нейронні мережі (CNN) - Основи
* Згорткові шари (convolutional layers), фільтри, stride, padding.
* Операція згортки та її інтуїтивне розуміння.
* Шари пулінгу (pooling layers).

### Лекція 13: Глибоке навчання - Згорткові нейронні мережі (CNN) - Архітектури та застосування
* Архітектура CNN для задач комп'ютерного зору (огляд LeNet, AlexNet, VGG).
* Застосування CNN для класифікації та розпізнавання образів.
* Трансферне навчання (transfer learning) у комп'ютерному зорі (основи).

### Лекція 14: Глибоке навчання - Рекурентні нейронні мережі (RNN) - Основи
* Послідовні дані та їх особливості.
* Архітектура RNN, проблема зникаючого та зростаючого градієнта.
* Основні типи RNN.

### Лекція 15: Глибоке навчання - Рекурентні нейронні мережі (RNN) - LSTM та GRU
* Довга короткочасна пам'ять (LSTM) - архітектура та принцип роботи.
* Вентильований рекурентний блок (GRU) - архітектура та принцип роботи.
* Застосування RNN для обробки послідовностей (основи).

## Частина 3: Застосування штучного інтелекту

### Лекція 16: Комп'ютерний зір (Computer Vision) - Задачі та сучасні архітектури
* Основні задачі комп'ютерного зору (класифікація, локалізація, детекція об'єктів, сегментація).
* Сучасні архітектури CNN для комп'ютерного зору (огляд ResNet, Inception).
* Обгорткові мережі для детекції об'єктів (основи YOLO, Faster R-CNN).

### Лекція 17: Обробка природної мови (Natural Language Processing - NLP) - Векторне представлення слів
* Основні задачі NLP (класифікація тексту, аналіз тональності, машинний переклад, генерування тексту, відповіді на запитання).
* Векторне представлення слів (word embeddings: Word2Vec, GloVe) - принципи та застосування.

### Лекція 18: Обробка природної мови (Natural Language Processing - NLP) - Трансформери та увага
* Трансформерні мережі (Transformer networks) - основна архітектура.
* Механізм уваги (attention mechanism) - принцип роботи та значення.
* Застосування трансформерів в NLP.

### Лекція 19: Великі мовні моделі (Large Language Models - LLM)
* Архітектура Transformer для LLM.
* Попереднє навчання (pre-training) та донавчання (fine-tuning) LLM.
* Приклади використання LLM (чат-боти, написання текстів, переклад).

### Лекція 19.1: Вступ до генеративних моделей (окрім LLM)
* Генеративні змагальні мережі (Generative Adversarial Networks - GANs) - основи, Варіаційні автокодувальники (Variational Autoencoders - VAEs) - основи.

### Лекція 20: Етичні та соціальні аспекти штучного інтелекту та майбутнє ШІ (огляд)
* Упередження в даних та моделях.
* Питання справедливості та прозорості ШІ.
* Поточні тенденції та майбутні напрямки розвитку ШІ.
* Ресурси для подальшого вивчення ШІ.