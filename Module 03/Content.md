**Лекція 3: Основи лінійної алгебри - Обернені матриці та системи рівнянь**

**Мета лекції:** Ознайомити слухачів з поняттям оберненої матриці, методами розв'язання систем лінійних рівнянь за допомогою матриць, концепцією лінійної залежності/незалежності векторів та основами власних значень і векторів. Показати зв'язок цих понять із задачами штучного інтелекту.

---

Доброго дня! Сьогодні ми продовжимо вивчати лінійну алгебру – математичний інструментарій, без якого неможливо уявити сучасний штучний інтелект. Ми розглянемо:

1.  **Обернена матриця:** Що це таке і коли вона існує?
2.  **Системи лінійних рівнянь:** Як матриці допомагають їх розв'язувати? (Матричний метод, метод Крамера)
3.  **Лінійна (не)залежність векторів:** Фундаментальна властивість векторів.
4.  **Власні значення та вектори:** Вступ до важливої концепції.

**Навіщо це потрібно в AI?**

Лінійна алгебра в AI – це мова опису даних та операцій над ними. Сьогоднішні теми важливі, бо вони допомагають:

* **Розв'язувати задачі оптимізації:** Знаходження найкращих параметрів для моделей машинного навчання часто зводиться до розв'язання систем рівнянь.
* **Аналізувати дані:** Поняття лінійної залежності допомагає виявляти надлишкові ознаки (фічі) в даних.
* **Зменшувати розмірність даних:** Власні значення та вектори лежать в основі таких методів, як аналіз головних компонент (PCA), що дозволяє спрощувати складні дані без втрати важливої інформації.
* **Розуміти роботу алгоритмів:** Багато алгоритмів (наприклад, лінійна регресія) напряму використовують матричні операції, включаючи обернення матриць.

---

**Обернена матриця**

* **Що це?**
    Уявіть, що у вас є число 5. Обернене до нього число (у множенні) - це $1/5$, бо $5 \times (1/5) = 1$. Для матриць схожа ідея. **Обернена матриця** до квадратної матриці $A$ – це така матриця $A^{-1}$, що їхній добуток дорівнює **одиничній матриці** $I$ (матриця з одиницями на діагоналі та нулями поза нею). Формально: $A \cdot A^{-1} = A^{-1} \cdot A = I$.

* **Умови існування:**
    Матриця $A$ **має бути квадратною** (кількість рядків = кількість стовпців). **Визначник (детермінант)** матриці $A$, який позначається як $det(A)$ або $|A|$, **не повинен дорівнювати нулю** ($det(A) \neq 0$). Такі матриці називаються **невиродженими** або **несингулярними**.

* **Приклад (2x2):**
    Нехай $A = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$.
    Визначник: $det(A) = ad - bc$.
    Якщо $det(A) \neq 0$, то обернена матриця існує і дорівнює:
    $A^{-1} = \frac{1}{det(A)} \begin{pmatrix} d & -b \\ -c & a \end{pmatrix}$.
    *Конкретний приклад:*
    $A = \begin{pmatrix} 4 & 7 \\ 2 & 6 \end{pmatrix}$
    $det(A) = 4 \times 6 - 7 \times 2 = 24 - 14 = 10 \neq 0$.
    $A^{-1} = \frac{1}{10} \begin{pmatrix} 6 & -7 \\ -2 & 4 \end{pmatrix} = \begin{pmatrix} 0.6 & -0.7 \\ -0.2 & 0.4 \end{pmatrix}$.
    *Перевірка:* $A \cdot A^{-1} = \begin{pmatrix} 4 & 7 \\ 2 & 6 \end{pmatrix} \begin{pmatrix} 0.6 & -0.7 \\ -0.2 & 0.4 \end{pmatrix} = \begin{pmatrix} (2.4-1.4) & (-2.8+2.8) \\ (1.2-1.2) & (-1.4+2.4) \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} = I$.

---

**Розв'язування систем лінійних рівнянь**

* **Запис у матричному вигляді:**
    Систему лінійних рівнянь, наприклад:
    $$\begin{cases} a_{11}x_1 + a_{12}x_2 = b_1 \\ a_{21}x_1 + a_{22}x_2 = b_2 \end{cases}$$
    Можна записати у вигляді матричного рівняння $A X = B$, де:
    $A = \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix}$ (матриця коефіцієнтів)
    $X = \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}$ (вектор невідомих)
    $B = \begin{pmatrix} b_1 \\ b_2 \end{pmatrix}$ (вектор вільних членів)

* **Метод 1: Матричний метод (з використанням оберненої матриці)**
    Якщо матриця $A$ квадратна і $det(A) \neq 0$, то існує $A^{-1}$. Домножимо рівняння $A X = B$ зліва на $A^{-1}$:
    $A^{-1} (A X) = A^{-1} B \implies (A^{-1} A) X = A^{-1} B \implies I X = A^{-1} B \implies X = A^{-1} B$.
    **Алгоритм:**
    1.  Записати систему у вигляді $AX=B$.
    2.  Знайти $A^{-1}$.
    3.  Обчислити добуток $A^{-1} B$. Результат - це вектор $X$ з розв'язками.
    *Приклад:* Розв'яжемо систему $\begin{cases} 4x + 7y = 1 \\ 2x + 6y = 3 \end{cases}$
    $A = \begin{pmatrix} 4 & 7 \\ 2 & 6 \end{pmatrix}$, $X = \begin{pmatrix} x \\ y \end{pmatrix}$, $B = \begin{pmatrix} 1 \\ 3 \end{pmatrix}$.
    Ми вже знайшли $A^{-1} = \begin{pmatrix} 0.6 & -0.7 \\ -0.2 & 0.4 \end{pmatrix}$.
    $X = A^{-1} B = \begin{pmatrix} 0.6 & -0.7 \\ -0.2 & 0.4 \end{pmatrix} \begin{pmatrix} 1 \\ 3 \end{pmatrix} = \begin{pmatrix} 0.6 \times 1 + (-0.7) \times 3 \\ -0.2 \times 1 + 0.4 \times 3 \end{pmatrix} = \begin{pmatrix} 0.6 - 2.1 \\ -0.2 + 1.2 \end{pmatrix} = \begin{pmatrix} -1.5 \\ 1 \end{pmatrix}$.
    Отже, $x = -1.5$, $y = 1$.

* **Метод 2: Метод Крамера**
    Також вимагає $det(A) \neq 0$. Розв'язок для кожної змінної $x_i$ знаходиться за формулою:
    $x_i = \frac{det(A_i)}{det(A)}$
    Де $A_i$ – це матриця $A$, у якій $i$-й стовпець замінено стовпцем вільних членів $B$.
    *Приклад (та сама система):*
    $A = \begin{pmatrix} 4 & 7 \\ 2 & 6 \end{pmatrix}$, $B = \begin{pmatrix} 1 \\ 3 \end{pmatrix}$. Ми знаємо $det(A) = 10$.
    $A_1 = \begin{pmatrix} 1 & 7 \\ 3 & 6 \end{pmatrix} \implies det(A_1) = 1 \times 6 - 7 \times 3 = 6 - 21 = -15$.
    $A_2 = \begin{pmatrix} 4 & 1 \\ 2 & 3 \end{pmatrix} \implies det(A_2) = 4 \times 3 - 1 \times 2 = 12 - 2 = 10$.
    $x = x_1 = \frac{det(A_1)}{det(A)} = \frac{-15}{10} = -1.5$.
    $y = x_2 = \frac{det(A_2)}{det(A)} = \frac{10}{10} = 1$.
    Результат той самий.

* **Зауваження:** Матричний метод більш загальний і часто ефективніший для великих систем в обчисленнях. Метод Крамера зручний для малих систем (2x2, 3x3) і для теоретичних міркувань.

---

**Лінійна залежність та незалежність векторів**

* **Що це?**
    Набір векторів $v_1, v_2, ..., v_k$ називається **лінійно незалежним**, якщо єдиний спосіб отримати нульовий вектор як їхню лінійну комбінацію – це взяти всі коефіцієнти рівними нулю: $c_1 v_1 + c_2 v_2 + ... + c_k v_k = \vec{0} \implies c_1 = c_2 = ... = c_k = 0$. Інтуїтивно: жоден вектор не можна виразити через інші; вони вказують у "справді різні" боки.
    Набір векторів називається **лінійно залежним**, якщо існують такі коефіцієнти $c_1, ..., c_k$, **хоча б один з яких не нуль**, що їхня лінійна комбінація дорівнює нульовому вектору: $c_1 v_1 + c_2 v_2 + ... + c_k v_k = \vec{0}$ (принаймні один $c_i \neq 0$). Інтуїтивно: принаймні один вектор є "надлишковим", бо його можна виразити як комбінацію інших.

* **Приклади:**
    * **Незалежні:** Вектори $\vec{v}_1 = (1, 0)$ і $\vec{v}_2 = (0, 1)$ у 2D-просторі. $c_1(1, 0) + c_2(0, 1) = (c_1, c_2) = (0, 0)$. Це можливо лише коли $c_1 = 0$ і $c_2 = 0$.
    * **Залежні:** Вектори $\vec{u}_1 = (1, 2)$ і $\vec{u}_2 = (2, 4)$. Ми бачимо, що $\vec{u}_2 = 2 \cdot \vec{u}_1$. Отже, можна записати: $2 \cdot \vec{u}_1 - 1 \cdot \vec{u}_2 = 2(1, 2) - 1(2, 4) = (2, 4) - (2, 4) = (0, 0)$. Коефіцієнти $c_1=2$ і $c_2=-1$ не нульові. Ці вектори лежать на одній прямій.
    * **Залежні:** Вектори $\vec{w}_1=(1,0,0)$, $\vec{w}_2=(0,1,0)$, $\vec{w}_3=(1,1,0)$. Тут $\vec{w}_3 = \vec{w}_1 + \vec{w}_2$. Отже, $1 \cdot \vec{w}_1 + 1 \cdot \vec{w}_2 - 1 \cdot \vec{w}_3 = \vec{0}$. Коефіцієнти (1, 1, -1) не нульові.

* **Як перевірити?**
    Скласти матрицю, де стовпцями (або рядками) є ці вектори. Якщо матриця квадратна, обчислити її визначник. Якщо $det(A) \neq 0$, вектори лінійно незалежні. Якщо $det(A) = 0$, вони лінійно залежні. У загальному випадку використовують поняття **рангу матриці**. Якщо ранг дорівнює кількості векторів, вони незалежні.

* **Значення в AI:** Допомагає зрозуміти структуру даних. Якщо стовпці даних (ознаки) лінійно залежні, одна з ознак є надлишковою і може бути видалена без втрати інформації (feature selection).

---

**Власні значення та власні вектори (Основи)**

* **Ідея:** Коли матриця $A$ діє (множиться) на вектор $v$, результат $Av$ зазвичай є новим вектором, що вказує в іншому напрямку. Але існують спеціальні **ненульові** вектори $v$, для яких множення на матрицю $A$ діє просто як масштабування (розтягнення або стиснення) вздовж того ж напрямку: $A v = \lambda v$.

* **Визначення:**
    * $v$ – **власний вектор** матриці $A$ (eigenvector). Це ненульовий вектор, напрямок якого не змінюється при множенні на $A$.
    * $\lambda$ (лямбда) – **власне значення** матриці $A$ (eigenvalue). Це скалярний коефіцієнт, який показує, у скільки разів власний вектор $v$ розтягується або стискається під дією $A$.

* **Як їх знаходять (загальний принцип):**
    1.  Рівняння $A v = \lambda v$ переписують як $A v - \lambda I v = 0$, або $(A - \lambda I) v = 0$.
    2.  Це рівняння має ненульовий розв'язок $v$ тільки тоді, коли матриця $(A - \lambda I)$ є виродженою, тобто її визначник дорівнює нулю: $det(A - \lambda I) = 0$.
    3.  Це рівняння називається **характеристичним рівнянням**. Його розв'язки дають власні значення $\lambda$.
    4.  Для кожного знайденого $\lambda$ розв'язують систему $(A - \lambda I) v = 0$, щоб знайти відповідні власні вектори $v$.

* **Простий приклад:**
    Нехай $A = \begin{pmatrix} 3 & 0 \\ 0 & 2 \end{pmatrix}$ (діагональна матриця).
    * Спробуємо вектор $v_1 = (1, 0)$: $A v_1 = \begin{pmatrix} 3 & 0 \\ 0 & 2 \end{pmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 3 \\ 0 \end{pmatrix} = 3 \begin{pmatrix} 1 \\ 0 \end{pmatrix}$. Отже, $v_1 = (1, 0)$ – власний вектор, $\lambda_1 = 3$ – відповідне власне значення.
    * Спробуємо вектор $v_2 = (0, 1)$: $A v_2 = \begin{pmatrix} 3 & 0 \\ 0 & 2 \end{pmatrix} \begin{pmatrix} 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 0 \\ 2 \end{pmatrix} = 2 \begin{pmatrix} 0 \\ 1 \end{pmatrix}$. Отже, $v_2 = (0, 1)$ – власний вектор, $\lambda_2 = 2$ – відповідне власне значення.
    * Для діагональних матриць власні значення – це елементи на діагоналі, а власні вектори – стандартні базисні вектори.

* **Значення в AI:**
    * **PCA (Аналіз головних компонент):** Власні вектори матриці коваріації даних вказують напрямки найбільшої варіативності даних, а власні значення показують "важливість" цих напрямків. Це дозволяє зменшити розмірність, зберігши максимум інформації.
    * Аналіз стійкості систем, вібрацій.
    * Використовуються в алгоритмах на кшталт Google PageRank.

---

**Підсумки**

* **Обернена матриця ($A^{-1}$)** існує для квадратних невироджених матриць ($det(A) \neq 0$) і дозволяє "ділити" на матрицю.
* **Системи лінійних рівнянь ($AX=B$)** можна розв'язувати за допомогою оберненої матриці ($X = A^{-1}B$) або методу Крамера ($x_i = det(A_i)/det(A)$).
* **Лінійно незалежні вектори** не можуть бути виражені один через одного; **лінійно залежні** – можуть (один з них надлишковий).
* **Власні вектори ($v$)** матриці $A$ зберігають свій напрямок при множенні на $A$ ($Av = \lambda v$), а **власні значення ($\lambda$)** показують коефіцієнт масштабування.
* Ці поняття є фундаментальними інструментами для багатьох задач машинного навчання та аналізу даних.

**Наступні кроки**

* Опрацюйте приклади з лекції.
* Спробуйте самостійно розв'язати кілька систем 2x2 або 3x3 матричним методом та методом Крамера.
* Подумайте, як лінійна залежність може проявлятися у реальних даних (наприклад, зріст у сантиметрах і зріст у метрах – лінійно залежні ознаки).
* На наступних лекціях ми побачимо, як ці концепції застосовуються в конкретних алгоритмах AI.

---

Дякую за увагу! Тепер я готовий відповісти на ваші запитання.