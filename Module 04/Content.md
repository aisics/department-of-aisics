**Лекція 4: Основи математичного аналізу - Функції та похідні**

**Мета лекції:** Ознайомити слухачів з базовими поняттями математичного аналізу: функціями однієї та багатьох змінних, їх графіками, похідною та її змістом, а також основними правилами диференціювання. Пояснити, чому ці концепції є критично важливими для розуміння та побудови моделей штучного інтелекту.

---

Доброго дня! Сьогодні ми зануримось в ще одну ключову галузь математики для штучного інтелекту – **математичний аналіз**, або як його ще називають, **числення (calculus)**. Якщо лінійна алгебра дає нам інструменти для роботи з даними у вигляді векторів і матриць, то математичний аналіз дає інструменти для розуміння **змін** та **оптимізації**. Сьогодні ми розглянемо:

1.  **Функції:** Що це таке, які бувають (однієї та багатьох змінних), їхні властивості.
2.  **Графіки функцій:** Візуальне представлення функціональних залежностей.
3.  **Похідна:** Одне з найважливіших понять аналізу – що воно означає?
4.  **Правила диференціювання:** Як знаходити похідні?

**Навіщо це потрібно в AI?**

Уявіть, що ви навчаєте модель машинного навчання. Як дізнатися, наскільки добре вона працює? Зазвичай вводять **функцію втрат (loss function)**, яка показує, наскільки велика помилка моделі. Наша мета – **мінімізувати** цю помилку. Математичний аналіз, зокрема поняття **похідної**, лежить в основі найпопулярніших методів оптимізації в AI, таких як **градієнтний спуск (gradient descent)**. Саме похідні показують, як змінити параметри моделі, щоб найшвидше зменшити помилку. Без розуміння функцій і похідних неможливо зрозуміти, як "вчаться" нейронні мережі та інші моделі.

---

**Поняття функції**

* **Що таке функція?**
    Найпростіше уявити функцію як **правило** або **"чорну скриньку"**, яка приймає щось на вхід (аргумент) і видає щось на вихід (значення функції). Для одного входу – завжди один визначений вихід.
    * *Аналогія:* Кавовий автомат. Ви вносите певну суму (вхід) і натискаєте кнопку (правило) – отримуєте конкретний напій (вихід).

* **Функція однієї змінної:**
    Це функція, яка залежить лише від одного вхідного значення. Позначається зазвичай як $y = f(x)$, де $x$ – незалежна змінна (аргумент), а $y$ – залежна змінна (значення функції).
    * **Область визначення (Domain):** Множина всіх допустимих значень аргументу $x$.
    * **Область значень (Range):** Множина всіх можливих значень $y$, яких функція може набувати.
    * *Приклад 1:* $f(x) = x^2$.
        Область визначення: всі дійсні числа $\mathbb{R}$ (будь-яке число можна піднести до квадрату).
        Область значень: $y \ge 0$ (квадрат числа не може бути від'ємним).
    * *Приклад 2:* $g(x) = 1/x$.
        Область визначення: всі дійсні числа, крім $x=0$ (на нуль ділити не можна).
        Область значень: всі дійсні числа, крім $y=0$.

* **Функція багатьох змінних:**
    Це функція, яка залежить від двох або більше вхідних значень. Позначається як $z = f(x, y)$, $w = f(x, y, z)$ тощо.
    * **Область визначення:** Множина всіх допустимих наборів значень аргументів (наприклад, пар $(x, y)$).
    * **Область значень:** Множина всіх можливих значень функції.
    * *Приклад 1:* $f(x, y) = x + y$.
        Область визначення: всі можливі пари дійсних чисел $(x, y)$.
        Область значень: всі дійсні числа $\mathbb{R}$.
    * *Приклад 2:* $h(x, y) = \sqrt{x^2 + y^2}$ (відстань від точки $(x, y)$ до початку координат).
        Область визначення: всі пари $(x, y)$.
        Область значень: $z \ge 0$.
    * *Важливість в AI:* Функції втрат у машинному навчанні – це майже завжди функції багатьох змінних (параметрів моделі, яких можуть бути мільйони!). Наприклад, $Loss = f(w_1, w_2, ..., w_n, b)$, де $w_i$ та $b$ - ваги та зміщення моделі.

---

**Графіки функцій**

* **Що це?** Графік – це візуальне представлення функції, яке допомагає інтуїтивно зрозуміти її поведінку.
* **Функція однієї змінної ($y=f(x)$):**
    Графік будується на 2D-площині (декартова система координат). Кожна точка на графіку має координати $(x, f(x))$. Він показує, як змінюється $y$ при зміні $x$.
    * *Приклад:* Графік $f(x) = x^2$ – це парабола, що проходить через точку (0,0) і симетрична відносно осі Y.

* **Функція двох змінних ($z=f(x, y)$):**
    Графік будується у 3D-просторі. Кожна точка на графіку має координати $(x, y, f(x, y))$. Це вже не лінія, а **поверхня**.
    * *Приклад:* Графік $f(x, y) = x^2 + y^2$ – це параболоїд обертання (схожий на чашу).
    * Іноді для візуалізації використовують **лінії рівня (contour plot)** – криві на 2D-площині, де функція має стале значення (як на топографічних картах).
* **Функції багатьох (>2) змінних:**
    Пряма візуалізація неможлива (ми живемо у 3D світі). Проте концепція залишається – це певна "гіперповерхня" у багатовимірному просторі.
* *Значення в AI:* Хоч ми не можемо візуалізувати функцію втрат для мільйонів параметрів, уявлення про неї як про "ландшафт" у багатовимірному просторі, де ми шукаємо найнижчу точку (мінімум), є дуже корисним.

---

**Поняття похідної**

* **Основна ідея:** Похідна функції в певній точці показує, **наскільки швидко функція змінюється** саме в цій точці. Це **миттєва швидкість зміни**.

* **Геометричний зміст:**
    Похідна функції $f(x)$ в точці $x_0$, що позначається як $f'(x_0)$ або $\frac{dy}{dx}|_{x=x_0}$, дорівнює **кутовому коефіцієнту (тангенсу кута нахилу) дотичної лінії** до графіка функції $y=f(x)$ в точці $(x_0, f(x_0))$.
    * **Дотична** – це пряма, яка "торкається" графіка в одній точці і має такий самий нахил, як і графік у цій точці.
    * *Інтуїція:* Якщо похідна велика і додатня – функція швидко зростає. Якщо похідна велика і від'ємна – функція швидко спадає. Якщо похідна дорівнює нулю – функція має "горизонтальну" дотичну (можливо, це точка мінімуму, максимуму або перегину).

* **Фізичний зміст:**
    Якщо функція описує залежність шляху від часу $s(t)$, то її похідна $s'(t)$ – це **миттєва швидкість** $v(t)$. Якщо функція описує швидкість $v(t)$, то її похідна $v'(t)$ – це **миттєве прискорення** $a(t)$. Загалом, похідна описує швидкість будь-якого процесу, що описується функцією.

* **Ключове значення для AI (Градієнтний спуск):**
    Уявіть функцію втрат $L(w)$ як пагорб, де $w$ – параметр моделі. Ми хочемо знайти найнижчу точку (мінімум $L$). Похідна $L'(w)$ вказує на нахил пагорба в поточній точці $w$.
    * Якщо $L'(w) > 0$, пагорб йде вгору – нам треба рухатись назад (зменшувати $w$).
    * Якщо $L'(w) < 0$, пагорб йде вниз – нам треба рухатись вперед (збільшувати $w$).
    * Градієнтний спуск робить крок у напрямку, протилежному до знаку похідної: $w_{new} = w_{old} - \eta \cdot L'(w_{old})$, де $\eta$ (ета) – швидкість навчання (learning rate). Для багатьох параметрів використовують **градієнт** – вектор з похідних по кожному параметру.

---

**Правила диференціювання**

Знаходити похідну за означенням (через границю) довго і складно. На щастя, існують прості правила для знаходження похідних основних функцій та їх комбінацій. Нехай $c$ – стала, $f(x)$ та $g(x)$ – функції.

1.  **Похідна сталої:** $(c)' = 0$
    * Стала не змінюється, її швидкість зміни – нуль.
2.  **Степенева функція:** $(x^n)' = n x^{n-1}$
    * *Приклад:* $(x^3)' = 3x^{3-1} = 3x^2$. $(x)' = (x^1)' = 1x^0 = 1$. $(\sqrt{x})' = (x^{1/2})' = \frac{1}{2}x^{-1/2} = \frac{1}{2\sqrt{x}}$.
3.  **Множення на сталу:** $(c \cdot f(x))' = c \cdot f'(x)$
    * *Приклад:* $(5x^3)' = 5 \cdot (x^3)' = 5 \cdot (3x^2) = 15x^2$.
4.  **Сума/Різниця:** $(f(x) \pm g(x))' = f'(x) \pm g'(x)$
    * *Приклад:* $(x^2 + 4x - 1)' = (x^2)' + (4x)' - (1)' = 2x + 4 \cdot (x)' - 0 = 2x + 4$.
5.  **Добуток:** $(f(x) \cdot g(x))' = f'(x)g(x) + f(x)g'(x)$
    * *Приклад:* $(x^2 \cdot \sin x)' = (x^2)' \cdot \sin x + x^2 \cdot (\sin x)' = 2x \sin x + x^2 \cos x$. (Припускаємо, що похідна $\sin x$ це $\cos x$).
6.  **Частка:** $(\frac{f(x)}{g(x)})' = \frac{f'(x)g(x) - f(x)g'(x)}{(g(x))^2}$ (при $g(x) \neq 0$)
    * *Приклад:* $(\frac{x}{x+1})' = \frac{(x)'(x+1) - x(x+1)'}{(x+1)^2} = \frac{1 \cdot (x+1) - x \cdot 1}{(x+1)^2} = \frac{x+1-x}{(x+1)^2} = \frac{1}{(x+1)^2}$.
7.  **Складна функція (Chain Rule) – ДУЖЕ ВАЖЛИВО для AI!**
    Якщо $y = f(g(x))$ (функція від функції), то $y' = f'(g(x)) \cdot g'(x)$.
    * *Ідея:* Похідна "зовнішньої" функції $f$, обчислена у точці "внутрішньої" функції $g(x)$, помножена на похідну "внутрішньої" функції $g$.
    * *Приклад 1:* $y = (x^2+1)^3$. Тут $f(u)=u^3$ (зовнішня), $g(x)=x^2+1$ (внутрішня).
        $f'(u) = 3u^2$, $g'(x) = 2x$.
        $y' = f'(g(x)) \cdot g'(x) = 3(x^2+1)^2 \cdot (2x) = 6x(x^2+1)^2$.
    * *Приклад 2:* $y = \sin(x^2)$. $f(u)=\sin u$, $g(x)=x^2$. $f'(u)=\cos u$, $g'(x)=2x$.
        $y' = \cos(x^2) \cdot 2x$.
    * *Значення в AI:* Нейронні мережі – це по суті величезні складні функції (композиції багатьох простих функцій). Щоб обчислити градієнт для градієнтного спуску, використовується алгоритм **зворотного поширення помилки (backpropagation)**, який є ефективним застосуванням правила ланцюжка (Chain Rule) для обчислення похідних складної функції втрат по всім параметрам мережі.

---

**Підсумки**

* **Функція** – це правило, що ставить у відповідність входу (аргументу) вихід (значення). Бувають функції однієї та багатьох змінних.
* **Графік** – візуалізація функції.
* **Похідна ($f'(x)$)** – це миттєва швидкість зміни функції, або нахил дотичної до її графіка.
* **Похідні є ОСНОВОЮ оптимізації в AI**, зокрема для **градієнтного спуску**, який мінімізує функцію втрат.
* Існують **правила диференціювання**, які спрощують знаходження похідних. **Правило ланцюжка (Chain Rule)** є критично важливим для навчання нейронних мереж.

**Наступні кроки**

* Спробуйте знайти похідні для простих функцій, використовуючи вивчені правила.
* Подумайте про функцію втрат як про "ландшафт", а про похідну/градієнт як про напрямок спуску з цього ландшафту.
* На наступній лекції ми детальніше поговоримо про похідні функцій багатьох змінних (частинні похідні, градієнт) та їх застосування в оптимізації.

---

Дякую за увагу! Тепер я готовий відповісти на ваші запитання.