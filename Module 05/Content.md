**Лекція 5: Основи математичного аналізу - Частинні похідні та оптимізація**

**Мета лекції:** Розширити поняття похідної на функції багатьох змінних (частинні похідні, градієнт, гессіан). Пояснити їхній геометричний зміст та показати, як ці інструменти використовуються для знаходження екстремумів функцій, що є основою оптимізації в машинному навчанні.

---

Доброго дня! Минулого разу ми говорили про функції однієї змінної та їхні похідні, які показують швидкість зміни функції. Ми також згадали, що похідні є ключовим інструментом для оптимізації, зокрема для методу градієнтного спуску, який допомагає моделям ШІ "навчатися".

Однак, як ми знаємо, моделі машинного навчання, особливо нейронні мережі, мають величезну кількість параметрів. Їхня функція втрат залежить не від однієї, а від багатьох змінних. Як аналізувати зміну таких функцій? Як знаходити їхні мінімуми? Саме про це ми поговоримо сьогодні. Наш план:

1.  **Частинні похідні:** Як знаходити похідну функції, коли змінних багато?
2.  **Градієнт:** Що це таке і чому він вказує "найкращий" напрямок?
3.  **Матриця Гессе:** Погляд на "кривину" функції.
4.  **Знаходження екстремумів:** Як знайти мінімум або максимум функції (основи)?

**Навіщо це потрібно в AI?**

Практично все навчання моделей ШІ – це **оптимізація**. Ми намагаємось знайти такий набір параметрів моделі (ваг, зміщень), при якому функція втрат (помилка) буде **мінімальною**. Градієнтний спуск, який ми згадували, є лише одним з методів оптимізації. Розуміння частинних похідних, градієнта та умов екстремуму дозволяє не тільки глибше зрозуміти градієнтний спуск, але й вивчити більш просунуті методи оптимізації, зрозуміти проблеми, що виникають при навчанні (локальні мінімуми, сідлові точки), та як їх долати.

---

**Частинні похідні**

* **Ідея:** Коли функція залежить від кількох змінних (наприклад, $f(x, y)$), як виміряти швидкість її зміни? Ми можемо подивитись, як вона змінюється, якщо **змінювати лише одну змінну**, а всі інші **вважати константами (сталими)**. Це і є ідея частинної похідної.

* **Визначення та Позначення:**
    * **Частинна похідна функції $f$ по змінній $x$** (позначається $\frac{\partial f}{\partial x}$ або $f_x$) – це похідна функції $f$ по змінній $x$, обчислена за припущення, що всі інші змінні (наприклад, $y, z, ...$) є сталими.
    * Аналогічно, **частинна похідна по $y$** ($\frac{\partial f}{\partial y}$ або $f_y$) – це похідна по $y$, де $x, z, ...$ вважаються сталими.

* **Приклад:**
    Нехай $f(x, y) = x^2 + 3xy + y^4$.
    * Щоб знайти $\frac{\partial f}{\partial x}$, вважаємо $y$ константою:
        $\frac{\partial f}{\partial x} = \frac{\partial}{\partial x}(x^2) + \frac{\partial}{\partial x}(3y \cdot x) + \frac{\partial}{\partial x}(y^4)$
        Тут $(x^2)' = 2x$. У доданку $3xy$, $3y$ - це константа, отже $(3y \cdot x)' = 3y \cdot (x)' = 3y \cdot 1 = 3y$. У доданку $y^4$, $y$ - константа, отже $(y^4)' = 0$.
        Результат: $\frac{\partial f}{\partial x} = 2x + 3y$.
    * Щоб знайти $\frac{\partial f}{\partial y}$, вважаємо $x$ константою:
        $\frac{\partial f}{\partial y} = \frac{\partial}{\partial y}(x^2) + \frac{\partial}{\partial y}(3x \cdot y) + \frac{\partial}{\partial y}(y^4)$
        Тут $(x^2)' = 0$ (бо $x$ - константа). У доданку $3xy$, $3x$ - константа, отже $(3x \cdot y)' = 3x \cdot (y)' = 3x \cdot 1 = 3x$. У доданку $y^4$, похідна по $y$ дорівнює $4y^3$.
        Результат: $\frac{\partial f}{\partial y} = 3x + 4y^3$.

* **Частинні похідні вищих порядків:**
    Ми можемо брати похідні від похідних.
    * **Другі "чисті" похідні:** $\frac{\partial^2 f}{\partial x^2} = \frac{\partial}{\partial x} (\frac{\partial f}{\partial x})$ і $\frac{\partial^2 f}{\partial y^2} = \frac{\partial}{\partial y} (\frac{\partial f}{\partial y})$.
    * **Другі "мішані" похідні:** $\frac{\partial^2 f}{\partial x \partial y} = \frac{\partial}{\partial x} (\frac{\partial f}{\partial y})$ і $\frac{\partial^2 f}{\partial y \partial x} = \frac{\partial}{\partial y} (\frac{\partial f}{\partial x})$.
    * **Теорема Клеро (про рівність мішаних похідних):** Якщо другі мішані похідні існують і неперервні в околі точки, то вони рівні: $\frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2 f}{\partial y \partial x}$.
    * *Приклад (продовження):*
        $\frac{\partial^2 f}{\partial x^2} = \frac{\partial}{\partial x}(2x + 3y) = 2$.
        $\frac{\partial^2 f}{\partial y^2} = \frac{\partial}{\partial y}(3x + 4y^3) = 12y^2$.
        $\frac{\partial^2 f}{\partial x \partial y} = \frac{\partial}{\partial x}(3x + 4y^3) = 3$.
        $\frac{\partial^2 f}{\partial y \partial x} = \frac{\partial}{\partial y}(2x + 3y) = 3$. (Бачимо, що вони рівні).

---

**Градієнт**

* **Визначення:** **Градієнт** функції багатьох змінних $f(x_1, x_2, ..., x_n)$ – це **вектор**, складений з усіх її частинних похідних першого порядку.
    * Позначається: $\nabla f$ (читається "набла еф") або grad $f$.
    * $\nabla f = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, ..., \frac{\partial f}{\partial x_n} \right)$.

* **Приклад:** Для нашої функції $f(x, y) = x^2 + 3xy + y^4$:
    $\nabla f = \left( \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y} \right) = (2x + 3y, 3x + 4y^3)$.
    * Градієнт є вектором, який залежить від точки. Наприклад, у точці $(1, 1)$:
        $\nabla f(1, 1) = (2(1) + 3(1), 3(1) + 4(1)^3) = (5, 7)$.
    * У точці $(0, 1)$:
        $\nabla f(0, 1) = (2(0) + 3(1), 3(0) + 4(1)^3) = (3, 4)$.

* **Геометричний зміст:**
    Це найважливіше! Вектор градієнта $\nabla f$, обчислений в певній точці, має дві властивості:
    1.  Він вказує у **напрямку найшвидшого зростання** функції в цій точці.
    2.  Його **довжина (модуль)** $||\nabla f||$ дорівнює цій максимальній швидкості зростання.
    * *Аналогія:* Уявіть, що ви стоїте на схилі гори (графік функції $f(x,y)$). Градієнт $\nabla f$ у вашій точці - це стрілка, що лежить на схилі і вказує точно вгору по найкрутішому підйому. Довжина стрілки показує, наскільки крутий цей підйом.

* **Значення для AI (Градієнтний спуск):**
    Ми хочемо **мінімізувати** функцію втрат $L$. Градієнт $\nabla L$ вказує напрямок найшвидшого *зростання* $L$. Щоб рухатись до мінімуму (найшвидше *спадання*), нам потрібно рухатись у **протилежному** напрямку, тобто у напрямку $-\nabla L$.
    Саме тому в градієнтному спуску ми оновлюємо параметри $w$ за правилом:
    $w_{new} = w_{old} - \eta \nabla L(w_{old})$
    де $\eta$ – швидкість навчання (маленький крок), а $\nabla L$ – градієнт функції втрат по всіх параметрах $w$.

---

**Матриця Гессе (Гессіан)**

* **Визначення:** **Матриця Гессе** (або **Гессіан**) функції $f(x_1, ..., x_n)$ – це квадратна матриця, що містить усі її **другі** частинні похідні.
    * Позначається: $H$ або $\nabla^2 f$.
    * $H = \begin{pmatrix} \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \dots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\ \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \dots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \dots & \frac{\partial^2 f}{\partial x_n^2} \end{pmatrix}$
    * Якщо другі похідні неперервні, то матриця Гессе є **симетричною** ($H^T = H$).

* **Приклад:** Для $f(x, y) = x^2 + 3xy + y^4$:
    Ми знайшли: $\frac{\partial^2 f}{\partial x^2}=2$, $\frac{\partial^2 f}{\partial y^2}=12y^2$, $\frac{\partial^2 f}{\partial x \partial y}=3$, $\frac{\partial^2 f}{\partial y \partial x}=3$.
    $H = \begin{pmatrix} \frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x \partial y} \\ \frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2} \end{pmatrix} = \begin{pmatrix} 2 & 3 \\ 3 & 12y^2 \end{pmatrix}$.

* **Геометричний зміст та значення для AI:**
    Якщо градієнт описує "нахил" функції (перша похідна), то гессіан описує її **"кривину"** (друга похідна).
    * Гессіан використовується в **тесті другої похідної** для визначення типу критичної точки (мінімум, максимум, сідлова точка) – про це далі.
    * Він лежить в основі **оптимізаційних методів другого порядку** (наприклад, метод Ньютона). Ці методи враховують кривину функції і можуть сходитись до мінімуму набагато швидше, ніж градієнтний спуск (який використовує лише інформацію першого порядку). Однак обчислення та обернення матриці Гессе для функцій багатьох змінних (як у нейромережах) є дуже ресурсозатратним, тому їх використовують рідше або застосовують наближені методи.

---

**Знаходження екстремумів (Основи)**

**Екстремум** – це загальна назва для максимуму або мінімуму функції. Нас в AI найчастіше цікавить **мінімум** функції втрат.

* **Функція однієї змінної $y=f(x)$:**
    1.  **Знайти критичні точки:** Це точки, де $f'(x) = 0$ або похідна не існує. Це "підозрілі" точки, де може бути екстремум (горизонтальна дотична).
    2.  **Перевірити кожну критичну точку $x_0$ за допомогою другої похідної:**
        * Якщо $f''(x_0) > 0$, то в точці $x_0$ – **локальний мінімум** (функція "вгнута", як U).
        * Якщо $f''(x_0) < 0$, то в точці $x_0$ – **локальний максимум** (функція "опукла", як ∩).
        * Якщо $f''(x_0) = 0$, тест не дає відповіді (потрібні додаткові дослідження).

* **Функція багатьох змінних $z=f(x, y, ...)$:**
    1.  **Знайти критичні точки:** Це точки, де **градієнт дорівнює нульовому вектору**, $\nabla f = \vec{0}$. Тобто, всі перші частинні похідні одночасно дорівнюють нулю: $\frac{\partial f}{\partial x_1} = 0, \frac{\partial f}{\partial x_2} = 0, ...$ . Це точки, де "ландшафт" функції є локально пласким.
    2.  **Перевірити кожну критичну точку $(a, b, ...)$ за допомогою матриці Гессе $H$:** (Спрощена версія для двох змінних $f(x,y)$)
        * Обчислити гессіан $H = \begin{pmatrix} f_{xx} & f_{xy} \\ f_{yx} & f_{yy} \end{pmatrix}$ у критичній точці $(a,b)$.
        * Обчислити **визначник гессіана** $D = det(H) = f_{xx}f_{yy} - (f_{xy})^2$.
        * Обчислити значення $f_{xx}$ у точці $(a,b)$.
        * **Правила:**
            * Якщо $D > 0$ і $f_{xx}(a,b) > 0 \implies$ **локальний мінімум** у $(a,b)$.
            * Якщо $D > 0$ і $f_{xx}(a,b) < 0 \implies$ **локальний максимум** у $(a,b)$.
            * Якщо $D < 0 \implies$ **сідлова точка** у $(a,b)$ (в одному напрямку мінімум, в іншому – максимум, як гірський перевал або сідло).
            * Якщо $D = 0 \implies$ тест не дає відповіді.

* **Значення для AI:**
    Градієнтний спуск шукає точки, де $\nabla L \approx \vec{0}$. Тест другої похідної допомагає зрозуміти, що це за точка. У багатовимірних просторах, характерних для нейромереж, **сідлові точки** зустрічаються набагато частіше, ніж локальні мінімуми. Це одна з проблем оптимізації: алгоритм може "застрягти" у сідловій точці, де градієнт нульовий, хоча це не справжній мінімум. Сучасні оптимізатори (Adam, RMSProp) мають механізми, що допомагають долати сідлові точки.

---

**Підсумки**

* **Частинні похідні** дозволяють аналізувати зміну функцій багатьох змінних по кожній змінній окремо.
* **Градієнт ($\nabla f$)** – це вектор перших частинних похідних, що вказує у напрямку найшвидшого зростання функції. Напрямок $-\nabla f$ використовується у градієнтному спуску для мінімізації.
* **Матриця Гессе ($H$)** – це матриця других частинних похідних, що описує кривину функції і використовується у тесті другої похідної та методах оптимізації другого порядку.
* **Знаходження екстремумів** (особливо мінімумів) є центральною задачею оптимізації в AI. Воно починається з пошуку точок, де градієнт дорівнює нулю, та аналізу цих точок за допомогою других похідних (гессіана).

**Наступні кроки**

* Спробуйте обчислити градієнт і гессіан для простих функцій двох змінних.
* Подумайте про різницю між локальним мінімумом і сідловою точкою на прикладі "ландшафту" функції.
* На наступних лекціях ми перейдемо до основ теорії ймовірностей та статистики, які є іншою важливою математичною опорою для штучного інтелекту.

---

Дякую за увагу! Я готовий відповісти на ваші запитання.