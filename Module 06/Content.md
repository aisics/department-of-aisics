**Лекція 6: Основи теорії ймовірностей**

**Мета лекції:** Ознайомити слухачів з фундаментальними поняттями теорії ймовірностей: випадковими подіями, ймовірністю, умовною ймовірністю, незалежністю подій та ключовою формулою Баєса. Пояснити, чому теорія ймовірностей є абсолютно необхідною для роботи зі штучним інтелектом.

---

Доброго дня! Після вивчення основ лінійної алгебри та математичного аналізу, які допомагають нам працювати з даними та оптимізацією, ми переходимо до ще одного наріжного каменя математики для ШІ – **теорії ймовірностей**.

Чому це так важливо? Світ навколо нас сповнений **невизначеності**. Дані, з якими працюють системи ШІ, часто є неповними, зашумленими або стохастичними (випадковими) за своєю природою. Теорія ймовірностей дає нам математичний апарат для того, щоб кількісно оцінювати цю невизначеність, моделювати випадкові процеси та робити висновки в умовах неповної інформації. Від розпізнавання спаму до медичної діагностики, від безпілотних автомобілів до генерації тексту – ймовірнісні методи лежать в основі багатьох сучасних AI-систем.

Сьогодні ми розглянемо базові концепції:

1.  **Основні поняття:** Випадкова подія, ймовірність, простір подій.
2.  **Аксіоми ймовірності:** Фундаментальні правила.
3.  **Умовна ймовірність:** Ймовірність події за умови, що сталася інша.
4.  **Формула Баєса:** Як оновлювати наші "переконання" на основі нових даних.
5.  **Незалежні події:** Коли події не впливають одна на одну.

---

**Основні поняття теорії ймовірностей**

* **Випадковий експеримент (Random Experiment):** Це будь-який процес, результат якого наперед точно невідомий.
    * *Приклади:* Підкидання монети, кидання грального кубика, вимірювання температури завтра, результат медичного аналізу.

* **Простір елементарних подій (Sample Space, Ω):** Це множина **всіх можливих** унікальних результатів випадкового експерименту.
    * *Приклади:*
        * Підкидання монети: Ω = {Герб, Решка} (або {H, T}).
        * Кидання кубика: Ω = {1, 2, 3, 4, 5, 6}.
        * Вимірювання температури: Ω може бути інтервалом чисел, наприклад, [-50°C, +60°C].

* **Випадкова подія (Random Event, A, B, ...):** Це будь-яка підмножина простору елементарних подій Ω. Тобто, це один або декілька можливих результатів експерименту, які нас цікавлять.
    * *Приклади (для кубика):*
        * Подія A = "Випало число 3". A = {3}.
        * Подія B = "Випало парне число". B = {2, 4, 6}.
        * Подія C = "Випало число більше 4". C = {5, 6}.
        * Неможлива подія (порожня множина, $\emptyset$): "Випало число 7".
        * Вірогідна подія (весь простір Ω): "Випало число від 1 до 6".

* **Ймовірність (Probability, P(A)):** Це числова міра того, наскільки **ймовірно** трапиться подія A. Ймовірність завжди знаходиться в межах від 0 до 1:
    * $P(A) = 0$ означає, що подія A **неможлива**.
    * $P(A) = 1$ означає, що подія A **вірогідна** (обов'язково станеться).
    * $0 < P(A) < 1$ означає, що подія є випадковою.
    * *Класичне визначення (для скінченних Ω з рівноможливими наслідками):* Ймовірність події A дорівнює відношенню кількості наслідків, сприятливих для A ($|A|$), до загальної кількості можливих наслідків ($|\Omega|$):
        $P(A) = \frac{|A|}{|\Omega|}$
        * *Приклад (кубик):* $P(\text{"Випало парне число"}) = P(B) = \frac{|\{2, 4, 6\}|}{|\{1, 2, 3, 4, 5, 6\}|} = \frac{3}{6} = 0.5$.
    * *Зауваження:* Класичне визначення працює не завжди (наприклад, якщо наслідки не рівноможливі, як у "нечесній" монеті, або якщо простір подій нескінченний).

---

**Аксіоми ймовірності (Аксіоми Колмогорова)**

Це фундаментальні властивості, яким має задовольняти будь-яка функція ймовірності P:

1.  **Невід'ємність:** Для будь-якої події A, її ймовірність не може бути від'ємною:
    $P(A) \ge 0$.
2.  **Нормування:** Ймовірність вірогідної події (тобто, всього простору елементарних подій Ω) дорівнює одиниці:
    $P(\Omega) = 1$.
3.  **Адитивність (для несумісних подій):** Якщо події $A_1, A_2, ...$ є **попарно несумісними** (тобто, ніякі дві з них не можуть відбутися одночасно, $A_i \cap A_j = \emptyset$ для $i \neq j$), то ймовірність того, що відбудеться хоча б одна з цих подій (їх об'єднання), дорівнює сумі їхніх ймовірностей:
    $P(A_1 \cup A_2 \cup ...) = P(A_1) + P(A_2) + ...$.
    * *Приклад (кубик):* Події "Випало 1" ($A_1=\{1\}$) і "Випало 2" ($A_2=\{2\}$) несумісні.
        $P(\text{"Випало 1 або 2"}) = P(A_1 \cup A_2) = P(A_1) + P(A_2) = \frac{1}{6} + \frac{1}{6} = \frac{2}{6} = \frac{1}{3}$.

З цих аксіом випливають інші важливі властивості, наприклад: $P(\emptyset) = 0$, $P(A) \le 1$, $P(\neg A) = 1 - P(A)$ (де $\neg A$ - протилежна подія).

---

**Умовна ймовірність**

* **Ідея:** Часто нас цікавить ймовірність події A, **за умови, що ми вже знаємо, що інша подія B відбулася**. Це називається умовною ймовірністю A за умови B.

* **Визначення та Позначення:** Умовна ймовірність події A за умови B (позначається $P(A|B)$) визначається як:
    $P(A|B) = \frac{P(A \cap B)}{P(B)}$, за умови, що $P(B) > 0$.
    Де $P(A \cap B)$ – це ймовірність того, що **обидві** події A і B відбудуться одночасно (їх перетин).

* **Інтуїція:** Знання про те, що подія B відбулася, фактично звужує наш простір можливих подій до B. Умовна ймовірність $P(A|B)$ показує, яку частку цього нового простору B займають наслідки, сприятливі також і для A.

* **Приклад:** Кидаємо кубик. Нехай A = "Випало число 2", B = "Випало парне число".
    Ω = {1, 2, 3, 4, 5, 6}. A = {2}, B = {2, 4, 6}.
    $P(A) = 1/6$. $P(B) = 3/6 = 1/2$.
    Подія $A \cap B$ = "Випало 2 І випало парне число" = {2}. $P(A \cap B) = 1/6$.
    Тоді умовна ймовірність випадання 2 за умови, що випало парне число:
    $P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{1/6}{3/6} = \frac{1}{3}$.
    Це логічно: якщо ми знаємо, що випало парне число ({2, 4, 6}), то існує 3 рівноможливих варіанти, і лише один з них - це 2.

* **Значення для AI:** Умовні ймовірності є основою для моделювання залежностей між змінними. Наприклад, у розпізнаванні мови: яка ймовірність наступного слова *за умови* попередніх слів? У медичній діагностиці: яка ймовірність хвороби *за умови* певних симптомів?

---

**Незалежні події**

* **Ідея:** Дві події називаються **незалежними**, якщо настання однієї з них **ніяк не впливає** на ймовірність настання іншої.

* **Формальні визначення:** Події A і B є незалежними, якщо виконується **будь-яка** з наступних еквівалентних умов:
    1.  $P(A|B) = P(A)$ (знання про B не змінює ймовірність A).
    2.  $P(B|A) = P(B)$ (знання про A не змінює ймовірність B).
    3.  $P(A \cap B) = P(A) \cdot P(B)$ (ймовірність їх одночасної появи дорівнює добутку їхніх індивідуальних ймовірностей). **Це визначення найчастіше використовується для перевірки незалежності.**

* **Приклад 1 (Незалежні):** Два кидки монети. A = "Перший кидок - Герб", B = "Другий кидок - Герб".
    $P(A) = 1/2$, $P(B) = 1/2$. Подія $A \cap B$ = "Обидва кидки - Герб". Є 4 рівноможливі наслідки: {HH, HT, TH, TT}, отже $P(A \cap B) = 1/4$.
    Перевіряємо: $P(A) \cdot P(B) = (1/2) \cdot (1/2) = 1/4$. Оскільки $P(A \cap B) = P(A)P(B)$, події незалежні.

* **Приклад 2 (Залежні):** Витягаємо дві карти з колоди (52 карти) *без повернення*. A = "Перша карта - Король", B = "Друга карта - Король".
    $P(A) = 4/52 = 1/13$.
    Щоб знайти $P(B)$, треба розглянути два випадки (перша - Король або ні), але простіше знайти $P(B|A)$. Якщо перша карта - Король (подія A сталася), то в колоді залишилась 51 карта, серед яких 3 Королі. Отже, $P(B|A) = 3/51 = 1/17$.
    Оскільки $P(B|A) \neq P(A)$ (бо $1/17 \neq 1/13$), події A і B є **залежними**.

* **Значення для AI:** Поняття незалежності дуже важливе. Деякі прості моделі (наприклад, **Наївний Баєсівський класифікатор**) роблять сильне припущення про незалежність ознак (фіч), що спрощує обчислення, але може не відповідати дійсності. Розуміння залежностей між змінними є ключовим для побудови точних моделей.

---

**Формула Баєса (Теорема Баєса)**

* **Ідея:** Формула Баєса дозволяє нам "перевернути" умовну ймовірність. Якщо ми знаємо ймовірність події B за умови A ($P(B|A)$), то ця формула дозволяє обчислити ймовірність події A за умови B ($P(A|B)$). Це фундаментальний інструмент для **оновлення наших знань або переконань ($P(A)$) на основі нових доказів або даних (подія B)**.

* **Формула:**
    $P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$

* **Термінологія:**
    * $P(A|B)$: **Апостеріорна ймовірність** (Posteriоr) – ймовірність гіпотези A *після* отримання даних B. Це те, що ми хочемо знайти.
    * $P(B|A)$: **Правдоподібність** (Likelihood) – ймовірність отримати дані B, *якщо* гіпотеза A вірна.
    * $P(A)$: **Апріорна ймовірність** (Prior) – наша початкова віра в гіпотезу A *до* отримання даних B.
    * $P(B)$: **Ймовірність даних** (Evidence) – повна ймовірність отримати дані B (незалежно від гіпотези A). Часто обчислюється за **формулою повної ймовірності**: $P(B) = P(B|A)P(A) + P(B|\neg A)P(\neg A)$, де $\neg A$ – гіпотеза, протилежна до A.

* **Приклад (Медичний тест):**
    Нехай A = "Пацієнт хворий на певну хворобу". B = "Тест на цю хворобу позитивний".
    * Припустимо, ми знаємо (з медичної статистики):
        * $P(A) = 0.01$ (Апріорна ймовірність: 1% населення хворіє).
        * $P(B|A) = 0.9$ (Правдоподібність: якщо людина хвора, тест показує "+" у 90% випадків – чутливість тесту).
        * $P(B|\neg A) = 0.05$ (Ймовірність хибнопозитивного результату: якщо людина здорова ($\neg A$), тест показує "+" у 5% випадків). $P(\neg A) = 1 - P(A) = 0.99$.
    * Знайдемо $P(B)$ за формулою повної ймовірності:
        $P(B) = P(B|A)P(A) + P(B|\neg A)P(\neg A) = (0.9)(0.01) + (0.05)(0.99) = 0.009 + 0.0495 = 0.0585$. (Загальна ймовірність отримати позитивний тест).
    * Тепер використаємо формулу Баєса, щоб знайти $P(A|B)$ – ймовірність того, що людина дійсно хвора, якщо її тест позитивний:
        $P(A|B) = \frac{P(B|A)P(A)}{P(B)} = \frac{(0.9)(0.01)}{0.0585} = \frac{0.009}{0.0585} \approx 0.1538$.
    * *Інтерпретація:* Навіть при позитивному тесті, ймовірність того, що пацієнт дійсно хворий, становить лише близько 15.4%! Це тому, що апріорна ймовірність хвороби була низькою, а тест дає певну кількість хибнопозитивних результатів.

* **Значення для AI:** Формула Баєса – це серце **баєсівських методів** в машинному навчанні. Вона використовується для:
    * Класифікації (Наївний Баєс для фільтрації спаму).
    * Побудови моделей, що навчаються з даних (Баєсівські мережі).
    * Оновлення параметрів моделі в процесі навчання.
    * Роботи з невизначеністю в експертних системах.

---

**Підсумки**

* Теорія ймовірностей надає мову та інструменти для опису та аналізу **невизначеності**.
* Основні поняття включають **випадкові події**, **простір подій** та **ймовірність**.
* Фундаментальні правила задаються **аксіомами Колмогорова**.
* **Умовна ймовірність** ($P(A|B)$) враховує наявність попередньої інформації.
* **Незалежні події** не впливають одна на одну ($P(A \cap B) = P(A)P(B)$).
* **Формула Баєса** ($P(A|B) = \frac{P(B|A)P(A)}{P(B)}$) дозволяє оновлювати ймовірності гіпотез на основі нових даних і є основою баєсівського підходу в AI.
* Розуміння цих концепцій є критично важливим для роботи з реальними даними та побудови багатьох сучасних систем штучного інтелекту.

**Наступні кроки**

* Подумайте над прикладами умовних ймовірностей та незалежності з повсякденного життя.
* Спробуйте розв'язати прості задачі з використанням формули Баєса.
* На наступних лекціях ми продовжимо вивчати ймовірність, розглядаючи випадкові величини та їх розподіли, які дозволяють моделювати числові характеристики випадкових явищ.

---

Дякую за увагу! Я готовий відповісти на ваші запитання.