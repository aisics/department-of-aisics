Гаразд, ось скрипт сьомої лекції на тему "Випадкові величини та основні розподіли", створений для початківців у сфері ШІ.

---

**Лекція 7: Випадкові величини та основні розподіли**

**Мета лекції:** Ввести поняття випадкових величин як спосіб числового представлення результатів випадкових експериментів. Розглянути способи опису їхньої поведінки за допомогою функцій розподілу та щільності ймовірності. Ознайомити з ключовими числовими характеристиками (математичне сподівання, дисперсія) та найважливішими типами розподілів (Бернуллі, біноміальний, нормальний), що часто зустрічаються в задачах ШІ.

---

Доброго дня! На попередній лекції ми познайомилися з основами теорії ймовірностей – мовою для опису невизначеності через події та їхні ймовірності. Однак часто нас цікавлять не самі події (як-от "випав герб" чи "тест позитивний"), а певні **числові характеристики**, пов'язані з результатами випадкових експериментів. Наприклад, *скільки* разів випав герб при 10 підкиданнях, або *яке* значення температури буде завтра. Для цього в теорії ймовірностей використовується поняття **випадкової величини**.

Сьогодні ми поговоримо про:

1.  **Випадкові величини:** Що це таке, і чим відрізняються дискретні та неперервні.
2.  **Розподіли ймовірностей:** Як описати, які значення і з якою ймовірністю може приймати випадкова величина (функція розподілу, щільність).
3.  **Математичне сподівання та дисперсія:** Основні числові характеристики випадкової величини ("середнє" та "розкид").
4.  **Основні розподіли:** Розглянемо три "стовпи" теорії ймовірностей – розподіли Бернуллі, біноміальний та нормальний.

**Навіщо це потрібно в AI?**

Випадкові величини та їхні розподіли – це фундаментальні інструменти для моделювання реального світу в AI:

* **Ознаки (Features):** Багато ознак у наборах даних є випадковими величинами (наприклад, кількість кімнат у будинку – дискретна ВВ, ціна будинку – неперервна ВВ). Розуміння їх розподілу допомагає в аналізі даних та побудові моделей.
* **Моделювання невизначеності:** Шум у даних, помилки вимірювань, випадковість у поведінці користувачів – все це моделюється за допомогою випадкових величин та їх розподілів (часто – нормального).
* **Виходи моделей:** Результати роботи багатьох моделей ШІ самі є випадковими величинами або параметрами розподілів (наприклад, модель класифікації може видавати ймовірності належності до класів, що описуються розподілом Бернуллі або категорійним).
* **Статистичне висновування:** Порівняння моделей, перевірка гіпотез, побудова довірчих інтервалів – все це базується на теорії розподілів.

---

**Випадкові величини (Random Variables)**

* **Ідея:** Часто результати експерименту не є числами (наприклад, "Герб", "Решка"). Випадкова величина – це спосіб присвоїти числове значення кожному можливому наслідку випадкового експерименту.
* **Формальне визначення:** **Випадкова величина** (ВВ), яку зазвичай позначають великими латинськими літерами ($X, Y, Z$), – це функція, яка ставить у відповідність кожному елементарному наслідку $\omega$ з простору подій $\Omega$ певне дійсне число $X(\omega)$.
    Простіше кажучи, це величина, значення якої є числовим результатом випадкового явища.

* **Типи випадкових величин:**
    1.  **Дискретна випадкова величина (Discrete RV):** Може приймати лише скінченну або зліченну кількість значень (значення можна "перерахувати"). Зазвичай це результати підрахунку.
        * *Приклади:*
            * $X$ = кількість гербів при 3 підкиданнях монети (можливі значення: 0, 1, 2, 3).
            * $Y$ = кількість дефектних виробів у партії з 100 штук (можливі значення: 0, 1, ..., 100).
            * $Z$ = результат кидання грального кубика (можливі значення: 1, 2, 3, 4, 5, 6).
    2.  **Неперервна випадкова величина (Continuous RV):** Може приймати будь-яке значення з деякого неперервного інтервалу дійсних чисел. Зазвичай це результати вимірювання.
        * *Приклади:*
            * $H$ = зріст людини (може бути, наприклад, будь-яке значення між 150 см і 200 см).
            * $T$ = температура повітря.
            * $E$ = похибка вимірювання датчиком.
            * $L$ = час життя лампочки.

---

**Розподіли ймовірностей**

Розподіл ймовірностей повністю описує випадкову величину, вказуючи, які значення вона може приймати і з якою ймовірністю. Спосіб опису залежить від типу ВВ.

* **Для дискретних ВВ:**
    * **Закон розподілу (Probability Mass Function - PMF):** Це функція $p(x)$, яка для кожного можливого значення $x_i$ дискретної ВВ $X$ дає ймовірність того, що $X$ прийме саме це значення: $p(x_i) = P(X = x_i)$.
    * Закон розподілу часто задають таблицею або формулою.
    * **Властивості PMF:**
        1.  $p(x_i) \ge 0$ для всіх $x_i$.
        2.  $\sum_{i} p(x_i) = 1$ (сума ймовірностей усіх можливих значень дорівнює 1).
    * *Приклад (кидання кубика):* $X$ = число на кубику.
        | $x_i$ | 1 | 2 | 3 | 4 | 5 | 6 |
        |---|---|---|---|---|---|---|
        | $p(x_i)$ | 1/6 | 1/6 | 1/6 | 1/6 | 1/6 | 1/6 |
        Сума $p(x_i) = 6 \times (1/6) = 1$.

* **Для неперервних ВВ:**
    * Ймовірність того, що неперервна ВВ $X$ прийме *одне конкретне* значення, зазвичай дорівнює нулю ($P(X=x) = 0$). Тому ми говоримо про ймовірність потрапляння $X$ у певний *інтервал*.
    * **Щільність ймовірності (Probability Density Function - PDF):** Це функція $f(x)$, яка описує відносну ймовірність того, що ВВ $X$ прийме значення поблизу $x$. Ймовірність потрапляння $X$ в інтервал $[a, b]$ дорівнює **площі під графіком функції $f(x)$** на цьому інтервалі:
        $P(a \le X \le b) = \int_a^b f(x) dx$.
    * **Властивості PDF:**
        1.  $f(x) \ge 0$ для всіх $x$.
        2.  $\int_{-\infty}^{\infty} f(x) dx = 1$ (загальна площа під графіком дорівнює 1).
    * **Важливо:** Значення $f(x)$ саме по собі *не є ймовірністю* і може бути більшим за 1. Ймовірністю є лише *площа* під графіком.
    * *Аналогія:* Уявіть неоднорідний металевий стержень. Щільність $f(x)$ у точці $x$ показує "концентрацію маси". Щоб знайти масу (ймовірність) відрізка $[a, b]$, треба проінтегрувати щільність по цьому відрізку.

* **Функція розподілу (Cumulative Distribution Function - CDF):**
    * Визначається як для дискретних, так і для неперервних ВВ.
    * **Функція розподілу $F(x)$** дає ймовірність того, що випадкова величина $X$ прийме значення, **менше або рівне** $x$: $F(x) = P(X \le x)$.
    * **Властивості CDF:**
        1.  $0 \le F(x) \le 1$.
        2.  $F(x)$ є неспадною функцією (при збільшенні $x$, $F(x)$ може лише зростати або залишатися сталою).
        3.  $F(-\infty) = \lim_{x \to -\infty} F(x) = 0$.
        4.  $F(+\infty) = \lim_{x \to +\infty} F(x) = 1$.
    * Для дискретних ВВ $F(x)$ є східчастою функцією: $F(x) = \sum_{x_i \le x} p(x_i)$.
    * Для неперервних ВВ $F(x)$ є неперервною функцією: $F(x) = \int_{-\infty}^x f(t) dt$. Звідси випливає, що щільність $f(x)$ є похідною від функції розподілу: $f(x) = F'(x)$.

---

**Математичне сподівання та дисперсія**

Це дві найважливіші числові характеристики, що описують розподіл випадкової величини.

* **Математичне сподівання (Expected Value, E[X] або μ):**
    * Це "середнє очікуване" значення випадкової величини, зважене на ймовірності її можливих значень. По суті, це центр мас розподілу.
    * **Для дискретних ВВ:** $E[X] = \mu = \sum_{i} x_i p(x_i)$.
    * **Для неперервних ВВ:** $E[X] = \mu = \int_{-\infty}^{\infty} x f(x) dx$.
    * *Приклад (кидання кубика):*
        $E[X] = 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + 3 \cdot \frac{1}{6} + 4 \cdot \frac{1}{6} + 5 \cdot \frac{1}{6} + 6 \cdot \frac{1}{6} = \frac{1+2+3+4+5+6}{6} = \frac{21}{6} = 3.5$.
        (Хоча ВВ не може прийняти значення 3.5, це її середнє очікуване значення при великій кількості кидків).

* **Дисперсія (Variance, Var(X) або D[X] або σ²):**
    * Вимірює ступінь **розкиду** значень випадкової величини навколо її математичного сподівання. Показує, наскільки сильно значення ВВ "відхиляються" від середнього.
    * **Формальне визначення:** Дисперсія - це математичне сподівання квадрату відхилення ВВ від її математичного сподівання: $Var(X) = E[(X - \mu)^2]$.
    * **Обчислювальна формула (часто зручніша):** $Var(X) = E[X^2] - (E[X])^2$.
        Де $E[X^2] = \sum_{i} x_i^2 p(x_i)$ (для дискретних) або $E[X^2] = \int_{-\infty}^{\infty} x^2 f(x) dx$ (для неперервних).
    * **Властивості:** $Var(X) \ge 0$. $Var(c) = 0$ (для сталої). $Var(X+c) = Var(X)$. $Var(cX) = c^2 Var(X)$.

* **Стандартне відхилення (Standard Deviation, σ):**
    * Це корінь квадратний з дисперсії: $\sigma = \sqrt{Var(X)}$.
    * Перевага: стандартне відхилення вимірюється в тих самих одиницях, що й сама випадкова величина $X$, що робить його більш інтерпретовним показником розкиду.

* **Значення для AI:** Математичне сподівання дає уявлення про "типове" значення ознаки або прогнозу. Дисперсія та стандартне відхилення показують мінливість, шум, невизначеність або ризик, пов'язані з цією величиною. Ці характеристики використовуються для нормалізації даних, оцінки якості моделей, аналізу ризиків.

---

**Основні розподіли**

Існує багато різних типів розподілів, але три з них зустрічаються особливо часто:

1.  **Розподіл Бернуллі (Bernoulli Distribution):**
    * Описує випадковий експеримент з **одним випробуванням** і **двома можливими наслідками**, які умовно називають "успіх" (значення 1) та "невдача" (значення 0).
    * **Параметр:** $p$ – ймовірність "успіху" ($P(X=1) = p$). Тоді ймовірність "невдачі" $P(X=0) = 1-p$.
    * **PMF:** $p(x) = p^x (1-p)^{1-x}$ для $x \in \{0, 1\}$.
    * **Характеристики:** $E[X] = p$, $Var(X) = p(1-p)$.
    * *Приклади:* Одне підкидання монети ($p=0.5$), клік користувача на рекламу ($p$ - ймовірність кліку), результат медичного тесту (позитивний/негативний).
    * *Застосування в AI:* Моделювання бінарних подій, бінарна класифікація.

2.  **Біноміальний розподіл (Binomial Distribution):**
    * Описує **кількість "успіхів" ($k$)** у **фіксованій серії ($n$)** незалежних **випробувань Бернуллі**, де ймовірність "успіху" ($p$) в кожному випробуванні однакова.
    * **Параметри:** $n$ – кількість випробувань, $p$ – ймовірність успіху в одному випробуванні.
    * **PMF:** Ймовірність отримати рівно $k$ успіхів у $n$ випробуваннях:
        $P(X=k) = \binom{n}{k} p^k (1-p)^{n-k}$, де $k = 0, 1, ..., n$.
        $\binom{n}{k} = \frac{n!}{k!(n-k)!}$ – біноміальний коефіцієнт ("кількість способів вибрати $k$ успіхів з $n$ випробувань").
    * **Характеристики:** $E[X] = np$, $Var(X) = np(1-p)$.
    * *Приклади:* Кількість гербів при 10 підкиданнях монети ($n=10, p=0.5$). Кількість бракованих деталей у вибірці з 50 штук ($n=50, p$ - ймовірність браку). Кількість пацієнтів, що одужали, з 20, які приймали ліки ($n=20, p$ - ймовірність одужання).
    * *Застосування в AI:* Аналіз результатів A/B тестів, моделювання кількості подій у фіксованій кількості спроб.

3.  **Нормальний розподіл (Normal Distribution / Гауссів розподіл):**
    * **Найважливіший** неперервний розподіл. Його графік (PDF) має характерну **дзвоноподібну форму**, симетричну відносно середнього.
    * **Параметри:** $\mu$ – математичне сподівання (визначає центр дзвону), $\sigma^2$ – дисперсія (визначає "ширину" дзвону; $\sigma$ – стандартне відхилення). Позначення: $X \sim N(\mu, \sigma^2)$.
    * **PDF:** $f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$. (Формулу знати не обов'язково, важливо розуміти форму та параметри).
    * **Характеристики:** $E[X] = \mu$, $Var(X) = \sigma^2$. Симетричний: середнє = медіана = мода. Повністю визначається середнім та дисперсією.
    * **Центральна гранична теорема (CLT):** Одна з причин його поширеності – сума (або середнє арифметичне) великої кількості незалежних, однаково розподілених випадкових величин (навіть якщо вони самі не є нормальними!) має розподіл, близький до нормального.
    * *Приклади:* Зріст або вага людей, похибки вимірювань, шум у сигналах, багато природних та економічних показників.
    * *Застосування в AI:* **Дуже широке!** Моделювання неперервних ознак, припущення про розподіл помилок у багатьох моделях (лінійна регресія), моделювання шумів, ініціалізація ваг у нейромережах, представлення невизначеності у баєсівських моделях.

---

**Підсумки**

* **Випадкові величини** ($X, Y, ...$) переводять результати випадкових експериментів у числову форму. Бувають **дискретними** та **неперервними**.
* Поведінка ВВ описується **розподілом ймовірностей** (законом розподілу/PMF для дискретних, щільністю/PDF для неперервних, або функцією розподілу/CDF для обох).
* **Математичне сподівання ($E[X]$)** – це середнє очікуване значення, а **дисперсія ($Var(X)$)** та **стандартне відхилення ($\sigma$)** – міри розкиду значень навколо середнього.
* Розподіли **Бернуллі**, **біноміальний** та **нормальний** є фундаментальними і часто використовуються для моделювання різних явищ та в алгоритмах штучного інтелекту.
* Розуміння цих концепцій є критично важливим для аналізу даних, побудови моделей та інтерпретації результатів в умовах невизначеності.

**Наступні кроки**

* Подумайте, які випадкові величини та розподіли могли б описувати дані у знайомих вам сферах (наприклад, оцінки студентів, ціни на товари, погода).
* На наступній лекції ми розглянемо основи статистики – науки про збір, аналіз, інтерпретацію та представлення даних, яка тісно пов'язана з теорією ймовірностей.

---

Дякую за увагу! Я готовий відповісти на ваші запитання.