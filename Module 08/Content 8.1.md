**Лекція 8.1: Навчання без учителя (детальніше)**

**Мета лекції:** Глибше розглянути парадигму навчання без учителя, яку ми згадували раніше. Детально розібрати два фундаментальних завдання та популярні алгоритми для їх вирішення: кластеризацію за допомогою K-Means та зменшення розмірності за допомогою методу головних компонент (PCA).

---

Доброго дня! На одній з попередніх лекцій ми говорили про три основні парадигми машинного навчання. Якщо навчання з учителем використовує розмічені дані (вхід-вихід), а навчання з підкріпленням – систему винагород, то **навчання без учителя (Unsupervised Learning)** має справу з **нерозміченими даними**. Його мета – не передбачити конкретну відповідь, а самостійно **знайти приховані структури, закономірності та взаємозв'язки** в самих даних.

Це надзвичайно важливий напрямок, адже в реальному світі величезні масиви даних не мають готових міток. Тут, у Львові, як і будь-де, нас оточує безліч неструктурованої інформації – тексти, зображення, дані з датчиків. Навчання без учителя допомагає нам розібратися в цих даних, зрозуміти їхню внутрішню організацію.

Сьогодні ми детальніше розглянемо два ключових завдання навчання без учителя:

1.  **Кластеризація (Clustering):** Групування схожих об'єктів разом. Розглянемо алгоритм **K-Means**.
2.  **Зменшення розмірності (Dimensionality Reduction):** Спрощення даних шляхом зменшення кількості ознак. Розглянемо **Метод Головних Компонент (PCA)**.

---

**Кластеризація (Clustering)**

* **Мета:** Розділити набір даних на групи (кластери) таким чином, щоб об'єкти **всередині** одного кластера були максимально **схожими** один на одного, а об'єкти **з різних** кластерів – максимально **відмінними**. Ми не знаємо заздалегідь, які це будуть групи – алгоритм має знайти їх сам.

* **Аналогія:** Уявіть, що вам дали мішок з різними фруктами (яблука, апельсини, банани) і попросили розсортувати їх по кошиках, не кажучи назв фруктів. Ви будете групувати їх за схожістю форми, кольору, розміру. Це і є кластеризація.

* **Алгоритм K-Means (K-Середніх):** Один з найпростіших та найпопулярніших алгоритмів кластеризації.
    * **Вхід:** Набір даних (вектори ознак $x_1, ..., x_m$) та бажана **кількість кластерів $K$**. Значення $K$ є **гіперпараметром** – його обирає користувач.
    * **Вихід:** Належність кожної точки даних до одного з $K$ кластерів та координати центрів цих кластерів (**центроїдів**).
    * **Алгоритм (ітеративний):**
        1.  **Ініціалізація:** Випадковим чином обрати $K$ точок з даних як початкові центроїди кластерів. (Існують кращі методи ініціалізації, як K-Means++, що допомагають уникнути поганих початкових позицій).
        2.  **Крок Призначення (Assignment Step):** Для **кожної** точки даних обчислити відстань до **кожного** з $K$ центроїдів (зазвичай використовується Евклідова відстань). Призначити кожну точку даних до **найближчого** до неї центроїда. Таким чином формуються $K$ початкових кластерів.
        3.  **Крок Оновлення (Update Step):** Для **кожного** кластера перерахувати позицію його центроїда як **середнє арифметичне** (центр мас) усіх точок даних, що були призначені до цього кластера на попередньому кроці.
        4.  **Повторення:** Повторювати Крок 2 (Призначення) та Крок 3 (Оновлення) до тих пір, поки центроїди практично не перестануть зміщуватися, або поки належність точок до кластерів не стабілізується. Алгоритм зійшовся.
    * **Вибір $K$:** Це окрема задача. Часто використовують "метод ліктя" (Elbow Method) – будують графік залежності сумарної внутрішньокластерної варіації від $K$ і шукають точку "зламу" ("лікоть") на графіку. Або використовують інші метрики (напр., силуетний аналіз). Часто вибір $K$ вимагає експертних знань про дані.
    * **Переваги K-Means:**
        * Простота реалізації та інтерпретації.
        * Обчислювальна ефективність, добре масштабується на великі датасети.
        * Добре працює, коли кластери мають приблизно сферичну форму та чітко розділені.
    * **Недоліки K-Means:**
        * Необхідність **заздалегідь вказувати кількість кластерів $K$**.
        * Чутливість до **початкового вибору центроїдів** (може збігатися до локального оптимуму; рекомендується запускати кілька разів з різними ініціалізаціями).
        * Погано працює з кластерами несферичної форми, різного розміру чи щільності.
        * Чутливий до **викидів** (outliers) у даних.
        * Вимагає **масштабування ознак**, оскільки базується на відстанях.
    * **Застосування:** Сегментація клієнтів (маркетинг), кластеризація документів за темами, сегментація зображень (за кольором пікселів), виявлення аномалій (точки, далекі від будь-якого центроїда).

---

**Зменшення розмірності (Dimensionality Reduction)**

* **Мета:** Зменшити кількість ознак (стовпців, вимірів) у наборі даних, намагаючись зберегти якомога більше **важливої інформації** (часто – дисперсії даних).

* **Навіщо це потрібно?**
    * **Боротьба з "прокляттям розмірності":** Багато алгоритмів ML погано працюють або вимагають експоненційно більше даних при дуже великій кількості ознак.
    * **Прискорення обчислень:** Менше ознак – швидше навчання та прогнозування.
    * **Візуалізація даних:** Зменшення розмірності до 2D або 3D дозволяє візуалізувати складні багатовимірні дані на графіку.
    * **Стиснення даних:** Зменшення обсягу пам'яті для зберігання.
    * **Видалення шуму:** Може допомогти усунути надлишкові або шумові ознаки.

* **Метод Головних Компонент (Principal Component Analysis - PCA):** Найпопулярніший **лінійний** метод зменшення розмірності.
    * **Ключова ідея:** Знайти новий набір координатних осей – **Головних Компонент (Principal Components)** – таких, що:
        1.  Вони є **ортогональними** одна до одної (некорельованими).
        2.  Перша головна компонента (PC1) вказує напрямок **максимальної дисперсії** (розкиду) даних.
        3.  Друга головна компонента (PC2) вказує напрямок максимальної **залишкової** дисперсії, будучи ортогональною до PC1.
        4.  І так далі... Кожна наступна компонента захоплює максимальну з решти дисперсії та є ортогональною до всіх попередніх.
    * **Як працює (концептуально):**
        1.  **Стандартизація даних:** Дуже важливо! Перед застосуванням PCA дані потрібно стандартизувати (відцентрувати з нульовим середнім та одиничною дисперсією).
        2.  **Обчислення коваріаційної матриці:** Розрахувати матрицю коваріацій між усіма ознаками.
        3.  **Знаходження власних векторів та власних значень:** Знайти власні вектори та власні значення коваріаційної матриці (згадуємо лінійну алгебру!).
            * **Власні вектори** визначають **напрямки** головних компонент.
            * **Власні значення** вказують **величину дисперсії**, яку "захоплює" кожна головна компонента.
        4.  **Вибір головних компонент:** Відсортувати власні вектори за спаданням відповідних власних значень. Вибрати перші $k$ власних векторів (головних компонент), де $k$ – бажана нова розмірність. $k$ часто вибирають так, щоб зберегти певний відсоток загальної дисперсії (наприклад, 95% або 99%).
        5.  **Проектування даних:** Перетворити (спроектувати) початкові дані на новий підпростір, утворений вибраними $k$ головними компонентами. Це і будуть дані зі зменшеною розмірністю.
    * **Інтерпретація:** Головні компоненти є лінійними комбінаціями початкових ознак. Аналізуючи коефіцієнти (loadings) у цих комбінаціях, іноді можна зрозуміти, які початкові ознаки роблять найбільший внесок у ту чи іншу компоненту (хоча інтерпретація не завжди проста).
    * **Переваги PCA:**
        * Простий та обчислювально ефективний метод (базується на стандартних лінійно-алгебраїчних операціях).
        * Гарантує отримання некорельованих компонент.
        * Ефективний для стиснення даних та видалення лінійного шуму.
        * Широко використовується для візуалізації.
    * **Недоліки PCA:**
        * **Припускає лінійність:** Не може виявити складні нелінійні структури в даних.
        * **Чутливий до масштабування ознак:** Необхідна стандартизація.
        * **Інтерпретованість:** Головні компоненти можуть бути важкими для інтерпретації у термінах початкових ознак.
        * **Не враховує мітки класів:** Це метод без учителя, тому напрямки максимальної дисперсії не обов'язково є найкращими для розділення класів у задачах класифікації (хоча PCA часто використовують як крок попередньої обробки).
    * **Застосування:** Попередня обробка даних перед застосуванням інших алгоритмів ML, візуалізація багатовимірних даних, компресія зображень (напр., метод "власних облич"), шумозаглушення.

---

**Підсумки**

* **Навчання без учителя** дозволяє знаходити приховані структури в нерозмічених даних.
* **Кластеризація** групує схожі об'єкти. **K-Means** – простий та популярний ітеративний алгоритм, що вимагає задання кількості кластерів $K$ і чутливий до ініціалізації та форми кластерів.
* **Зменшення розмірності** спрощує дані, зберігаючи важливу інформацію. **PCA** – основний лінійний метод, що знаходить **головні компоненти** (напрямки максимальної дисперсії) і проектує дані на них. Вимагає стандартизації даних.
* Ці методи є важливими інструментами для **дослідницького аналізу даних (Exploratory Data Analysis - EDA)**, **попередньої обробки даних** та виявлення внутрішньої структури, що може бути корисним навіть перед застосуванням методів навчання з учителем.

Розуміння цих підходів розширює наш погляд на можливості машинного навчання поза задачами прямого прогнозування за мітками.

**Наступні кроки**

* Подумайте, як би ви могли застосувати кластеризацію до даних про відвідувачів веб-сайту або покупців у магазині.
* Уявіть, що у вас є датасет з багатьма корельованими ознаками (напр., різні вимірювання розміру будинку). Як PCA міг би допомогти спростити ці дані?
* Після цього детальнішого погляду на навчання без учителя, ми повернемося до методів навчання з учителем, а саме до більш просунутих класичних алгоритмів, таких як SVM, або продовжимо занурення в глибоке навчання.

---

Дякую за увагу! Я готовий відповісти на ваші запитання.