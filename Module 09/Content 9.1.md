**Лекція 9.1: Класичні алгоритми машинного навчання**

**Мета лекції:** Розширити арсенал методів машинного навчання за межі лінійних моделей. Познайомити слухачів з інтуїтивно зрозумілими та потужними алгоритмами на основі дерев: Деревами рішень, їхнім ансамблевим покращенням – Випадковими лісами, та оглянути концепцію Градієнтного бустингу.

---

Доброго дня! На попередніх лекціях ми розглянули лінійну та логістичну регресію – чудові інструменти, але вони базуються на припущенні про лінійну залежність між ознаками та цільовою змінною (або лінійну межу між класами). Однак реальні дані часто мають значно складнішу, нелінійну структуру.

Сьогодні ми познайомимося з класом алгоритмів, які чудово справляються з нелінійностями і є надзвичайно популярними, особливо при роботі з **табличними даними** – це моделі на основі дерев рішень. Ми розглянемо:

1.  **Дерева рішень (Decision Trees):** Базовий будівельний блок, інтуїтивно зрозумілий та інтерпретований.
2.  **Випадкові ліси (Random Forests):** Як об'єднання багатьох дерев допомагає створити значно потужнішу модель?
3.  **Градієнтний бустинг (Gradient Boosting):** Ще один підхід до ансамблювання дерев (оглядово).

Ці алгоритми часто є першим вибором для багатьох практичних задач класифікації та регресії завдяки їхній гнучкості та високій точності.

---

**Дерева рішень (Decision Trees)**

* **Ідея:** Дерево рішень – це модель, що нагадує блок-схему або гру "вгадай об'єкт за 20 запитань". Вона послідовно ставить "запитання" (робить перевірки) щодо ознак вхідного об'єкта, щоб дійти до кінцевого висновку – прогнозу.

* **Структура дерева:**
    * **Кореневий вузол (Root Node):** Верхній вузол, що представляє всю вибірку даних на початку.
    * **Внутрішні вузли (Internal Nodes):** Кожен такий вузол представляє **перевірку (тест)** однієї з ознак за певним **порогом**. Наприклад, "Чи є вік < 30 років?" або "Чи є площа > 50 м²?".
    * **Гілки (Branches):** Виходять з внутрішніх вузлів і представляють **результати** перевірки (напр., "Так"/"Ні", "True"/"False"). Кожна гілка веде до наступного вузла.
    * **Листові вузли (Leaf Nodes / Terminal Nodes):** Кінцеві вузли дерева. Вони **не мають** вихідних гілок і містять **фінальний прогноз**:
        * У задачах **класифікації:** Мітка класу (найбільш частого в цьому листі).
        * У задачах **регресії:** Числове значення (зазвичай середнє значення цільової змінної для всіх прикладів, що потрапили в цей лист).

* **Приклад (Класифікація):** Уявімо, ми хочемо спрогнозувати, чи купить користувач товар ("Купить"/"Не купить") на основі його віку та міста проживання. Дерево може виглядати так:
    1.  **(Кореневий вузол):** Перевірка: `Вік < 35?`
    2.  **--> (Гілка "Так"):** Перевірка: `Місто = "Львів"?`
    3.  **----> (Гілка "Так"): (Лист)** Прогноз: "Купить".
    4.  **----> (Гілка "Ні"): (Лист)** Прогноз: "Не купить".
    5.  **--> (Гілка "Ні"): (Лист)** Прогноз: "Не купить".
    Для нового користувача ми просто проходимо шлях по дереву відповідно до його ознак, доки не дійдемо до листового вузла з прогнозом.

* **Як будується дерево (інтуїція):** Алгоритм намагається знайти таку послідовність перевірок (розбиттів даних), яка найкращим чином розділяє дані на "чисті" групи (в ідеалі, щоб у кожному листовому вузлі були приклади лише одного класу).
    * На кожному кроці алгоритм вибирає **найкращу ознаку** та **найкращий поріг** для розбиття даних у поточному вузлі.
    * "Найкраще" розбиття – це те, яке максимально зменшує "нечистоту" (impurity) або збільшує "однорідність" дочірніх вузлів порівняно з батьківським.
    * **Критерії розбиття (для класифікації):**
        * **Нечистота Джині (Gini Impurity):** Вимірює ймовірність неправильної класифікації випадково вибраного елемента з вузла. Чим нижче значення, тим "чистіший" вузол.
        * **Ентропія (Entropy) / Приріст інформації (Information Gain):** Ентропія вимірює хаос або невизначеність у вузлі. Алгоритм вибирає розбиття, яке дає максимальне зменшення ентропії (найбільший приріст інформації).
    * **Зупинка росту:** Дерево перестає рости, коли виконується одна з умов: досягнута максимальна глибина, у вузлі залишилося занадто мало прикладів, усі приклади у вузлі належать до одного класу, або подальше розбиття не дає суттєвого покращення.

* **Переваги Дерев рішень:**
    * **Інтерпретованість:** Легко візуалізувати та зрозуміти логіку прийняття рішень ("біла скринька").
    * Добре працюють з різними типами даних (числовими, категоріальними).
    * Не вимагають масштабування ознак.
    * Здатні моделювати нелінійні залежності.

* **Недоліки Дерев рішень:**
    * **Схильність до перенавчання:** Легко будують дуже складні дерева, які ідеально підлаштовуються під навчальні дані, але погано працюють на нових (висока дисперсія). Потребують обмеження глибини або "обрізання" (pruning).
    * **Нестабільність:** Невеликі зміни у вхідних даних можуть призвести до побудови зовсім іншого дерева.
    * Можуть створювати упереджені дерева, якщо деякі класи значно переважають інші.

---

**Випадкові ліси (Random Forests)**

Щоб подолати недоліки окремих дерев рішень (особливо перенавчання та нестабільність), був запропонований метод **ансамблювання** – об'єднання багатьох дерев у "ліс".

* **Ідея ансамблювання:** "Мудрість натовпу". Об'єднати прогнози багатьох різних (і відносно незалежних) моделей, щоб отримати більш точний та стабільний загальний прогноз.

* **Як працює Випадковий ліс:**
    1.  **Беггінг (Bagging = Bootstrap Aggregating):**
        * Створюється багато ($N$, напр., 100 або 1000) **вибірок з поверненням (bootstrap samples)** з початкового навчального датасету. Кожна така вибірка має той самий розмір, що й оригінальна, але деякі приклади можуть повторюватися, а деякі – бути відсутніми.
    2.  **Навчання багатьох дерев:** На кожній bootstrap-вибірці навчається **окреме дерево рішень**.
    3.  **Випадковість ознак:** **Ключовий момент!** При побудові кожного дерева, на кожному кроці розбиття вузла розглядається не весь набір ознак, а лише **випадкова підмножина** ознак (напр., $\sqrt{k}$ з $k$ доступних ознак). Це робить дерева більш різноманітними та **декорельованими**.
    4.  **Агрегація прогнозів:** Щоб отримати фінальний прогноз для нового прикладу:
        * **Класифікація:** Кожне дерево "голосує" за свій клас. Перемагає клас, що набрав **більшість голосів**.
        * **Регресія:** Береться **середнє арифметичне** прогнозів усіх дерев у лісі.

* **Чому це працює?**
    * **Зменшення перенавчання:** Усереднення прогнозів багатьох різних дерев (з високою дисперсією, але низьким зміщенням) значно зменшує загальну дисперсію ансамблю.
    * **Стабільність:** Модель стає значно менш чутливою до конкретного складу навчальної вибірки.
    * **Висока точність:** Зазвичай значно перевершує точність окремого дерева рішень.

* **Переваги Випадкових лісів:**
    * Висока точність прогнозування на багатьох типах задач.
    * Стійкість до перенавчання.
    * Добре працює з великою кількістю ознак.
    * Нечутливий до масштабування ознак.
    * Дозволяє оцінити **важливість ознак** (feature importance).

* **Недоліки Випадкових лісів:**
    * **Втрата інтерпретованості:** "Ліс" з сотень дерев значно важче інтерпретувати, ніж одне дерево.
    * Більш ресурсоємний для навчання та прогнозування, ніж одне дерево.
    * Може не дуже добре працювати на дуже розріджених даних.

---

**Градієнтний бустинг (Gradient Boosting - огляд)**

Це ще один потужний метод ансамблювання, але він працює інакше, ніж Випадковий ліс.

* **Ідея бустингу:** Будувати ансамбль **послідовно**. Кожна наступна модель (зазвичай просте дерево) намагається **виправити помилки**, зроблені попередніми моделями.
* **Як працює (концептуально):**
    1.  Починаємо з простої моделі (напр., що прогнозує середнє значення).
    2.  Обчислюємо помилки (різницю між прогнозами та реальними значеннями) поточної моделі.
    3.  Навчаємо **нове слабке дерево** (зазвичай неглибоке), яке намагається **передбачити ці помилки**.
    4.  Додаємо прогноз цього нового дерева (з певним невеликим коефіцієнтом – швидкістю навчання) до загального прогнозу ансамблю.
    5.  Повторюємо кроки 2-4 багато разів (будуємо багато дерев), причому кожне наступне дерево фокусується на тих прикладах, де попередні моделі помилялися найбільше.
* **Відомі реалізації:** AdaBoost, **Gradient Boosting Machines (GBM)**, та їхні надзвичайно популярні та оптимізовані версії: **XGBoost**, **LightGBM**, **CatBoost**.
* **Переваги:**
    * Часто досягає **найвищої точності** на структурованих (табличних) даних, перевершуючи навіть випадкові ліси.
* **Недоліки:**
    * **Більш чутливий до налаштування гіперпараметрів** (швидкість навчання, глибина дерев, кількість дерев), ніж випадковий ліс. Потребує ретельної настройки.
    * Більш схильний до перенавчання, якщо не налаштувати правильно.
    * Навчання є послідовним, що може бути повільнішим (хоча сучасні бібліотеки добре оптимізовані).

---

**Підсумки**

* **Дерева рішень** – інтуїтивно зрозумілі, інтерпретовані моделі, що добре працюють з нелінійними даними, але схильні до перенавчання.
* **Випадкові ліси** – потужний ансамбль, що комбінує багато декорельованих дерев (навчених на bootstrap-вибірках з випадковими підмножинами ознак) для зменшення перенавчання та покращення точності.
* **Градієнтний бустинг** – інший тип ансамблю, де моделі будуються послідовно, виправляючи помилки попередніх. Часто дає найкращі результати на табличних даних, але вимагає ретельнішого налаштування.
* Ці методи є надзвичайно важливими інструментами в арсеналі фахівця з машинного навчання, особливо при роботі з табличними даними, і є чудовою альтернативою або доповненням до лінійних моделей та нейронних мереж.

**Наступні кроки**

* Подумайте, для якої з розглянутих раніше задач (напр., класифікація спаму) краще підійшло б дерево рішень, а для якої – лінійна модель? Чому?
* Як концепція ансамблювання може бути застосована не лише до дерев рішень?
* Тепер, маючи уявлення про лінійні моделі та моделі на основі дерев, ми готові глибше зануритися у світ глибокого навчання та нейронних мереж, починаючи з їхніх базових будівельних блоків на наступній лекції.

---

Дякую за увагу! Я готовий відповісти на ваші запитання.