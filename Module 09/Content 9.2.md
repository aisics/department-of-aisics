**Лекція 9.2: Інші методи навчання з учителем**

**Мета лекції:** Продовжити знайомство з класичними, але важливими алгоритмами навчання з учителем. Розглянути інтуїтивно зрозумілий метод K-найближчих сусідів (KNN) та потужний метод опорних векторів (SVM), їхні основні принципи роботи, переваги та недоліки.

---

Доброго дня! Минулого разу ми розглянули моделі, що базуються на деревах рішень, які добре справляються з нелінійними залежностями в даних. Сьогодні ми продовжимо розширювати наш арсенал методів навчання з учителем і познайомимося ще з двома дуже різними, але впливовими підходами:

1.  **Метод K-найближчих сусідів (K-Nearest Neighbors - KNN):** Простий, інтуїтивний метод, що базується на схожості між об'єктами.
2.  **Метод опорних векторів (Support Vector Machines - SVM):** Потужний метод, що шукає оптимальну межу між класами.

Ці методи, хоч і належать до "класичного" машинного навчання, все ще активно використовуються і є важливими для розуміння різних підходів до моделювання даних.

---

**Метод K-найближчих сусідів (K-Nearest Neighbors - KNN)**

* **Основна ідея:** Дуже проста і життєва: "Подібні об'єкти знаходяться поруч". Щоб класифікувати новий, невідомий об'єкт, ми дивимося на його **найближчих сусідів** у просторі ознак серед тих об'єктів, які ми вже бачили (навчальна вибірка), і присвоюємо йому той клас, який **найчастіше зустрічається** серед цих сусідів.

* **Тип алгоритму:** KNN належить до **інстанс-базованих (instance-based)** або **"лінивих" (lazy learning)** алгоритмів. Він не будує явну модель під час "навчання" – фактично, фаза навчання полягає лише у **запам'ятовуванні** всіх даних навчальної вибірки. Вся основна обчислювальна робота відбувається під час **прогнозування**.

* **Як працює KNN для класифікації:**
    1.  **Запам'ятати дані:** Зберегти всі навчальні приклади разом з їхніми мітками класів.
    2.  **Отримати новий об'єкт:** Взяти новий об'єкт $x_{new}$, для якого потрібно зробити прогноз.
    3.  **Обчислити відстані:** Знайти відстань від $x_{new}$ до **кожного** об'єкта у навчальній вибірці. Вибір метрики відстані залежить від типу даних:
        * **Евклідова відстань:** Найпоширеніша для числових ознак (звичайна "пряма" відстань).
        * **Манхеттенська відстань:** Сума модулів різниць по координатах.
        * **Відстань Хеммінга:** Для категоріальних ознак (кількість позицій, де значення не збігаються).
    4.  **Знайти K сусідів:** Вибрати $k$ об'єктів з навчальної вибірки, які мають **найменшу** відстань до $x_{new}$. Число $k$ – це **гіперпараметр**, який ми задаємо самі (напр., $k=3, k=5, k=7$).
    5.  **Провести "голосування":** Подивитися на мітки класів цих $k$ найближчих сусідів. Присвоїти об'єкту $x_{new}$ той клас, який має **більшість голосів** серед його $k$ сусідів. (Для уникнення нічиїх часто обирають непарне $k$).

* **KNN для регресії:** Замість голосування класами, прогнозом для $x_{new}$ буде **середнє** (або медіана) значень цільової змінної його $k$ найближчих сусідів.

* **Вибір $k$:**
    * Мале $k$ (напр., $k=1$): Модель дуже чутлива до шумів, межа рішення може бути дуже "ламаною". Низьке зміщення, висока дисперсія.
    * Велике $k$: Межа рішення стає більш гладкою, модель менш чутлива до шумів, але може "розмивати" локальні патерни. Високе зміщення, низька дисперсія.
    * Оптимальне $k$ зазвичай підбирають за допомогою крос-валідації.

* **Переваги KNN:**
    * Дуже простий для розуміння та реалізації.
    * Не потребує явного етапу навчання.
    * Легко адаптується до нових даних (просто додаємо їх до бази).
    * Добре працює на задачах з нелінійними межами рішень.

* **Недоліки KNN:**
    * **Обчислювально дорогий на етапі прогнозування:** Треба обчислювати відстані до всіх навчальних точок. Дуже повільний для великих датасетів.
    * **Дуже чутливий до масштабування ознак:** Ознаки з великими значеннями будуть домінувати у відстані. **Обов'язково** потрібно масштабувати (нормалізувати або стандартизувати) дані перед використанням KNN.
    * **"Прокляття розмірності":** Погано працює, коли ознак дуже багато (у багатовимірному просторі поняття "близькості" стає менш значущим).
    * Потребує вибору відповідної метрики відстані.
    * Необхідно зберігати весь навчальний датасет.

---

**Метод опорних векторів (Support Vector Machines - SVM)**

Це значно складніший, але й часто потужніший метод, особливо популярний до розквіту глибокого навчання.

* **Основна ідея (для класифікації):** Знайти таку **оптимальну розділяючу гіперплощину** (лінію у 2D, площину у 3D, гіперплощину у вищих розмірностях), яка найкращим чином розділяє класи даних, максимізуючи при цьому **зазор (margin)** між цією гіперплощиною та найближчими точками кожного класу.

* **Ключові поняття:**
    * **Розділяюча гіперплощина (Separating Hyperplane):** Лінія/площина, що розділяє класи.
    * **Зазор (Margin):** Відстань від гіперплощини до найближчих точок кожного класу. SVM прагне **максимізувати** цей зазор. Інтуїція: чим ширший зазор, тим впевненіше розділення і тим краще модель, ймовірно, узагальнить результат на нових даних.
    * **Опорні вектори (Support Vectors):** Саме ті точки даних, які лежать **найближче** до розділяючої гіперплощини (на межі зазору). Саме ці точки **визначають** положення та орієнтацію оптимальної гіперплощини. Всі інші точки не впливають на рішення (якщо їх видалити, гіперплощина не зміниться). Це робить SVM ефективним з точки зору пам'яті.

* **Лінійний SVM:** Якщо дані можна ідеально розділити лінійно, SVM знаходить єдину гіперплощину з максимальним зазором.

* **Нелінійна класифікація – "Ядровий трюк" (Kernel Trick):**
    * **Проблема:** Що робити, якщо дані не можна розділити прямою лінією чи площиною?
    * **Ідея:** Спробувати перетворити дані, спроектувавши їх у **простір вищої розмірності**, де вони, можливо, *стануть* лінійно роздільними.
    * **Ядровий трюк:** Геніальний математичний прийом, який дозволяє SVM обчислювати скалярні добутки (що визначають подібність та відстані) між точками так, **ніби вони знаходяться у цьому просторі вищої розмірності**, але **без явного обчислення координат** у цьому новому просторі. Це значно економить обчислення.
    * **Ядрові функції (Kernels):** Функції, що обчислюють цю "подібність" у неявному просторі. Популярні ядра:
        * **Лінійне:** $K(x, y) = x^T y$ (звичайний скалярний добуток – для лінійно роздільних даних).
        * **Поліноміальне:** $K(x, y) = (\gamma x^T y + r)^d$ (для поліноміальних меж).
        * **Радіально-базисна функція (RBF) / Гауссове ядро:** $K(x, y) = \exp(-\gamma ||x - y||^2)$ (дуже гнучке та популярне, може створювати складні нелінійні межі). $\gamma$ – гіперпараметр, що контролює "ширину" впливу кожної точки.

* **М'який зазор (Soft Margin):** На практиці дані рідко ідеально роздільні. SVM дозволяє деяким точкам порушувати межу зазору або навіть бути неправильно класифікованими. Ступінь "м'якості" контролюється **гіперпараметром C** (параметр регуляризації). Малий C – ширший зазор, більше порушень; великий C – вужчий зазор, менше порушень (але потенційно перенавчання).

* **SVM для регресії (SVR):** Ідея адаптована і для регресії. Замість максимізації зазору між класами, SVR намагається знайти функцію, навколо якої якомога більше точок потрапляє у "трубку" певної ширини (зазору $\epsilon$), мінімізуючи при цьому помилки для точок поза трубкою та складність самої функції.

* **Переваги SVM:**
    * Ефективний у просторах високої розмірності (коли ознак багато).
    * Працює добре, навіть якщо розмірність більша за кількість прикладів.
    * Ефективний з точки зору пам'яті (використовує лише опорні вектори).
    * Гнучкий завдяки різним ядровим функціям (моделює нелінійні залежності).
    * Часто забезпечує високу точність класифікації.

* **Недоліки SVM:**
    * Обчислювально складний для тренування на дуже великих датасетах.
    * Вибір ядра та його гіперпараметрів ($C, \gamma$ для RBF) є критичним і вимагає ретельного підбору (напр., за допомогою крос-валідації).
    * Результат менш інтерпретований ("чорна скринька"), особливо з нелінійними ядрами.
    * Потребує масштабування ознак.
    * Базова версія – для бінарної класифікації (мультикласова реалізується через комбінацію бінарних).

---

**Підсумки**

* **KNN** – простий, інтуїтивний "лінивий" алгоритм, що класифікує об'єкти на основі їхніх найближчих сусідів у просторі ознак. Вимагає масштабування даних та може бути повільним на етапі прогнозу.
* **SVM** – потужний метод, що шукає оптимальну розділяючу гіперплощину з максимальним зазором між класами. Використовує **опорні вектори** та **ядровий трюк** для ефективної роботи з нелінійними даними та у високих розмірностях. Вимагає ретельного налаштування гіперпараметрів.
* Обидва ці методи є важливими представниками класичного машинного навчання і можуть бути ефективними інструментами для певних типів задач та даних, доповнюючи лінійні моделі, дерева рішень та нейронні мережі.

Знання різних алгоритмів дозволяє вибирати найкращий інструмент для конкретної задачі.

**Наступні кроки**

* Подумайте, яка метрика відстані була б доречною для KNN, якби ваші ознаки були текстовими (напр., наявність певних слів)?
* Як ви гадаєте, чому SVM може бути ефективним у високих розмірностях, де KNN має проблеми?
* Тепер, коли ми розглянули широкий спектр класичних методів та основи нейронних мереж, ми готові перейти до більш глибокого вивчення сучасних архітектур глибокого навчання та їх застосувань.

---

Дякую за увагу! Я готовий відповісти на ваші запитання.