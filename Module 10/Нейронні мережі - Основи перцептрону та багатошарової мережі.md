**Лекція 10: Нейронні мережі - Основи перцептрону та багатошарової мережі**

**Мета лекції:** Ознайомити слухачів з базовими концепціями штучних нейронних мереж (ANN), починаючи від біологічного натхнення та моделі штучного нейрона (перцептрона). Розглянути роль функцій активації, архітектуру простої багатошарової мережі (MLP) та процес прямого поширення сигналу.

---

Доброго дня! На попередніх лекціях ми розглянули лінійні моделі – лінійну та логістичну регресію. Це потужні інструменти, але вони мають обмеження, особливо коли залежності в даних є складними та нелінійними. Сьогодні ми починаємо вивчати **штучні нейронні мережі (Artificial Neural Networks - ANN)** – клас моделей, натхненних роботою людського мозку, які здатні вивчати надзвичайно складні закономірності в даних і досягли вражаючих результатів у таких сферах, як розпізнавання зображень, обробка природної мови та багато інших.

Ми розглянемо самі основи:

1.  **Від біологічного до штучного нейрона:** Як влаштований "будівельний блок" мережі?
2.  **Функції активації:** Навіщо потрібна нелінійність?
3.  **Багатошарові мережі:** Як нейрони об'єднуються в структуру?
4.  **Пряме поширення:** Як мережа обробляє вхідні дані для отримання результату?

---

**Біологічний нейрон та штучний нейрон (перцептрон)**

* **Біологічне натхнення (дуже спрощено):**
    Людський мозок складається з мільярдів **нейронів**, з'єднаних між собою. Кожен нейрон має:
    * **Дендрити:** Отримують сигнали від інших нейронів.
    * **Тіло клітини (Сома):** Обробляє (сумує) отримані сигнали.
    * **Аксон:** Передає вихідний сигнал далі.
    * **Синапси:** Місця з'єднання між аксоном одного нейрона та дендритом іншого. Сила зв'язку (синаптична вага) може змінюватись – це основа навчання.
    Нейрон "активується" (посилає сигнал по аксону), якщо сумарний вхідний сигнал перевищує певний поріг.

* **Штучний нейрон (Перцептрон / Одиниця):**
    Це математична модель, що імітує основні функції біологічного нейрона.
    * **Входи ($x_1, x_2, ..., x_n$):** Числові значення, що надходять до нейрона. Це можуть бути ознаки вхідних даних або виходи нейронів попереднього шару.
    * **Ваги ($w_1, w_2, ..., w_n$):** Числа, що присвоюються кожному входу. Вони представляють "силу" або "важливість" кожного вхідного сигналу (аналог синаптичної ваги). **Саме ваги змінюються під час навчання мережі.**
    * **Зміщення (Bias, $b$ або $w_0$):** Додатковий параметр, який додається до суми зважених входів. Він дозволяє нейрону легше активуватися або, навпаки, важче (зсуває поріг активації). Робить модель гнучкішою.
    * **Суматор:** Обчислює **зважену суму** входів плюс зміщення:
        $z = (w_1 x_1 + w_2 x_2 + ... + w_n x_n) + b = \sum_{i=1}^{n} w_i x_i + b$
        У векторній формі (якщо додати $x_0 = 1$ і включити $b$ як $w_0$): $z = w^T x$. (Знову бачимо лінійну комбінацію, як у попередніх моделях).
    * **Функція активації ($g(z)$):** Нелінійна функція, яка застосовується до зваженої суми $z$. Визначає вихідний сигнал нейрона $a$.
        $a = g(z)$

    **Таким чином, штучний нейрон обчислює зважену суму своїх входів, додає зміщення, а потім застосовує функцію активації.**

---

**Функції активації (Activation Functions)**

* **Навіщо потрібні?** Функції активації вводять **нелінійність** у нейронну мережу. Це **критично важливо!** Якби всі нейрони використовували лише лінійну активацію (або не використовували її взагалі), то вся мережа, незалежно від кількості шарів, поводилася б як звичайна лінійна модель (лінійна комбінація лінійних комбінацій є лінійною комбінацією). Саме нелінійні функції активації дозволяють нейронним мережам апроксимувати складні, нелінійні залежності в даних.

* **Популярні функції активації:**

    1.  **Сигмоїда (Sigmoid):** $g(z) = \frac{1}{1 + e^{-z}}$
        * Вихідний діапазон: (0, 1). S-подібна крива.
        * Використання: Раніше часто використовувалась у прихованих шарах, зараз переважно у вихідному шарі для задач бінарної класифікації (вихід інтерпретується як ймовірність).
        * Недоліки: Проблема "затухаючих градієнтів" (vanishing gradients) – похідна дуже мала при великих $|z|$, що сповільнює навчання глибоких мереж; вихід не центрується навколо нуля.

    2.  **Гіперболічний тангенс (Tanh):** $g(z) = \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$
        * Вихідний діапазон: (-1, 1). Також S-подібна крива.
        * Переваги: Вихід центрований навколо нуля, що може прискорити навчання порівняно з сигмоїдою.
        * Недоліки: Проблема затухаючих градієнтів залишається.

    3.  **ReLU (Rectified Linear Unit):** $g(z) = \max(0, z)$
        * Вихідний діапазон: [0, +∞). Графік: 0 для $z<0$, і пряма $y=z$ для $z \ge 0$.
        * Переваги: **Дуже популярна** для прихованих шарів. Обчислювально ефективна. Значно зменшує проблему затухаючих градієнтів для позитивних значень $z$.
        * Недоліки: Проблема "вмираючого ReLU" (нейрон може перестати активуватися, якщо його вхід завжди від'ємний); вихід не центрований навколо нуля.

    4.  **Leaky ReLU та її варіанти:** $g(z) = \max(\alpha z, z)$, де $\alpha$ - мале число (наприклад, 0.01).
        * Вирішує проблему "вмираючого ReLU", дозволяючи невеликий ненульовий градієнт для від'ємних $z$.

    * **Інші:** Існують інші функції (ELU, Swish, Softmax для вихідного шару мультикласової класифікації тощо). Вибір залежить від задачі, архітектури мережі та експериментальних результатів. ReLU та її варіанти є гарною відправною точкою для прихованих шарів.

---

**Архітектура багатошарової нейронної мережі (Multi-Layer Perceptron - MLP)**

Один нейрон може виконувати лише прості завдання. Щоб моделювати складні залежності, нейрони об'єднують у шари, а шари – у мережу. MLP – це класичний тип нейронної мережі прямого поширення.

* **Шари (Layers):**
    1.  **Вхідний шар (Input Layer):** Не містить обчислювальних нейронів. Просто приймає вектор вхідних ознак $x = (x_1, ..., x_n)$ і передає ці значення нейронам першого прихованого шару. Кількість вузлів у вхідному шарі дорівнює кількості ознак вхідних даних.
    2.  **Приховані шари (Hidden Layers):** Один або декілька шарів між вхідним та вихідним. Кожен прихований шар складається з певної кількості нейронів. Нейрони кожного шару отримують сигнали від нейронів попереднього шару, виконують обчислення (зважена сума + функція активації) і передають результати далі. Саме в прихованих шарах відбувається виявлення складних закономірностей та представлень даних. Мережі з одним чи більше прихованими шарами називають **глибокими нейронними мережами (Deep Neural Networks)**.
    3.  **Вихідний шар (Output Layer):** Останній шар мережі, який генерує кінцевий результат (прогноз). Кількість нейронів та тип функції активації у вихідному шарі залежать від задачі:
        * *Регресія:* Зазвичай один нейрон з лінійною функцією активації (або без неї).
        * *Бінарна класифікація:* Один нейрон з сигмоїдною функцією активації (вихід - ймовірність класу 1).
        * *Мультикласова класифікація:* $K$ нейронів (де $K$ - кількість класів) з функцією активації **Softmax** (виходи - ймовірності належності до кожного з $K$ класів, що сумуються до 1).

* **З'єднання:** В MLP зазвичай використовуються **повнозв'язні (fully connected / dense)** шари: кожен нейрон одного шару з'єднаний з кожним нейроном наступного шару. Кожне таке з'єднання має свою вагу.

* **Параметри мережі:** Всі ваги та зміщення всіх нейронів у всіх шарах є параметрами, які мережа вивчає під час тренування. У глибоких мережах таких параметрів можуть бути мільйони або навіть мільярди.

---

**Пряме поширення (Forward Propagation)**

Це процес обчислення вихідного сигналу нейронної мережі для заданого вхідного прикладу $x$. Інформація рухається **вперед** через мережу, від вхідного шару до вихідного.

* **Кроки (для мережі з L шарів, де шар 0 – вхідний):**
    1.  **Вхід:** Подати вектор ознак $x$ на вхідний шар. Активація вхідного шару $a^{[0]} = x$.
    2.  **Для кожного шару $l$ від 1 до $L$:**
        a.  **Обчислити зважену суму ($z^{[l]}$):** Кожен нейрон шару $l$ обчислює зважену суму виходів (активацій $a^{[l-1]}$) нейронів попереднього шару $(l-1)$ плюс своє зміщення. У матричному вигляді для всього шару:
            $z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]}$
            де $W^{[l]}$ – матриця ваг для шару $l$ (рядки відповідають нейронам шару $l$, стовпці – нейронам шару $l-1$), $b^{[l]}$ – вектор зміщень для шару $l$.
        b.  **Застосувати функцію активації ($a^{[l]}$):** До кожного елемента вектора $z^{[l]}$ застосовується функція активації $g^{[l]}$, прийнята для цього шару:
            $a^{[l]} = g^{[l]}(z^{[l]})$
            Вектор $a^{[l]}$ є виходом (активацією) шару $l$ і входом для наступного шару $l+1$.
    3.  **Вихід мережі:** Активація останнього (вихідного) шару $a^{[L]}$ є кінцевим прогнозом мережі $\hat{y}$.
        $\hat{y} = a^{[L]}$

* **Приклад:** Уявіть мережу: вхід (2 ознаки) -> 1 прихований шар (3 нейрони, ReLU) -> вихід (1 нейрон, Sigmoid).
    1.  Вхід $x = (x_1, x_2)$. $a^{[0]} = x$.
    2.  Прихований шар: $z^{[1]} = W^{[1]} a^{[0]} + b^{[1]}$. $a^{[1]} = ReLU(z^{[1]})$. $a^{[1]}$ - вектор з 3 елементів.
    3.  Вихідний шар: $z^{[2]} = W^{[2]} a^{[1]} + b^{[2]}$. $\hat{y} = a^{[2]} = Sigmoid(z^{[2]})$. $\hat{y}$ - одне число (ймовірність).

Пряме поширення – це лише обчислювальна частина роботи мережі. Воно дозволяє отримати прогноз для заданого входу. Процес **навчання** мережі (тобто знаходження оптимальних ваг $W^{[l]}$ та зміщень $b^{[l]}$) відбувається за допомогою алгоритму **зворотного поширення помилки (Backpropagation)** та градієнтного спуску, про які ми поговоримо наступного разу.

---

**Підсумки**

* Штучні нейронні мережі натхненні біологічними нейронами, але є їх математичними моделями.
* Базовий елемент – **штучний нейрон (перцептрон)**, який обчислює зважену суму входів, додає зміщення і застосовує **функцію активації**.
* **Функції активації (Sigmoid, Tanh, ReLU та ін.)** вводять необхідну **нелінійність**, дозволяючи мережам вивчати складні залежності.
* Нейрони організовуються в **шари (вхідний, приховані, вихідний)**, утворюючи **багатошарові мережі (MLP)**.
* **Пряме поширення** – це процес обчислення виходу мережі шляхом послідовної передачі та обробки сигналу від входу до виходу через усі шари.

Нейронні мережі – це потужний інструмент, що об'єднує ідеї лінійних моделей, оптимізації та складних нелінійних перетворень для вирішення широкого кола задач ШІ.

**Наступні кроки**

* Подумайте, чим архітектура MLP відрізняється від лінійної чи логістичної регресії. В чому її потенційна перевага?
* На наступній лекції ми розберемося, як нейронні мережі навчаються – розглянемо алгоритм зворотного поширення помилки (Backpropagation).

---

Дякую за увагу! Не забуваємо про вподобайку, поширення та чесний відгук чи запитання в коментарях, до зустрічі!