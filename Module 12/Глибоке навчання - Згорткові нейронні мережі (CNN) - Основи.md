**Лекція 12: Глибоке навчання - Згорткові нейронні мережі (CNN) - Основи**

**Мета лекції:** Познайомити слухачів з основами згорткових нейронних мереж (Convolutional Neural Networks - CNN), які є ключовою архітектурою глибокого навчання, особливо успішною в задачах комп'ютерного зору. Розглянути будівельні блоки CNN: згорткові шари, операцію згортки, фільтри та шари пулінгу.

---

Доброго дня! На попередніх лекціях ми вивчили основи багатошарових перцептронів (MLP) – нейронних мереж, де кожен нейрон одного шару з'єднаний з кожним нейроном наступного. Такі мережі добре працюють на структурованих даних, але мають суттєві недоліки при роботі з даними високої розмірності, такими як **зображення**. По-перше, кількість параметрів (ваг) стає надзвичайно великою. По-друге, MLP ігнорують **просторову структуру** зображення (те, що пікселі, розташовані поруч, зазвичай пов'язані між собою).

**Згорткові нейронні мережі (Convolutional Neural Networks - CNN або ConvNets)** були розроблені, щоб подолати ці проблеми. Вони використовують спеціальні операції, які враховують просторову ієрархію в даних і дозволяють ефективно виявляти локальні ознаки (лінії, кути, текстури), а потім комбінувати їх для розпізнавання більш складних об'єктів. CNN здійснили революцію в комп'ютерному зорі та використовуються у розпізнаванні об'єктів, сегментації зображень, аналізі медичних знімків та багатьох інших задачах.

Сьогодні ми розберемо ключові компоненти CNN:

1.  **Операція згортки:** Що це таке і як вона працює?
2.  **Згорткові шари:** Як операція згортки використовується в мережі (фільтри, крок, доповнення).
3.  **Шари пулінгу:** Як зменшити розмірність даних, зберігши важливу інформацію?

---

**Операція згортки (Convolution Operation)**

Це фундаментальна операція в CNN, яка дозволяє виявляти локальні ознаки в даних.

* **Вхідні дані:** Зазвичай це зображення, представлене як 2D матриця пікселів (для чорно-білого зображення) або 3D тензор (висота x ширина x канали кольору, наприклад, RGB).

* **Фільтр (Filter) / Ядро (Kernel):** Це **маленька матриця ваг** (наприклад, 3x3, 5x5). Кожен фільтр призначений для виявлення певної **ознаки** (патерну), такої як вертикальна лінія, горизонтальна лінія, кут, певна текстура тощо. **Значення ваг у фільтрі є параметрами, які мережа вивчає під час тренування.**

* **Процес згортки (інтуїтивно):**
    1.  Візьміть фільтр.
    2.  Накладіть його на невелику ділянку (рецептивне поле) у верхньому лівому кутку вхідного зображення (або карти ознак з попереднього шару).
    3.  Виконайте **поелементне множення** значень фільтра на відповідні значення пікселів під ним.
    4.  **Просумуйте** всі результати цього множення. Отримане єдине число записується у відповідну позицію вихідної **карти ознак (Feature Map)** або **карти активацій (Activation Map)**.
    5.  **Зсуньте** фільтр на певну кількість пікселів (крок) вправо.
    6.  Повторіть кроки 3-4. Продовжуйте зсувати фільтр вправо до кінця рядка.
    7.  Зсуньте фільтр вниз на певну кількість пікселів (крок) і повторіть процес для наступного рядка.
    8.  Продовжуйте, поки фільтр не пройде по всьому вхідному зображенню.

* **Аналогія:** Уявіть, що ви водите маленьким "детектором візерунків" (фільтр) по всьому зображенню. Коли детектор знаходить візерунок, на який він налаштований, він "сигналізує" про це, записуючи високе значення у вихідну карту ознак у відповідному місці.

* **Карта ознак (Feature Map):** Результатом застосування одного фільтра до вхідних даних є нова 2D матриця (карта ознак), яка показує, в яких місцях вхідного зображення була виявлена ознака, на яку "налаштований" цей фільтр.

---

**Згорткові шари (Convolutional Layers)**

Шар нейронної мережі, що виконує операцію згортки.

* **Кілька фільтрів:** Зазвичай згортковий шар використовує не один, а **багато фільтрів** одночасно. Кожен фільтр навчається розпізнавати **різну** ознаку. Наприклад, один може шукати вертикальні лінії, інший – горизонтальні, третій – певний колір чи текстуру. Кожен фільтр генерує свою власну карту ознак. Таким чином, вихід згорткового шару – це "стопка" або **об'єм** з кількох карт ознак (3D тензор: висота x ширина x кількість_фільтрів).

* **Ключові параметри згорткового шару:**
    * **Розмір фільтра (Filter Size / Kernel Size):** Визначає розмір рецептивного поля (наприклад, 3x3, 5x5, 7x7). Менші фільтри (3x3, 5x5) використовуються найчастіше, оскільки вони ефективно виявляють локальні ознаки.
    * **Кількість фільтрів (Number of Filters / Depth):** Визначає кількість карт ознак на виході шару (глибину вихідного об'єму). Чим більше фільтрів, тим більше різних ознак може виявити шар.
    * **Крок (Stride):** Кількість пікселів, на яку фільтр зсувається за один крок при русі по вхідному зображенню. Stride=1 (зсув на 1 піксель) є стандартним. Stride > 1 призводить до зменшення просторових розмірів (висоти та ширини) вихідної карти ознак (downsampling).
    * **Доповнення (Padding):** Додавання пікселів (зазвичай нульових) навколо країв вхідного зображення або карти ознак перед застосуванням згортки.
        * **Навіщо потрібне:**
            1.  **Контроль розміру виходу:** Без padding кожен згортковий шар зменшував би розміри карти ознак. Padding дозволяє зберегти просторові розміри (часто використовують "same" padding) або контролювати їх зменшення ("valid" padding – без доповнення).
            2.  **Обробка країв:** Дозволяє фільтрам краще обробляти пікселі біля меж зображення.

* **Спільне використання параметрів (Parameter Sharing):** **Дуже важлива ідея CNN!** Один і той самий фільтр (з однаковими вагами) використовується для сканування **всього** вхідного зображення. Це означає, що якщо мережа навчилася розпізнавати, наприклад, вертикальну лінію в одній частині зображення, вона зможе розпізнати її і в будь-якій іншій частині. Це:
    1.  **Значно зменшує кількість параметрів** для навчання порівняно з повнозв'язними шарами MLP.
    2.  Робить мережу **інваріантною до зсуву** (translation invariant) – положення ознаки на зображенні стає менш важливим.

* **Функція активації:** Після операції згортки до кожної карти ознак поелементно застосовується нелінійна функція активації, найчастіше **ReLU ($g(z) = \max(0, z)$)**. Це додає нелінійності, необхідної для навчання складних залежностей. Повний крок: $Output = ReLU(Convolution(Input))$.

---

**Шари пулінгу (Pooling Layers / Subsampling Layers)**

* **Мета:** Зменшити просторові розміри (висоту та ширину) карт ознак, зберігаючи при цьому найважливішу інформацію. Це допомагає:
    1.  **Зменшити кількість обчислень** та параметрів у наступних шарах.
    2.  Зробити представлення ознак більш **стійким до невеликих зсувів та деформацій** на зображенні (додає інваріантності).
    3.  Контролювати **перенавчання**.

* **Принцип роботи:** Застосовується до кожної карти ознак окремо. Карта ділиться на невеликі прямокутні ділянки (зазвичай без перекриття), і для кожної ділянки обчислюється певне агреговане значення.

* **Типи пулінгу:**
    1.  **Макс-пулінг (Max Pooling):** **Найпоширеніший.** Для кожної ділянки вибирається **максимальне** значення.
        * *Ефект:* Зберігає найсильніший відгук на ознаку в межах ділянки. Добре працює на практиці.
    2.  **Середній пулінг (Average Pooling):** Для кожної ділянки обчислюється **середнє** значення.
        * *Ефект:* Дає більш згладжене представлення. Іноді використовується, але рідше за макс-пулінг.

* **Параметри пулінгу:**
    * **Розмір вікна (Pool Size):** Розмір ділянки, по якій проводиться агрегація (наприклад, 2x2).
    * **Крок (Stride):** Крок, з яким вікно рухається по карті ознак. Часто крок дорівнює розміру вікна (наприклад, Stride=2 для вікна 2x2), що призводить до зменшення розмірів удвічі та відсутності перекриття вікон.

* **Важливо:** Шари пулінгу зазвичай **не мають параметрів для навчання** (ваг чи зміщень). Вони виконують фіксовану операцію.

---

**Типова архітектура CNN (Короткий огляд)**

Класична CNN часто складається з послідовності блоків, кожен з яких містить:

1.  Один або декілька **Згорткових шарів** (для виявлення ознак).
2.  **Функцію активації** (зазвичай ReLU) після кожного згорткового шару.
3.  **Шар пулінгу** (для зменшення розмірності та інваріантності).

Ці блоки можуть повторюватися кілька разів. Чим глибше в мережі, тим складніші та абстрактніші ознаки виявляють згорткові шари (від простих ліній та кутів до частин об'єктів і цілих об'єктів).

Після кількох таких згортково-пулінгових блоків, отримані карти ознак зазвичай "випрямляють" (flatten) у довгий вектор і подають на вхід одного або кількох **повнозв'язних шарів (MLP)**, які виконують фінальну класифікацію або регресію.

Навчання CNN, як і інших нейромереж, відбувається за допомогою алгоритму **зворотного поширення помилки (Backpropagation)** та оптимізаторів типу **SGD** або **Adam**, але обчислення градієнтів адаптовані для операцій згортки та пулінгу.

---

**Підсумки**

* **CNN** – це спеціалізований тип нейронних мереж, особливо ефективний для обробки даних з просторовою структурою, таких як зображення.
* **Згорткові шари** використовують **фільтри (ядра)** для виявлення локальних ознак за допомогою **операції згортки**. **Спільне використання параметрів** робить їх ефективними та інваріантними до зсуву. Важливими параметрами є розмір фільтра, кількість фільтрів, крок (**stride**) та доповнення (**padding**).
* **Функція активації (ReLU)** додає нелінійність після згортки.
* **Шари пулінгу (Max Pooling, Average Pooling)** зменшують просторові розміри карт ознак, роблячи модель більш стійкою та обчислювально ефективною.
* Комбінація цих шарів дозволяє CNN автоматично вивчати ієрархію ознак – від простих до складних.

CNN є основою багатьох сучасних досягнень у комп'ютерному зорі та продовжують активно розвиватися.

**Наступні кроки**

* Подумайте, чому спільне використання параметрів у згорткових шарах є перевагою порівняно з повнозв'язними шарами MLP при роботі з зображеннями.
* Як ви гадаєте, чому макс-пулінг може бути ефективнішим за середній пулінг для задач розпізнавання об'єктів?
* На наступних лекціях ми можемо розглянути інші типи архітектур (наприклад, рекурентні мережі для послідовностей) або заглибитись в практичні аспекти тренування глибоких мереж.

---

Дякую за увагу! Не забуваємо про вподобайку, поширення та чесний відгук чи запитання в коментарях, до зустрічі!