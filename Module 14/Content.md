**Лекція 14: Глибоке навчання - Рекурентні нейронні мережі (RNN) - Основи**

**Мета лекції:** Ознайомити слухачів з рекурентними нейронними мережами (Recurrent Neural Networks - RNN), призначеними для роботи з послідовними даними. Розглянути особливості таких даних, базову архітектуру RNN, проблеми, що виникають при їх навчанні (зникаючі/зростаючі градієнти), та основні типи задач, які вирішують RNN.

---

Доброго дня! На попередніх лекціях ми розглядали багатошарові перцептрони (MLP), які добре працюють зі структурованими даними, та згорткові нейронні мережі (CNN), що чудово підходять для даних з просторовою структурою, таких як зображення. Однак значна частина даних у реальному світі має **послідовну природу**: текст, мова, часові ряди (ціни акцій, погода), музика, відео.

Стандартні мережі прямого поширення (MLP, CNN) обробляють кожен вхідний приклад незалежно і не мають "пам'яті" про попередні приклади. Це робить їх непридатними для задач, де **порядок елементів** та контекст є критично важливими. Для роботи з такими даними були розроблені **Рекурентні Нейронні Мережі (RNN)**.

Сьогодні ми:

1.  Обговоримо **особливості послідовних даних**.
2.  Розглянемо **архітектуру простої RNN** та її ключову ідею – "пам'ять".
3.  Поговоримо про **основні проблеми** навчання RNN.
4.  Розглянемо **основні типи** задач, які вирішують RNN.

---

**Послідовні дані та їх особливості**

* **Що це?** Дані, де елементи надходять у певній послідовності, і ця послідовність має значення.
* **Ключові характеристики:**
    * **Залежність від порядку:** Значення або сенс поточного елемента часто залежить від попередніх (а іноді й наступних) елементів. Перестановка елементів руйнує структуру та сенс. (Порівняйте: "собака вкусила людину" vs "людина вкусила собаку").
    * **Змінна довжина:** Послідовності можуть мати різну довжину (речення складаються з різної кількості слів, часові ряди можуть охоплювати різні періоди). Це створює труднощі для моделей, що очікують вхід фіксованого розміру.
    * **Довгострокові залежності (Long-Range Dependencies):** Значення поточного елемента може залежати від елементів, що зустрічалися значно раніше в послідовності. (Наприклад, у тексті: "Я народився у Львові ... саме тому я так добре розмовляю українською").

* **Приклади:**
    * **Текст:** Послідовність символів, слів, речень.
    * **Часові ряди:** Послідовність вимірювань у часі (ціни на акції, температура, показники датчиків).
    * **Аудіо:** Послідовність амплітуд звукових коливань.
    * **Відео:** Послідовність кадрів (зображень).
    * **Музика:** Послідовність нот або аудіосигналів.
    * **Біологічні послідовності:** ДНК, РНК, послідовності білків.

---

**Архітектура RNN та концепція "пам'яті"**

* **Основна ідея RNN:** Ввести в мережу **"пам'ять"** про попередні елементи послідовності. Це досягається за допомогою **рекурентного (зворотного) зв'язку**: вихід мережі на попередньому кроці (або її внутрішній стан) використовується як частина входу на поточному кроці.

* **Розгортання у часі (Unrolling):** Щоб зрозуміти роботу RNN, зручно уявити її "розгорнутою" у часі. Для кожного елемента послідовності $x_t$ (в момент часу $t$) мережа виконує наступне:
    1.  Приймає поточний вхід $x_t$.
    2.  Приймає **прихований стан (hidden state)** $h_{t-1}$ з попереднього кроку. Цей стан $h_{t-1}$ містить інформацію про всі попередні елементи послідовності ($x_0, x_1, ..., x_{t-1}$).
    3.  Обчислює **новий прихований стан** $h_t$, комбінуючи поточний вхід $x_t$ та попередній стан $h_{t-1}$.
    4.  Обчислює **вихід** $y_t$ на основі поточного прихованого стану $h_t$.

    **Важливо:** На кожному кроці використовуються **ті самі** набори ваг та зміщень (parameter sharing over time). Це робить RNN ефективною для послідовностей змінної довжини.

* **Спрощені рівняння:**
    * **Оновлення прихованого стану:**
        $h_t = g_h(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$
        * $x_t$: Вхідний вектор на кроці $t$.
        * $h_{t-1}$: Прихований стан на кроці $t-1$ (початковий стан $h_0$ зазвичай нульовий).
        * $h_t$: Новий прихований стан на кроці $t$ ("пам'ять" мережі).
        * $W_{xh}$: Матриця ваг для зв'язку "вхід -> прихований стан".
        * $W_{hh}$: Матриця ваг для **рекурентного** зв'язку "попередній прихований стан -> поточний прихований стан".
        * $b_h$: Вектор зміщень для прихованого стану.
        * $g_h$: Функція активації прихованого шару (часто `tanh` або `ReLU`).
    * **Обчислення виходу:**
        $y_t = g_y(W_{hy} h_t + b_y)$
        * $y_t$: Вихідний вектор на кроці $t$.
        * $W_{hy}$: Матриця ваг для зв'язку "прихований стан -> вихід".
        * $b_y$: Вектор зміщень для виходу.
        * $g_y$: Функція активації вихідного шару (залежить від задачі: `linear` для регресії, `sigmoid` для бінарної класифікації, `softmax` для мультикласової класифікації).

* **Навчання:** RNN навчаються за допомогою алгоритму **Зворотного Поширення Помилки в Часі (Backpropagation Through Time - BPTT)**. Це адаптація стандартного backpropagation, яка враховує розгортання мережі у часі та спільні ваги.

---

**Проблема зникаючого та зростаючого градієнта**

При навчанні простих RNN за допомогою BPTT виникає серйозна проблема, пов'язана з обробкою довгих послідовностей:

* **Зникаючий градієнт (Vanishing Gradient):** Під час зворотного поширення помилки градієнти множаться на рекурентну матрицю ваг $W_{hh}$ на кожному кроці назад у часі. Якщо елементи цієї матриці (або похідні функції активації) менші за одиницю, то градієнт експоненційно **зменшується** при русі назад. В результаті, помилка з пізніх кроків майже не впливає на ваги, що відповідають за обробку ранніх елементів послідовності. Мережа "забуває" інформацію з далекого минулого і **не може вивчити довгострокові залежності**. Це була головна перешкода для ефективного використання простих RNN.

* **Зростаючий (вибухаючий) градієнт (Exploding Gradient):** Якщо ж елементи $W_{hh}$ (або похідні) більші за одиницю, градієнт може експоненційно **зростати** при русі назад. Це призводить до дуже великих оновлень ваг, нестабільності навчання і розбіжності алгоритму (часто проявляється як NaN - "Not a Number" - у значенні функції втрат). Цю проблему легше виявити та частково вирішити за допомогою техніки **обрізання градієнта (gradient clipping)** – обмеження максимальної величини градієнта.

Проблема зникаючого градієнта є більш підступною і фундаментальною для простих RNN.

---

**Основні типи RNN (за структурами вхід-вихід)**

RNN можуть мати різні архітектури залежно від співвідношення довжини вхідної та вихідної послідовностей:

1.  **Багато до одного (Many-to-One):** Мережа приймає на вхід послідовність, а видає єдиний результат в кінці.
    * *Приклади:* Аналіз тональності тексту (сентимент-аналіз: вхід - речення, вихід - "позитивний"/"негативний"), класифікація музичного жанру за аудіофрагментом.
2.  **Один до багатьох (One-to-Many):** Мережа приймає єдиний вхід, а генерує послідовність.
    * *Приклади:* Генерація тексту (вхід - початкове слово/символ, вихід - продовження речення), генерування музики, підписування зображень (image captioning: вхід - зображення (оброблене CNN), вихід - опис у вигляді послідовності слів).
3.  **Багато до багатьох (Many-to-Many), синхронізовані:** Довжина вхідної та вихідної послідовностей однакова, вихід генерується на кожному кроці.
    * *Приклади:* Класифікація відео по кадрах, тегування частин мови (Part-of-Speech tagging: для кожного слова в реченні визначається його частина мови).
4.  **Багато до багатьох (Many-to-Many), з затримкою (Sequence-to-Sequence, Seq2Seq):** Мережа читає всю вхідну послідовність, а потім генерує вихідну послідовність (яка може мати іншу довжину).
    * *Приклади:* Машинний переклад (вхід - речення однією мовою, вихід - речення іншою мовою), відповіді на запитання, створення резюме тексту. Зазвичай використовує архітектуру **кодувальника-декодувальника (Encoder-Decoder)**.

**Подолання проблеми зникаючих градієнтів: LSTM та GRU**

Щоб ефективно працювати з довгими послідовностями та долати проблему зникаючих градієнтів, були розроблені більш складні рекурентні модулі:

* **Довга короткострокова пам'ять (Long Short-Term Memory - LSTM):** Має складнішу внутрішню структуру з **трьома "вентилями" (gates)** – вхідним, забування та вихідним – та окремим **станом комірки (cell state)**. Ці вентилі динамічно контролюють, яка інформація додається до стану комірки, яка видаляється, і яка використовується для генерації виходу. Це дозволяє LSTM ефективно зберігати та використовувати інформацію протягом тривалих періодів часу.
* **Керований рекурентний блок (Gated Recurrent Unit - GRU):** Спрощена версія LSTM з двома вентилями (оновлення та скидання). Часто демонструє схожу продуктивність з LSTM, але є обчислювально трохи простішою.

**LSTM та GRU є стандартними блоками для побудови сучасних RNN**, оскільки вони значно краще справляються з навчанням довгострокових залежностей порівняно з простими RNN.

---

**Підсумки**

* **Послідовні дані** (текст, часові ряди, аудіо) вимагають моделей, що враховують порядок елементів та їх залежності.
* **Рекурентні нейронні мережі (RNN)** використовують **рекурентні зв'язки** та **прихований стан** для збереження "пам'яті" про попередні елементи послідовності.
* Навчання RNN за допомогою **BPTT** стикається з проблемами **зникаючих** (неможливість вивчити довгі залежності) та **зростаючих** (нестабільність навчання) градієнтів.
* **LSTM** та **GRU** – це більш просунуті рекурентні модулі з "вентилями", які значно ефективніше працюють з довгими послідовностями, долаючи проблему зникаючих градієнтів.
* RNN (особливо LSTM та GRU) використовуються для широкого кола задач: обробка природної мови, машинний переклад, аналіз часових рядів, генерування послідовностей тощо.

RNN, LSTM та GRU є потужними інструментами для моделювання динаміки та залежностей у послідовних даних, відкриваючи двері до вирішення багатьох складних завдань ШІ.

**Наступні кроки**

* Подумайте, які ще приклади послідовних даних ви знаєте з повсякденного життя чи роботи.
* Чому "пам'ять" є ключовою для обробки послідовностей?
* На наступній лекції ми можемо розглянути механізм уваги (Attention Mechanism) та архітектуру Transformer, які стали ще одним проривом у роботі з послідовностями, особливо в обробці природної мови.

---

Дякую за увагу! Я готовий відповісти на ваші запитання.