**Лекція 15: Глибоке навчання - Рекурентні нейронні мережі (RNN) - LSTM та GRU**

**Мета лекції:** Детальніше розглянути просунуті типи рекурентних блоків – LSTM (Довга короткострокова пам'ять) та GRU (Вентильований рекурентний блок). Пояснити їхню архітектуру та принцип роботи на концептуальному рівні, фокусуючись на тому, як вони вирішують проблему зникаючих градієнтів. Обговорити основні сфери застосування цих моделей в обробці послідовностей.

---

Доброго дня! На минулій лекції ми познайомилися з рекурентними нейронними мережами (RNN) – архітектурою, що має "пам'ять" завдяки рекурентним зв'язкам і прихованому стану $h_t$. Ми також з'ясували, що прості RNN страждають від серйозної проблеми **зникаючих градієнтів** при навчанні за допомогою Backpropagation Through Time (BPTT). Це значно обмежує їхню здатність вивчати **довгострокові залежності** в послідовностях – інформація з далекого минулого просто "губиться" на шляху градієнтів.

Сьогодні ми розглянемо два найпопулярніші та найефективніші рішення цієї проблеми, які стали стандартом де-факто для багатьох задач обробки послідовностей:

1.  **LSTM (Long Short-Term Memory):** Як ця архітектура використовує "вентилі" та "стан комірки" для керування пам'яттю?
2.  **GRU (Gated Recurrent Unit):** Як спрощена версія LSTM досягає схожих результатів?
3.  **Застосування:** Де саме ці потужні моделі знаходять своє застосування?

---

**Проблема простих RNN (Нагадування)**

У простому RNN прихований стан $h_t$ оновлюється за формулою типу $h_t = g_h(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$. Під час зворотного поширення помилки (BPTT) градієнт множиться на матрицю $W_{hh}$ (та похідну $g_h$) на кожному кроці назад у часі. Якщо ці значення систематично менші за 1, градієнт експоненційно згасає, унеможливлюючи передачу інформації про помилку на багато кроків назад і, відповідно, навчання довгострокових залежностей.

**Довга короткострокова пам'ять (LSTM - Long Short-Term Memory)**

Розроблена Зеппом Хохрайтером та Юргеном Шмідхубером у 1997 році, LSTM була спеціально створена для боротьби зі зникаючими градієнтами.

* **Ключова ідея:** Ввести додатковий компонент – **стан комірки (Cell State, $C_t$)**, який проходить крізь весь ланцюжок обчислень з мінімальними лінійними взаємодіями. $C_t$ діє як "конвеєрна стрічка" для інформації. LSTM може вибірково додавати або видаляти інформацію зі стану комірки за допомогою спеціальних механізмів – **вентилів (Gates)**.

* **Вентилі (Gates):** Це невеликі нейронні підмережі (зазвичай із сигмоїдною функцією активації, що видає значення від 0 до 1), які регулюють потік інформації. Вони вирішують, яку інформацію пропустити, а яку заблокувати. LSTM має три основні вентилі:

    1.  **Вентиль забування (Forget Gate, $f_t$):**
        * **Призначення:** Вирішує, яку інформацію **викинути** зі стану комірки $C_{t-1}$ (попереднього стану).
        * **Як працює:** Дивиться на попередній прихований стан $h_{t-1}$ та поточний вхід $x_t$ і видає значення від 0 до 1 для кожного числа у $C_{t-1}$. 1 означає "повністю зберегти", 0 означає "повністю забути".
        * *Концептуальна формула:* $f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)$ (де $[h_{t-1}, x_t]$ - конкатенація векторів).

    2.  **Вхідний вентиль (Input Gate, $i_t$):**
        * **Призначення:** Вирішує, яку **нову інформацію** додати до стану комірки.
        * **Як працює:** Складається з двох частин:
            * Сигмоїдний шар ($i_t$) вирішує, *які значення* ми будемо оновлювати (0 або 1). $i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)$.
            * Tanh-шар створює вектор *нових кандидатів* $\tilde{C}_t$, які можна додати до стану. $\tilde{C}_t = \tanh(W_C [h_{t-1}, x_t] + b_C)$.

    3.  **Оновлення стану комірки:** Старий стан $C_{t-1}$ множиться на $f_t$ (забуваємо непотрібне), а нові кандидати $\tilde{C}_t$ множаться на $i_t$ (вибираємо, що додати) і додаються до результату:
        $C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$ (де * - поелементне множення).

    4.  **Вихідний вентиль (Output Gate, $o_t$):**
        * **Призначення:** Вирішує, яку частину стану комірки $C_t$ видати назовні як **прихований стан $h_t$** (який також використовується для прогнозу $y_t$ і передається на наступний крок).
        * **Як працює:**
            * Сигмоїдний шар ($o_t$) вирішує, які частини стану комірки вивести. $o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)$.
            * Стан комірки $C_t$ пропускається через $\tanh$ (щоб значення були в діапазоні [-1, 1]) і множиться на вихід вентиля $o_t$:
            $h_t = o_t * \tanh(C_t)$.

* **Інтуїція:** Стан комірки $C_t$ дозволяє інформації протікати майже без змін, діючи як довгострокова пам'ять. Вентилі ж, керуючись поточним входом та попереднім виходом, гнучко контролюють, коли цю пам'ять читати, коли записувати нове, а коли забувати старе. Це дозволяє градієнтам краще поширюватися назад у часі.

---

**Вентильований рекурентний блок (GRU - Gated Recurrent Unit)**

Представлений Кюнгхюн Чо та іншими у 2014 році, GRU є новішою та дещо **спрощеною** альтернативою LSTM.

* **Ідея:** Досягти схожого ефекту контролю над потоком інформації та пам'яттю, але з меншою кількістю параметрів та простішою структурою. GRU об'єднує стан комірки та прихований стан і використовує лише **два вентилі**:

    1.  **Вентиль оновлення (Update Gate, $z_t$):**
        * **Призначення:** Вирішує, скільки інформації з **попереднього стану $h_{t-1}$** потрібно зберегти, а скільки взяти з **нового кандидата $\tilde{h}_t$**.
        * **Як працює:** $z_t = \sigma(W_z [h_{t-1}, x_t] + b_z)$. Якщо $z_t$ близький до 1, зберігається більше старої інформації; якщо близький до 0 – більше нової.

    2.  **Вентиль скидання (Reset Gate, $r_t$):**
        * **Призначення:** Вирішує, скільки інформації з **попереднього стану $h_{t-1}$** потрібно **"забути"** при обчисленні нового кандидата $\tilde{h}_t$.
        * **Як працює:** $r_t = \sigma(W_r [h_{t-1}, x_t] + b_r)$. Якщо $r_t$ близький до 0, то попередня інформація $h_{t-1}$ майже ігнорується при створенні кандидата.

    3.  **Обчислення кандидата ($\tilde{h}_t$):** Схоже на простий RNN або LSTM, але використовує вентиль скидання для контролю впливу $h_{t-1}$:
        $\tilde{h}_t = \tanh(W_h [r_t * h_{t-1}, x_t] + b_h)$.

    4.  **Оновлення прихованого стану ($h_t$):** Комбінує старий стан та кандидата за допомогою вентиля оновлення:
        $h_t = (1 - z_t) * h_{t-1} + z_t * \tilde{h}_t$. (Лінійна інтерполяція між старим і новим станом).

* **Порівняння LSTM та GRU:**
    * GRU має менше параметрів (ваг та зміщень), ніж LSTM, оскільки має лише 2 вентилі та не має окремого стану комірки.
    * Це може робити GRU обчислювально ефективнішим та трохи швидшим у навчанні.
    * На практиці їхня продуктивність часто дуже схожа. Немає однозначної відповіді, що краще – вибір часто залежить від конкретної задачі та даних, і варто експериментувати з обома. GRU може бути кращим вибором для менших датасетів.

---

**Застосування RNN (LSTM/GRU) для обробки послідовностей**

Завдяки своїй здатності моделювати довгострокові залежності, LSTM та GRU стали надзвичайно успішними у багатьох задачах, пов'язаних із послідовностями:

* **Обробка природної мови (Natural Language Processing - NLP):**
    * **Машинний переклад:** Архітектури Sequence-to-Sequence з LSTM/GRU кодувальниками та декодувальниками.
    * **Генерація тексту:** Написання текстів, продовження речень, створення діалогових систем (чат-ботів).
    * **Аналіз тональності (Sentiment Analysis):** Визначення емоційного забарвлення тексту.
    * **Розпізнавання іменованих сутностей (Named Entity Recognition - NER):** Знаходження імен людей, організацій, локацій у тексті.
    * **Моделювання мови (Language Modeling):** Передбачення наступного слова в реченні.

* **Аналіз часових рядів:**
    * Прогнозування цін на фінансових ринках.
    * Прогнозування погоди.
    * Аналіз медичних показників у часі (ЕКГ, ЕЕГ).
    * Виявлення аномалій у послідовних даних (наприклад, у логах системи або показниках датчиків).

* **Розпізнавання мови (Speech Recognition):** Перетворення аудіосигналу (послідовності) на текст (послідовність).

* **Інші:** Генерація музики, аналіз відео, біоінформатика.

---

**Підсумки**

* Прості RNN погано справляються з навчанням **довгострокових залежностей** через проблему **зникаючих градієнтів**.
* **LSTM** вирішує цю проблему за допомогою **стану комірки ($C_t$)** та **трьох вентилів (забування, вхідного, вихідного)**, які ретельно контролюють потік інформації та оновлення пам'яті.
* **GRU** є **спрощеною альтернативою** LSTM з **двома вентилями (оновлення та скидання)**, що об'єднує прихований стан та стан комірки і часто демонструє схожу ефективність при меншій кількості параметрів.
* **LSTM та GRU** є основними будівельними блоками для більшості сучасних систем, що працюють з **послідовними даними** в таких галузях, як NLP, аналіз часових рядів, розпізнавання мови тощо.

Розуміння принципів роботи LSTM та GRU є ключовим для роботи з послідовними даними в глибокому навчанні.

**Наступні кроки**

* Подумайте, в яких ще задачах важливе врахування порядку та довгострокових залежностей.
* Хоча LSTM та GRU є дуже потужними, останнім часом у багатьох задачах (особливо в NLP) їх почали витісняти ще новіші архітектури, такі як Transformer. На наступній лекції ми можемо розглянути саме їх та механізм уваги (Attention Mechanism), що лежить в їх основі.

---

Дякую за увагу! Я готовий відповісти на ваші запитання.