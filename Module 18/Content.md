**Лекція 18: Обробка природної мови (NLP) - Трансформери та увага**

**Мета лекції:** Познайомити слухачів з архітектурою Transformer та механізмом уваги (Attention), які здійснили революцію в обробці природної мови за останні роки. Пояснити основні принципи їхньої роботи та переваги над рекурентними мережами (RNN). Розглянути ключові сфери застосування трансформерів.

---

Доброго дня! На попередніх лекціях ми говорили про обробку послідовних даних за допомогою рекурентних нейронних мереж, зокрема LSTM та GRU, які ефективно борються з проблемою зникаючих градієнтів і можуть запам'ятовувати інформацію на досить довгих проміжках. Проте, навіть ці просунуті RNN мають свої обмеження:

1.  **Послідовна обробка:** Вони обробляють послідовність крок за кроком, що ускладнює паралелізацію обчислень на сучасному обладнанні (GPU/TPU) і може бути повільним для дуже довгих послідовностей.
2.  **Довгострокові залежності:** Хоча LSTM/GRU значно кращі за прості RNN, передача інформації через послідовний прихований стан все ще може призводити до "розмивання" контексту на дуже великих відстанях.

У 2017 році дослідники з Google представили статтю "Attention Is All You Need", яка запропонувала нову архітектуру – **Трансформер (Transformer)**. Ця архітектура відмовилася від рекурентних зв'язків на користь механізму, що отримав назву **увага (Attention)**. Саме про ці революційні концепції ми сьогодні й поговоримо.

Наш план:

1.  **Механізм уваги (Attention Mechanism):** Що це таке і чому це важливо?
2.  **Архітектура Трансформера:** З чого складається модель, що змінила сучасне NLP?
3.  **Застосування Трансформерів:** Де ці моделі використовуються?

---

**Механізм уваги (Attention Mechanism)**

* **Основна ідея:** Дозволити моделі при обробці (або генеруванні) певного елемента послідовності (наприклад, слова) **динамічно фокусуватися** на найрелевантніших частинах **іншої** послідовності (наприклад, вхідного речення при перекладі) або **тієї ж самої** послідовності (для кращого розуміння контексту). Модель "вирішує", на які слова варто звернути більше "уваги", а на які менше, залежно від поточного завдання.

* **Аналогія (Машинний переклад):** Коли ви перекладаєте речення, наприклад, з української на англійську, для перекладу конкретного українського слова вам може знадобитися подивитися не лише на саме це слово, а й на підмет, присудок, пов'язані з ним прикметники чи займенники, навіть якщо вони стоять в іншій частині речення. Механізм уваги дозволяє моделі робити щось подібне – встановлювати прямі зв'язки між словами, незалежно від відстані між ними.

* **Сама-увага (Self-Attention):** Ключовий тип уваги, що використовується в Трансформерах. Дозволяє моделі зважувати важливість **усіх слів у тій самій послідовності** при обробці (створенні представлення) **кожного окремого слова**.
    * **Як це допомагає?**
        * **Розуміння контексту:** Щоб зрозуміти значення слова "банк" у реченні "Я пішов у банк зняти гроші" vs "Я сидів на березі річки біля крутого банку", модель може звернути увагу на слова "гроші" або "річки" відповідно.
        * **Розв'язання анафори:** Щоб зрозуміти, до кого відноситься займенник "він" у тексті, модель може звернути увагу на попередні згадки імен чоловічого роду.
        * **Виявлення залежностей:** Наприклад, між дієсловом та його підметом/додатком, навіть якщо вони розділені багатьма словами.

* **Як працює (концептуально - Query, Key, Value):**
    1.  Для кожного слова (токена) в послідовності на основі його векторного представлення (word embedding + positional encoding) створюються три вектори: **Запит (Query, Q)**, **Ключ (Key, K)** та **Значення (Value, V)**. Це робиться за допомогою окремих навчених матриць ваг.
        * *Інтуїція:* Q – це поточне слово, що "запитує" про релевантну інформацію; K – це "ярлики" або "ідентифікатори" всіх слів, з якими можна порівнювати запит; V – це власне "зміст" або інформація, яку несе кожне слово.
    2.  **Обчислення оцінок уваги (Attention Scores):** Для Запиту $Q_i$ поточного слова обчислюється його подібність (зазвичай скалярний добуток) з Ключами $K_j$ **всіх** слів у послідовності (включаючи саме слово $i$). Чим вища оцінка, тим більш "релевантним" є слово $j$ для слова $i$.
    3.  **Нормалізація оцінок у ваги (Attention Weights):** Оцінки подібності перетворюються на ймовірності (ваги уваги $\alpha_{ij}$) за допомогою функції **Softmax**. Ці ваги сумуються до 1 і показують, яку частку "уваги" слово $i$ має приділити кожному слову $j$.
    4.  **Обчислення контекстуалізованого виходу:** Нове представлення для слова $i$ обчислюється як **зважена сума** всіх векторів Значень $V_j$, де вагами є відповідні коефіцієнти уваги $\alpha_{ij}$.
        $Output_i = \sum_{j} \alpha_{ij} V_j$.
        Таким чином, вихідне представлення кожного слова містить інформацію з усієї послідовності, зважену за ступенем релевантності.

* **Багатоголова увага (Multi-Head Attention):** Замість того, щоб застосовувати увагу один раз, її застосовують **кілька разів паралельно** ("голови уваги"). Кожна "голова" навчається фокусуватися на різних аспектах взаємозв'язків між словами (наприклад, одна на синтаксичних, інша на семантичних). Результати всіх голів потім об'єднуються.

**Переваги механізму уваги:**
* Моделює залежності між будь-якими двома словами **незалежно від відстані**.
* Обчислення уваги для всіх слів можна **ефективно паралелізувати**, на відміну від послідовної обробки в RNN.

---

**Архітектура Трансформера (Transformer Networks)**

Трансформер – це архітектура, яка **повністю базується на механізмі уваги** і не використовує рекурентних або згорткових шарів (у своїй основній версії).

* **Загальна структура (для задач Sequence-to-Sequence, як машинний переклад):**
    * **Кодувальник (Encoder):**
        * Призначення: Обробляє вхідну послідовність (напр., речення українською) і створює багаті контекстуалізовані представлення для кожного слова.
        * Складається зі стопки однакових шарів (напр., 6 або 12).
        * Кожен шар містить два основні під-шари:
            1.  **Багатоголова сама-увага (Multi-Head Self-Attention):** Дозволяє кожному слову вхідної послідовності "звернути увагу" на інші слова в тій самій послідовності.
            2.  **Повнозв'язна мережа прямого поширення (Position-wise Feed-Forward Network):** Проста двошарова MLP, що застосовується незалежно до представлення кожного слова.
        * Навколо кожного під-шару використовуються **пропускні з'єднання (skip connections)** (як у ResNet) та **нормалізація шарів (Layer Normalization)** для стабілізації навчання.
    * **Декодувальник (Decoder):**
        * Призначення: Генерує вихідну послідовність (напр., переклад англійською) крок за кроком.
        * Також складається зі стопки однакових шарів.
        * Кожен шар містить три основні під-шари:
            1.  **Маскована багатоголова сама-увага (Masked Multi-Head Self-Attention):** Те ж саме, що й у кодувальнику, але з "маскою", яка забороняє слову "звертати увагу" на наступні слова у вихідній послідовності (оскільки під час генерації ми ще не знаємо майбутніх слів).
            2.  **Багатоголова увага "кодувальник-декодувальник" (Encoder-Decoder Attention):** **Ключовий механізм!** Дозволяє кожному слову, що генерується декодувальником, "звернути увагу" на найрелевантніші частини **вхідної послідовності**, представленої виходом кодувальника.
            3.  **Повнозв'язна мережа прямого поширення (Feed-Forward Network):** Аналогічна тій, що в кодувальнику.
        * Також використовуються skip-connections та layer normalization.

* **Позиційне кодування (Positional Encoding):** Оскільки в Трансформері немає рекуренсії чи згортки, він сам по собі не знає про порядок слів. Щоб надати моделі інформацію про **позицію** слова у послідовності, до вхідних векторних представлень слів (word embeddings) додаються спеціальні **позиційні вектори**. Ці вектори генеруються за допомогою фіксованих функцій (синусів та косинусів) або можуть навчатися.

---

**Застосування Трансформерів в NLP**

Архітектура Трансформер справила величезний вплив на NLP і стала основою для більшості сучасних передових моделей:

1.  **Машинний переклад:** Трансформери значно покращили якість автоматичного перекладу.
2.  **Великі мовні моделі (Large Language Models - LLMs):** Трансформери (особливо їхні кодувальні або декодувальні частини) є основою для таких моделей, як:
    * **BERT (Bidirectional Encoder Representations from Transformers):** Використовує кодувальник Трансформера. Навчається на величезних обсягах тексту розуміти контекст слова, дивлячись одночасно на слова зліва і справа. Чудово підходить для задач **розуміння** мови (класифікація, відповіді на запитання, розпізнавання сутностей). Існують версії, навчені на українських текстах.
    * **GPT (Generative Pre-trained Transformer):** Використовує декодувальник Трансформера. Навчається передбачати наступне слово в тексті. Дуже потужна для задач **генерування** тексту (написання статей, діалоги, код), прикладом є ChatGPT.
    * Багато інших: T5, BART, XLNet, RoBERTa тощо.
3.  **Інші задачі NLP:** Сучасні трансформерні моделі (часто попередньо навчені та потім доуточнені – fine-tuned) використовуються для вирішення майже всього спектру NLP задач: аналізу тональності, реферування, розпізнавання мови, діалогових систем та ін.
4.  **Поза NLP:** Архітектурні ідеї Трансформера (особливо механізм уваги) зараз успішно застосовуються і в інших галузях, наприклад, у комп'ютерному зорі (**Vision Transformer - ViT**) та біоінформатиці.

---

**Підсумки**

* RNN, незважаючи на LSTM/GRU, мають обмеження в паралелізації та обробці дуже довгих залежностей.
* **Механізм уваги (Attention)** дозволяє моделі динамічно фокусуватися на релевантних частинах вхідних даних при обробці кожного елемента, долаючи обмеження відстані. **Сама-увага (Self-Attention)** аналізує взаємозв'язки всередині однієї послідовності.
* **Трансформер** – це архітектура, що базується переважно на механізмах уваги (особливо сама-уваги) та повнозв'язних мережах, відмовляючись від рекуренсії. Вона добре паралелізується та ефективно моделює залежності. Потребує **позиційного кодування** для врахування порядку слів.
* Трансформери стали домінуючою архітектурою в сучасному NLP, лягли в основу революційних моделей, таких як **BERT** та **GPT**, і знаходять застосування в дедалі ширшому колі задач ШІ.

Розуміння механізму уваги та архітектури Трансформер є ключовим для орієнтування в сучасному ландшафті глибокого навчання, особливо в обробці природної мови.

**Наступні кроки**

* Подумайте, чому можливість паралельної обробки є такою важливою для тренування великих моделей.
* Які переваги дає використання попередньо навчених моделей типу BERT чи GPT для вирішення конкретних NLP задач?
* На наступних лекціях ми можемо обговорити інші важливі теми, такі як навчання без учителя, генеративні моделі або етичні аспекти штучного інтелекту.

---

Дякую за увагу! Я готовий відповісти на ваші запитання.