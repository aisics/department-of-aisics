**Лекція 19.1: Вступ до генеративних моделей (окрім LLM)**

**Мета лекції:** Розширити знання про генеративні моделі за межі великих мовних моделей (LLM). Познайомити слухачів з двома іншими впливовими класами генеративних архітектур глибокого навчання: Варіаційними автокодувальниками (VAE) та Генеративними змагальними мережами (GAN). Пояснити їхні основні принципи роботи та сфери застосування.

---

Доброго дня! Минулого разу ми говорили про великі мовні моделі, які демонструють вражаючі здібності до **генерування** людського тексту. Сьогодні ми продовжимо тему **генеративних моделей**, але подивимось на інші підходи, які також навчилися створювати нові дані (особливо зображення, але не тільки), що виглядають дуже реалістично.

Моделі, які ми розглядали раніше для класифікації чи регресії (наприклад, логістична регресія, SVM, звичайні CNN), називаються **дискримінативними**. Вони вчаться знаходити межу між класами або прогнозувати значення, тобто моделюють умовну ймовірність $P(y|x)$. **Генеративні моделі**, навпаки, намагаються вивчити **саму структуру та розподіл ймовірностей вхідних даних $P(x)$**. Зрозумівши, як виглядають "типові" дані, вони можуть **генерувати нові приклади**, схожі на ті, на яких навчалися.

Ця здатність до "творчості" відкрила багато нових можливостей, і прогрес у цій сфері, який ми спостерігаємо сьогодні, 17 квітня 2025 року, просто вражає. Давайте розглянемо два ключових підходи (окрім LLM):

1.  **Варіаційні автокодувальники (Variational Autoencoders - VAEs).**
2.  **Генеративні змагальні мережі (Generative Adversarial Networks - GANs).**

---

**Варіаційні автокодувальники (Variational Autoencoders - VAEs)**

* **Зв'язок з автокодувальниками:** Звичайний автокодувальник – це нейронна мережа, що складається з **кодувальника (encoder)**, який стискає вхідні дані $x$ у представлення меншої розмірності (латентний код $z$), та **декодувальника (decoder)**, який намагається відновити початкові дані $\hat{x}$ з цього коду $z$. Мета – мінімізувати помилку реконструкції $||x - \hat{x}||^2$. Однак простір латентних кодів $z$ у звичайному автокодувальнику може бути неструктурованим, і генерація нових даних шляхом вибору випадкового $z$ часто дає погані результати.

* **Ідея VAE:** Замість того, щоб кодувати вхід $x$ в одну точку $z$, VAE кодує його в **розподіл ймовірностей** у латентному просторі. Зазвичай це багатовимірний нормальний (гауссів) розподіл, що характеризується **середнім значенням ($\mu$)** та **дисперсією ($\sigma^2$)**.
    * **Кодувальник (Encoder):** Приймає $x$ і видає параметри $\mu$ та $\sigma^2$.
    * **Латентний простір (Latent Space):** Простір прихованих представлень.
    * **Семплування:** З розподілу $N(\mu, \sigma^2)$, визначеного кодувальником, **випадковим чином** вибирається точка (семпл) $z$. Цей крок вносить стохастичність.
    * **Декодувальник (Decoder):** Приймає семпл $z$ і намагається відновити (згенеровати) вихід $\hat{x}$, схожий на початковий $x$.

* **Навчання VAE:** Оптимізується комбінована функція втрат, що складається з двох частин:
    1.  **Втрати реконструкції (Reconstruction Loss):** Наскільки добре декодер відновлює вхідні дані з латентного представлення (напр., MSE або перехресна ентропія). Змушує модель добре кодувати та декодувати.
    2.  **Втрати регуляризації (KL-дивергенція):** Штраф, який змушує розподіли $N(\mu, \sigma^2)$, що генеруються кодувальником для різних входів, бути **близькими** до стандартного нормального розподілу $N(0, 1)$. Це робить латентний простір **гладким та структурованим**, де близькі точки відповідають схожим об'єктам.

* **Генерація нових даних за допомогою VAE:**
    1.  Навчену модель використовуємо без кодувальника.
    2.  Генеруємо випадковий вектор $z$ зі стандартного нормального розподілу $N(0, 1)$.
    3.  Подаємо цей $z$ на вхід **декодувальника**.
    4.  Вихід декодувальника $\hat{x}$ і є новим, згенерованим прикладом даних.

* **Переваги VAE:**
    * Мають міцне теоретичне підґрунтя (базуються на варіаційному висновку).
    * Навчання зазвичай стабільне.
    * Латентний простір добре організований, що корисно для інтерполяції між даними.
    * Дозволяють отримати латентне представлення для існуючих даних.

* **Недоліки VAE:**
    * Згенеровані зображення часто виглядають **менш чіткими та більш "розмитими"** порівняно з найкращими результатами GAN.
    * Математика функції втрат (KL-дивергенція) може бути складною для повного розуміння.

---

**Генеративні змагальні мережі (GANs - Generative Adversarial Networks)**

Представлені Яном Гудфеллоу та колегами у 2014 році, GAN використовують зовсім інший, **змагальний підхід** до генерації даних.

* **Ключова ідея:** Навчити **дві нейронні мережі**, які "змагаються" одна з одною:
    1.  **Генератор (Generator, G):** Намагається створити **фальшиві дані** (напр., зображення) з випадкового шуму (вектора $z$), які були б не відрізнити від справжніх. Його мета – **обдурити** Дискримінатор.
    2.  **Дискримінатор (Discriminator, D):** Намагається **відрізнити** справжні дані (з навчального набору) від фальшивих даних, згенерованих Генератором. Його мета – **правильно класифікувати** вхідні дані як "справжні" (real) або "фальшиві" (fake).

* **Аналогія:** Уявіть собі гру між **фальшивомонетником (Генератор)** та **поліцейським детективом (Дискримінатор)**.
    * Фальшивомонетник вчиться робити все кращі підробки, аналізуючи, чому його попередні спроби були викриті.
    * Детектив вчиться все краще розпізнавати підробки, бачачи як справжні гроші, так і спроби фальшивомонетника.
    З часом обидва стають майстернішими у своїй справі.

* **Процес навчання (ітеративний):**
    1.  **Навчання Дискримінатора:**
        * Взяти партію справжніх даних та партію фальшивих даних (згенерованих поточним Генератором).
        * Навчити Дискримінатор правильно їх класифікувати (напр., справжні = 1, фальшиві = 0), оновлюючи **лише ваги Дискримінатора**.
    2.  **Навчання Генератора:**
        * Згенерувати нову партію фальшивих даних.
        * Пропустити їх через **"заморожений" Дискримінатор** (його ваги на цьому кроці не змінюються).
        * Обчислити функцію втрат для Генератора, яка показує, наскільки добре йому вдалося **обдурити** Дискримінатор (тобто наскільки близьким до 1 був вихід Дискримінатора для фальшивих даних).
        * Оновити **лише ваги Генератора** так, щоб він генерував дані, які Дискримінатор з більшою ймовірністю класифікує як справжні.
    3.  **Повторення:** Кроки 1 і 2 повторюються багато разів.

* **Результат:** В ідеалі, Генератор навчиться створювати настільки реалістичні дані, що Дискримінатор не зможе відрізнити їх від справжніх (його точність впаде до 50%, як при випадковому вгадуванні).

* **Переваги GAN:**
    * Здатні генерувати **надзвичайно реалістичні та чіткі** зображення (та інші типи даних), часто досягаючи передової якості.
    * Не вимагають прямого порівняння пікселів (як у втратах реконструкції VAE).

* **Недоліки GAN:**
    * **Нестабільність навчання:** Навчання GAN може бути дуже складним, вимагає ретельного налаштування архітектур та гіперпараметрів. Поширені проблеми:
        * **Колапс мод (Mode Collapse):** Генератор "зациклюється" на генерації лише кількох типів реалістичних зразків, ігноруючи різноманітність даних.
        * **Розбіжність:** Генератор і Дискримінатор не досягають стабільного стану.
    * **Немає прямого кодувальника:** Стандартний GAN не дає можливості отримати латентне представлення $z$ для вже існуючого зображення $x$.
    * Складність оцінки якості генерації.

* **Варіанти GAN:** Існує величезна кількість модифікацій GAN (DCGAN, WGAN, StyleGAN, CycleGAN, BigGAN та ін.), спрямованих на покращення стабільності навчання та якості генерації.

---

**VAE vs GAN: Коротка порівняльна таблиця**

| Характеристика      | VAE (Варіаційний Автокодувальник) | GAN (Генеративна Змагальна Мережа) |
| :------------------ | :--------------------------------- | :--------------------------------- |
| **Основна ідея** | Ймовірнісне кодування/декодування | Змагання Генератора та Дискримінатора |
| **Навчання** | Зазвичай стабільне                 | Може бути нестабільним             |
| **Якість генерації**| Часто "розмитіші" зразки          | Часто дуже чіткі, реалістичні зразки |
| **Латентний простір**| Навчається гладкий, структурований | Неявно визначений, менш структурований |
| **Кодувальник** | Є (для отримання $z$ з $x$)       | Немає у базовій версії           |
| **Функція втрат** | Реконструкція + KL-дивергенція   | Змагальна (мінмакс гра)             |

---

**Підсумки**

* **Генеративні моделі** вчаться відтворювати розподіл даних, що дозволяє їм створювати нові, раніше не бачені зразки.
* **VAE** використовують ймовірнісний підхід з кодувальником та декодувальником, навчаючись створювати структурований латентний простір та генерувати нові дані шляхом семплування з нього.
* **GAN** використовують змагальний підхід між **Генератором** (створює фальшиві дані) та **Дискримінатором** (намагається їх викрити), що часто призводить до генерації дуже реалістичних зразків, але може бути нестабільним у навчанні.
* Ці моделі відкрили нові горизонти в креативному ШІ, доповненні даних, моделюванні складних розподілів та багатьох інших застосуваннях.

Розуміння принципів VAE та GAN дає уявлення про різноманітність підходів до генеративного моделювання в глибокому навчанні.

**Наступні кроки**

* Пошукайте приклади зображень, згенерованих за допомогою VAE та GAN (напр., StyleGAN). Порівняйте їхню якість та стиль.
* Подумайте, які потенційні проблеми (окрім нестабільності навчання GAN) можуть виникати при використанні генеративних моделей (напр., створення діпфейків)?
* Ми завершуємо наш огляд основних концепцій ШІ. На фінальній лекції ми обговоримо етичні та соціальні аспекти штучного інтелекту, а також поглянемо на майбутні напрямки його розвитку.

---

Дякую за увагу! Не забуваємо про вподобайку, поширення та чесний відгук чи запитання в коментарях, до зустрічі!