Спробуємо візуалізувати цю ідею векторного представлення слів (word embeddings) на простому 2D прикладі для слів "яблуко", "груша" та "автомобіль".

**Евклідова відстань** – це те, що ми зазвичай інтуїтивно розуміємо під "відстанню" між двома точками на площині або у просторі. Це довжина прямої лінії, що з'єднує ці дві точки.

**Розрахунок:**

1.  **На площині (у 2D):**
    Якщо у вас є дві точки $P_1$ з координатами $(x_1, y_1)$ та $P_2$ з координатами $(x_2, y_2)$, то евклідова відстань $d(P_1, P_2)$ між ними обчислюється за теоремою Піфагора:
    * Знаходимо різницю координат по кожній осі: $(x_2 - x_1)$ та $(y_2 - y_1)$.
    * Підносимо кожну різницю до квадрату: $(x_2 - x_1)^2$ та $(y_2 - y_1)^2$.
    * Сумуємо ці квадрати: $(x_2 - x_1)^2 + (y_2 - y_1)^2$.
    * Беремо квадратний корінь з отриманої суми.

    Формула:
    $$d(P_1, P_2) = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}$$

2.  **У багатовимірному просторі (N вимірів):**
    Ідея та сама, просто додаються координати для всіх вимірів. Якщо у вас є дві точки (або вектори) у N-вимірному просторі:
    $P = (p_1, p_2, ..., p_n)$
    $Q = (q_1, q_2, ..., q_n)$
    То евклідова відстань $d(P, Q)$ обчислюється так:
    * Знаходимо різницю для кожної компоненти: $(q_1 - p_1), (q_2 - p_2), ..., (q_n - p_n)$.
    * Підносимо кожну різницю до квадрату: $(q_1 - p_1)^2, (q_2 - p_2)^2, ..., (q_n - p_n)^2$.
    * Сумуємо всі ці квадрати: $\sum_{i=1}^{n} (q_i - p_i)^2$.
    * Беремо квадратний корінь з суми.

    Формула:
    $$d(P, Q) = \sqrt{(q_1 - p_1)^2 + (q_2 - p_2)^2 + ... + (q_n - p_n)^2} = \sqrt{\sum_{i=1}^{n} (q_i - p_i)^2}$$

Уявімо, що ми змогли стиснути всю складність значення слова лише до **двох основних семантичних вимірів (осей)**. Звичайно, в реальності таких вимірів сотні, і вони не мають таких простих назв, але для ілюстрації візьмемо:

1.  **Вісь X: "Природність"** (Наскільки об'єкт є чимось природним)
    * Високі позитивні значення (+) = Дуже схоже на природне.
    * Високі негативні значення (-) = Штучне.
2.  **Вісь Y: "Солодкість"** (Наскільки об'єкт солодкий)
    * Високі позитивні значення (+) = Солодке.
    * Високі негативні значення (-) = Не солодке / Концепція нерелевантна.

Тепер присвоїмо *ілюстративні* координати (компоненти векторів) нашим словам у цьому 2D просторі:

* **Яблуко:**
    * Це фрукт (а отже висока "Природність"): **X = 0.8**
    * Зазвичай солодке (висока "Солодкість"): **Y = 0.7**
    * **Вектор("Яблуко") ≈ (0.8, 0.7)**

* **Груша:**
    * Також фрукт, дуже схожий на яблуко (висока "Фруктовість"): **X = 0.75** (трохи менше, можливо, через форму чи інший відтінок "фруктовості", але все ще близько до яблука)
    * Також солодка (висока "Солодкість"): **Y = 0.65** (знову ж таки, трохи інакше, але близько)
    * **Вектор("Груша") ≈ (0.75, 0.65)**

* **Автомобіль:**
    * Штучний (низька/від'ємна "Природність"): **X = -0.9**
    * Концепції солодкості абсолютно нерелевантні (низька/від'ємна "Солодкість/Дерево"): **Y = -0.8**
    * **Вектор("Автомобіль") ≈ (-0.9, -0.8)**

**Наочна демонстрація (уявний графік):**

Якщо ми намалюємо ці точки на графіку з осями X ("Фруктовість") та Y ("Солодкість/Дерево"):

```
        ^ Y (Солодкість)
        |
      1 +
        |       Яблуко(0.8, 0.7)
        |      *
        |     * Груша(0.75, 0.65)
      0 +---------------------------> X (Природність)
        |       |       |       |
   -1 + * Автомобіль(-0.9,-0.8)
        |
        |
```

* **Як компоненти утворюють "яблуко"?** Вектор (0.8, 0.7) не "утворює" яблуко в буквальному сенсі, але він *кодує* його ключові семантичні властивості у вибраних нами вимірах. Високе значення по осі X каже нам: "це щось дуже схоже на природний предмет", а високе значення по осі Y додає: "і воно солодке". Комбінація цих числових значень унікально позиціонує "яблуко" у нашому семантичному просторі.
* **Те саме для "груші":** Вектор (0.75, 0.65) так само кодує властивості груші – "природність" та "солодкість".
* **Чому "яблуко" і "груша" схожі?** Подивіться на графік! Точки, що представляють "яблуко" та "грушу", знаходяться **дуже близько** одна до одної. 
* **Чому "автомобіль" відрізняється?** Його вектор (-0.9, -0.8) знаходиться зовсім в іншій частині графіка – далеко від векторів фруктів. Його компоненти мають протилежні знаки та зовсім інші значення. Це відображає велику семантичну відстань між поняттями "фрукт" та "автомобіль".

**Приклад з нашими векторами:**

Давайте обчислимо відстані між векторами, які ми визначили для слів у нашому 2D прикладі, щоб ще краще зрозуміти наскільки ж близькі ці поняття:

* $Вектор(\text{"Яблуко"}) = P_1 = (0.8, 0.7)$
* $Вектор(\text{"Груша"}) = P_2 = (0.75, 0.65)$
* $Вектор(\text{"Автомобіль"}) = P_3 = (-0.9, -0.8)$

1.  **Відстань між "Яблуко" та "Груша":**
    $d(P_1, P_2) = \sqrt{(0.75 - 0.8)^2 + (0.65 - 0.7)^2}$
    $d(P_1, P_2) = \sqrt{(-0.05)^2 + (-0.05)^2}$
    $d(P_1, P_2) = \sqrt{0.0025 + 0.0025}$
    $d(P_1, P_2) = \sqrt{0.005} \approx 0.0707$
    *Це дуже мала відстань, що підтверджує їхню близькість.*

2.  **Відстань між "Яблуко" та "Автомобіль":**
    $d(P_1, P_3) = \sqrt{(-0.9 - 0.8)^2 + (-0.8 - 0.7)^2}$
    $d(P_1, P_3) = \sqrt{(-1.7)^2 + (-1.5)^2}$
    $d(P_1, P_3) = \sqrt{2.89 + 2.25}$
    $d(P_1, P_3) = \sqrt{5.14} \approx 2.267$
    *Це значно більша відстань, що показує семантичну віддаленість.*

Отже, евклідова відстань дає нам простий і зрозумілий спосіб виміряти, наскільки "далеко" знаходяться вектори (а отже, і поняття, які вони представляють) один від одного у багатовимірному просторі.

**Важливо пам'ятати:**

* Це лише **ілюстрація** у 2D. Справжні word embeddings мають сотні вимірів.
* Значення компонент та самі "назви" осей у реальних embeddings **не є такими інтерпретованими**. Вони вивчаються автоматично з величезних обсягів тексту і фіксують значно складніші та тонші відтінки значень та контекстуальних зв'язків.
* Але **основний принцип** залишається: семантично схожі слова мають близькі вектори у багатовимірному просторі.