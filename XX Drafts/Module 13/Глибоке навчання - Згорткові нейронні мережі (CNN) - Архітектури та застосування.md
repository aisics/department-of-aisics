**Лекція 13: Глибоке навчання - Згорткові нейронні мережі (CNN) - Архітектури та застосування**

**Мета лекції:** Розглянути, як базові блоки згорткових нейронних мереж (згорткові шари, шари пулінгу), вивчені минулого разу, об'єднуються у повноцінні архітектури. Ознайомитися з деякими історично важливими та впливовими архітектурами CNN (LeNet, AlexNet, VGG). Обговорити основні сфери застосування CNN у комп'ютерному зорі та ввести концепцію трансферного навчання як потужного підходу для практичного використання CNN.

---

Доброго дня! На попередній лекції ми розібрали ключові "цеглинки" згорткових нейронних мереж: згорткові шари з їх фільтрами, кроком та доповненням, а також шари пулінгу. Ми дізналися, як ці компоненти дозволяють мережі виявляти локальні ознаки та зменшувати розмірність даних, зберігаючи важливу інформацію.

Сьогодні ми побачимо, як ці "цеглинки" складаються у потужні структури. Ми:

1.  Розглянемо **ключові архітектури CNN**, які стали віхами в історії комп'ютерного зору.
2.  Обговоримо **основні застосування** CNN.
3.  Познайомимось із надзвичайно важливою концепцією **трансферного навчання (transfer learning)**.

---

**Архітектура CNN для задач комп'ютерного зору (Огляд LeNet, AlexNet, VGG)**

Побудова ефективної CNN – це мистецтво вибору правильної комбінації шарів, розмірів фільтрів, кількості фільтрів, функцій активації тощо. Історія розвитку CNN – це історія пошуку все більш глибоких та ефективних архітектур. Розглянемо декілька знакових прикладів:

1.  **LeNet-5 (Ян ЛеКун та ін., 1990-ті):**
    * **Контекст:** Одна з **перших успішних** CNN, що продемонструвала високу точність у розпізнаванні **рукописних цифр** (на відомому датасеті MNIST).
    * **Архітектура:** За сучасними мірками досить проста. Складалася з послідовності шарів: Згортковий -> Активація (Sigmoid/Tanh) -> Пулінг (середній) -> Згортковий -> Активація -> Пулінг -> Повнозв'язний -> Повнозв'язний -> Вихідний (Softmax).
    * **Ключові ідеї:** Встановила **стандартну структуру** CNN (чергування згорткових та пулінгових шарів). Показала ефективність вивчення ознак безпосередньо з даних та наскрізного навчання (end-to-end learning).

2.  **AlexNet (Олексій Крижевський та ін., 2012):**
    * **Контекст:** **Проривна** архітектура, яка з великим відривом перемогла у престижному змаганні **ImageNet Large Scale Visual Recognition Challenge (ILSVRC)**. Ця перемога стала каталізатором сучасної революції глибокого навчання.
    * **Архітектура:** Значно глибша та ширша за LeNet. Складалася з 5 згорткових шарів та 3 повнозв'язних шарів.
    * **Інновації та фактори успіху:**
        * **Використання ReLU:** Замість Sigmoid/Tanh була використана функція активації ReLU, яка значно прискорила навчання та допомогла уникнути проблеми затухаючих градієнтів.
        * **Використання Dropout:** Техніка регуляризації (про яку ми говорили минулого разу) застосовувалася до повнозв'язних шарів для боротьби з перенавчанням.
        * **Аугментація даних:** Штучне збільшення обсягу навчальних даних (наприклад, обрізання, віддзеркалення зображень).
        * **Тренування на GPU:** Використання графічних процесорів (GPU) зробило можливим тренування такої великої мережі на величезному датасеті ImageNet (понад мільйон зображень) за прийнятний час.
    * **Значення:** AlexNet довела, що глибокі згорткові мережі, навчені на великих обсягах даних, можуть досягати надзвичайної точності у складних задачах комп'ютерного зору.

3.  **VGGNet (Сімонян та Зіссерман, Оксфорд, 2014):**
    * **Контекст:** Дослідження впливу **глибини** мережі на її продуктивність. Показали, що збільшення глибини (кількості шарів) позитивно впливає на точність.
    * **Архітектура:** Характеризується **однорідністю та простотою**. Використовувалися лише **маленькі згорткові фільтри розміром 3x3** (з кроком 1 та доповненням 1, що зберігає розмір карти ознак) та шари **макс-пулінгу 2x2** (з кроком 2, що зменшує розміри вдвічі). Ці блоки (кілька згорток 3x3 + макс-пулінг) повторювалися. Найвідоміші варіанти – VGG-16 та VGG-19 (число вказує на кількість шарів з вагами).
    * **Ключові ідеї:** Глибина має значення! Використання стеків малих фільтрів (3x3) є ефективним – кілька шарів 3x3 мають таке ж "ефективне рецептивне поле", як один більший фільтр, але додають більше нелінійності і мають менше параметрів. Проста та уніфікована структура зробила VGG дуже впливовою.

* **Подальший розвиток:** Після VGG з'явилися ще більш складні та ефективні архітектури, такі як **GoogLeNet (Inception)** (використання "модулів" з паралельними згортками різних розмірів), **ResNet (Residual Networks)** (введення "залишкових з'єднань" або skip-connections, що дозволило тренувати значно глибші мережі – понад 100 шарів – долаючи проблему затухаючих градієнтів), **DenseNet**, **EfficientNet** та багато інших. Загальний тренд – до глибших, ширших та обчислювально ефективніших мереж.

---

**Застосування CNN**

Завдяки своїй здатності вивчати просторові ієрархії ознак, CNN стали домінуючим підходом у багатьох задачах, пов'язаних з аналізом візуальних даних:

1.  **Класифікація зображень (Image Classification):** Призначення одного мітки (класу) цілому зображенню. Наприклад, визначити, що зображено на фото: "кіт", "собака", "автомобіль". Саме на цій задачі змагались на ImageNet.
2.  **Детекція об'єктів (Object Detection):** Не лише класифікувати об'єкти на зображенні, але й визначити їхнє **місцезнаходження** за допомогою обмежувальних рамок (bounding boxes). Наприклад, знайти всіх людей та автомобілі на фотографії вулиці. Для цього використовуються спеціалізовані архітектури, що базуються на CNN (наприклад, YOLO, SSD, Faster R-CNN).
3.  **Сегментація зображень (Image Segmentation):** Класифікація **кожного пікселя** зображення. Дозволяє точно виділити межі об'єктів. Наприклад, на медичному знімку виділити область пухлини, або на фото вулиці розфарбувати різними кольорами дорогу, тротуар, будівлі, автомобілі. Використовуються архітектури типу U-Net, FCN.
4.  **Інші сфери:**
    * Розпізнавання облич.
    * Аналіз медичних зображень (рентген, МРТ, КТ).
    * Системи допомоги водію та безпілотні автомобілі (аналіз дорожньої сцени).
    * Генерування зображень (наприклад, у Generative Adversarial Networks - GAN).
    * Аналіз відео (розглядаючи відео як послідовність кадрів).
    * Навіть деякі задачі обробки природної мови (NLP), де текст розглядається як 1D-послідовність.

---

**Трансферне навчання (Transfer Learning) у комп'ютерному зорі**

* **Проблема:** Навчання глибоких CNN (як AlexNet, VGG, ResNet) "з нуля" вимагає величезних обсягів розмічених даних (сотні тисяч або мільйони зображень) та значних обчислювальних ресурсів (багато годин або днів тренування на потужних GPU). Для багатьох практичних задач таких ресурсів немає.

* **Ідея трансферного навчання:** **Перевикористати знання**, отримані моделлю, що була навчена на великому, загальному датасеті (наприклад, ImageNet), для вирішення **іншої, специфічної задачі**, для якої у нас є **менше даних**.

* **Чому це працює (особливо добре для CNN у комп'ютерному зорі):**
    * CNN вивчають **ієрархічні ознаки**.
    * **Ранні шари** (ближчі до входу) зазвичай вивчають **базові, загальні ознаки**: детектори країв, кутів, кольорових плям, простих текстур. Ці ознаки корисні практично для будь-якої задачі комп'ютерного зору.
    * **Пізніші шари** (ближчі до виходу) вивчають більш **складні та специфічні ознаки**: комбінації простих ознак, частини об'єктів, специфічні форми. Ці ознаки можуть бути більш залежними від початкової задачі (наприклад, розпізнавання 1000 класів ImageNet).

* **Стратегії трансферного навчання:**
    1.  **Використання як екстрактора ознак (Fixed Feature Extractor):**
        * Взяти готову CNN, попередньо навчену на ImageNet (наприклад, VGG16).
        * "Відрізати" її верхні повнозв'язні шари (класифікатор).
        * Пропустити **свої дані** через "заморожену" згорткову частину мережі (тобто ваги згорткових шарів не змінюються).
        * Виходи (активації) одного з останніх згорткових шарів використати як **нові ознаки** для своїх даних.
        * Навчити **новий, простий класифікатор** (наприклад, логістичну регресію, SVM або невеликий MLP) поверх цих витягнутих ознак, використовуючи свій (відносно невеликий) набір даних.
    2.  **Доуточнення (Fine-tuning):**
        * Взяти готову, попередньо навчену CNN.
        * Замінити її верхні повнозв'язні шари на нові, що відповідають **вашій** задачі (наприклад, якщо у вас 10 класів, а не 1000).
        * **Ініціалізувати** ваги всієї мережі (або принаймні згорткової частини) **попередньо навченими** значеннями.
        * **Продовжити навчання** (тренування) **всієї мережі** (або лише кількох останніх шарів) на **ваших даних**, але з **дуже малою швидкістю навчання (learning rate)**.
        * Це дозволяє мережі трохи "адаптувати" вивчені ознаки (особливо більш специфічні ознаки пізніх шарів) до особливостей вашого датасету. Зазвичай дає кращі результати, ніж екстракція ознак, якщо у вас є достатньо даних для доуточнення.

* **Переваги трансферного навчання:**
    * Дозволяє досягти високої точності навіть з **обмеженою кількістю даних** для цільової задачі.
    * **Прискорює процес розробки** та навчання.
    * Часто дає **кращі результати**, ніж навчання моделі "з нуля" на малих даних.

Трансферне навчання стало **стандартним підходом** у сучасному комп'ютерному зорі та все частіше застосовується в інших галузях ШІ.

---

**Підсумки**

* Архітектури CNN еволюціонували від ранніх моделей ( **LeNet-5**) до глибоких мереж (**AlexNet, VGG, ResNet** та ін.), що стало можливим завдяки інноваціям у функціях активації (ReLU), регуляризації (Dropout), використанню GPU та розумінню важливості глибини.
* CNN мають **широке застосування** у комп'ютерному зорі, включаючи класифікацію, детекцію об'єктів, сегментацію та багато іншого.
* **Трансферне навчання** є потужною технікою, що дозволяє ефективно використовувати попередньо навчені CNN для нових задач, особливо при обмежених даних, значно прискорюючи розробку та покращуючи результати.

Розуміння цих архітектур та підходів є ключовим для практичного застосування глибокого навчання у вирішенні реальних задач.

**Наступні кроки**

* Пошукайте приклади використання CNN у новинах або продуктах, якими ви користуєтесь (наприклад, розпізнавання облич у смартфоні, рекомендації товарів на основі фото).
* Подумайте, чи могли б ви використати трансферне навчання для якоїсь гіпотетичної задачі з обмеженими даними.
* На наступній лекції ми можемо звернути увагу на інший важливий клас нейронних мереж – рекурентні нейронні мережі (RNN), призначені для роботи з послідовними даними.

---

Дякую за увагу! Не забуваємо про вподобайку, поширення та чесний відгук чи запитання в коментарях, до зустрічі!