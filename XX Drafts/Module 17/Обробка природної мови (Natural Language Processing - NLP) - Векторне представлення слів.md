**Лекція 17: Обробка природної мови (NLP) - Векторне представлення слів**

**Мета лекції:** Ознайомити слухачів з основами обробки природної мови (Natural Language Processing - NLP) – галузі ШІ, що займається взаємодією комп'ютерів та людської мови. Розглянути ключові задачі NLP та фундаментальну проблему представлення слів у числовому форматі, придатному для моделей машинного навчання. Детально обговорити концепцію векторного представлення слів (word embeddings) та популярні методи їх створення (Word2Vec, GloVe).

---

Доброго дня! На попередніх лекціях ми багато говорили про моделі для обробки структурованих даних (MLP), зображень (CNN) та загальних послідовностей (RNN, LSTM, GRU). Сьогодні ми зосередимось на одній з найважливіших та найскладніших типів даних – **людській мові**. Галузь штучного інтелекту, що займається розумінням, інтерпретацією та генеруванням людської мови (української, англійської, будь-якої іншої), називається **Обробкою Природної Мови (Natural Language Processing - NLP)**.

Мова є надзвичайно складним явищем – вона неоднозначна, залежить від контексту, сповнена нюансів та культурних особливостей. Навчити комп'ютер "розуміти" мову – це величезний виклик.

Сьогодні ми:

1.  Розглянемо **основні задачі**, які вирішує NLP.
2.  Обговоримо критично важливий перший крок: як представити **слова у вигляді чисел**, зрозумілих для комп'ютера, при цьому зберігши їхнє значення? Поговоримо про **векторне представлення слів (word embeddings)**.

---

**Основні задачі обробки природної мови (NLP)**

NLP охоплює широке коло завдань, що дозволяють комп'ютерам виконувати корисні дії з текстом та мовленням:

1.  **Класифікація тексту (Text Classification):** Віднесення текстового документа до однієї чи кількох заздалегідь визначених категорій.
    * *Приклади:* Фільтрація спаму (листи поділяються на "спам" та "не спам"); визначення тематики новини ("політика", "спорт", "технології", "культура"); ідентифікація мови тексту.
2.  **Аналіз тональності (Sentiment Analysis):** Визначення емоційного забарвлення або думки, висловленої в тексті (позитивна, негативна, нейтральна).
    * *Приклади:* Аналіз відгуків клієнтів про товар чи послугу (наприклад, про готель чи ресторан у Львові); моніторинг згадок бренду в соціальних мережах; оцінка суспільної думки щодо події.
3.  **Машинний переклад (Machine Translation):** Автоматичний переклад тексту з однієї природної мови на іншу.
    * *Приклади:* Сервіси типу Google Translate, DeepL; переклад веб-сторінок, документів. Переклад з української на англійську і навпаки – яскравий приклад.
4.  **Генерування тексту (Text Generation):** Створення нового тексту, що виглядає як написаний людиною.
    * *Приклади:* Автоматичне написання статей (на основі даних), генерування описів товарів, створення діалогів для чат-ботів, автодоповнення тексту при введенні (як у пошукових системах).
5.  **Відповіді на запитання (Question Answering - QA):** Надання відповіді на запитання, сформульоване природною мовою. Відповідь може базуватися на наданому тексті (контексті) або на знаннях, отриманих з великого корпусу текстів.
    * *Приклади:* Голосові асистенти (Siri, Google Assistant); пошук інформації в базах знань чи документації.

* **Інші важливі задачі:** Розпізнавання іменованих сутностей (Named Entity Recognition - NER, знаходження імен, дат, організацій), визначення частин мови (Part-of-Speech - POS tagging), автоматичне реферування (Summarization), побудова діалогових систем (Chatbots) тощо.

---

**Потреба у числовому представленні слів**

Моделі машинного навчання та глибокого навчання працюють з числами (векторами, матрицями). Як "подати" слова на вхід таким моделям?

* **Прості (наївні) підходи та їх недоліки:**
    1.  **Унікальні ID:** Присвоїти кожному слову унікальний номер (напр., "кіт": 1, "пес": 2, "Львів": 3).
        * *Проблема:* Ніяк не відображає семантичну близькість. Числа 1 і 2 так само далекі, як 1 і 3.
    2.  **One-Hot Encoding (Гаряче кодування):** Створити вектор, довжина якого дорівнює розміру всього словника (напр., 10 000 або 100 000 слів). Для кожного слова вектор складається з нулів, крім однієї позиції (що відповідає індексу цього слова), де стоїть одиниця.
        * *Приклад:* Якщо словник ["кіт", "пес", "Львів"], то "кіт" -> `[1, 0, 0]`, "пес" -> `[0, 1, 0]`, "Львів" -> `[0, 0, 1]`.
        * *Проблеми:*
            * **Величезна розмірність:** Вектори стають дуже довгими.
            * **Розрідженість:** Майже всі елементи вектора – нулі.
            * **Відсутність семантики:** Всі вектори ортогональні один до одного. Скалярний добуток `вектор("кіт") · вектор("пес") = 0`, хоча ці слова семантично ближчі, ніж "кіт" та "Львів". Не відображає жодних смислових зв'язків.

* **Чого ми хочемо?** Нам потрібне таке числове представлення слів, яке:
    * Має **відносно низьку розмірність** (напр., 50-300 замість десятків тисяч).
    * Є **щільним** (dense), тобто більшість значень ненульові.
    * **Відображає семантичну близькість:** Слова зі схожим значенням або ті, що вживаються у схожих контекстах, повинні мати "близькі" вектори у цьому новому просторі.

---

**Векторне представлення слів (Word Embeddings)**

Це клас методів, що дозволяють представити слова у вигляді **щільних векторів низької розмірності**, які фіксують семантичні та синтаксичні відношення між словами.

* **Ключова ідея:** Навчитися представляти кожне слово як вектор дійсних чисел (напр., 300-вимірний вектор) так, щоб вектори слів зі схожими значеннями були близькими у векторному просторі (наприклад, мали високу косинусну подібність).

* **Гіпотеза дистрибутивної семантики:** В основі багатьох методів лежить принцип: "Значення слова визначається його оточенням" (John Rupert Firth). Тобто слова, що часто зустрічаються у схожих контекстах (поруч з однаковими словами), ймовірно, мають схожі значення.

* **Популярні методи:**

    1.  **Word2Vec (Міколов та ін., Google, 2013):**
        * **Підхід:** Навчає векторні представлення, намагаючись передбачити слова на основі їхнього контексту (або навпаки) у великому корпусі текстів. Не є єдиним алгоритмом, а скоріше набором з двох основних архітектур:
            * **CBOW (Continuous Bag-of-Words):** Модель намагається **передбачити центральне слово** на основі навколишніх слів (контексту). *Вхід:* вектори слів контексту. *Вихід:* вектор цільового слова.
            * **Skip-gram:** Модель намагається **передбачити слова контексту** на основі центрального слова. *Вхід:* вектор цільового слова. *Вихід:* вектори слів контексту. Зазвичай працює краще для рідкісних слів і вважається дещо якіснішим, хоча навчається повільніше.
        * **Як навчається (концептуально):** Використовується проста нейронна мережа (часто з одним прихованим шаром). В процесі навчання на задачі передбачення слова/контексту, ваги цієї мережі (особливо ваги, що пов'язують вхідні слова з прихованим шаром) оптимізуються. Саме ці вивчені **ваги і стають векторними представленнями (ембедінгами)** слів.
        * **Результат:** Отримані вектори дивовижним чином захоплюють семантичні відношення. Класичний приклад: `вектор('король') - вектор('чоловік') + вектор('жінка') ≈ вектор('королева')`. Можуть фіксуватися й інші аналогії, наприклад, столиці країн: `вектор('Париж') - вектор('Франція') + вектор('Україна') ≈ вектор('Київ')`.

    2.  **GloVe (Global Vectors for Word Representation) (Пеннінгтон, Сочер, Меннінг, Стенфорд, 2014):**
        * **Підхід:** На відміну від Word2Vec, що фокусується на локальному контексті, GloVe використовує **глобальну статистику співвживаності слів** у всьому корпусі текстів.
        * **Ідея:** Навчає вектори так, щоб їхній скалярний добуток був пов'язаний з логарифмом ймовірності того, що ці два слова з'являться разом (або близько одне до одного) у тексті.
        * **Результат:** Також генерує високоякісні векторні представлення, які добре фіксують семантичні аналогії.

* **Переваги Word Embeddings:**
    * Ефективно представляють семантичні відношення.
    * Мають відносно низьку розмірність.
    * Є щільними.
    * **Найважливіше:** Існують **попередньо навчені моделі** Word2Vec та GloVe (на величезних корпусах текстів, таких як Вікіпедія, Google News). Їх можна завантажити і використовувати у своїх NLP задачах, навіть якщо у вас небагато власних даних. Це приклад **трансферного навчання для NLP**.

* **Застосування:** Ці вектори слів потім слугують **вхідними даними** для більш складних моделей глибокого навчання (RNN, LSTM, GRU, CNN, Transformer), які вже вирішують конкретні NLP задачі (класифікацію, переклад тощо). Ембедінги надають моделям значно кращу "відправну точку", ніж прості ID або one-hot вектори.

---

**Підсумки**

* **NLP** – це галузь ШІ, що займається обробкою та розумінням людської мови, вирішуючи задачі від класифікації та аналізу тональності до машинного перекладу та генерування тексту.
* Для використання мовних даних в ML/DL моделях слова потрібно представити у числовому вигляді. Наївні методи (ID, one-hot encoding) неефективні та не передають семантики.
* **Векторне представлення слів (Word Embeddings)** – це спосіб представити слова як **щільні вектори низької розмірності**, де близькість векторів відображає семантичну близькість слів.
* **Word2Vec** (CBOW, Skip-gram) та **GloVe** – це популярні методи навчання таких векторів на великих текстових корпусах, що базуються на ідеї дистрибутивної семантики (аналіз контексту або глобальної співвживаності).
* Використання **попередньо навчених** word embeddings є стандартною практикою і важливим прикладом трансферного навчання в NLP.

Розуміння того, як слова перетворюються на вектори, є першим і надзвичайно важливим кроком до побудови ефективних систем обробки природної мови.

**Наступні кроки**

* Подумайте, які проблеми можуть виникати при використанні статичних word embeddings (наприклад, омоніми – слова з однаковим написанням, але різним значенням).
* Як можна представити не лише окремі слова, а й цілі речення чи документи?
* На наступній лекції ми поговоримо про архітектуру **Transformer** та **механізм уваги (Attention)**, які здійснили справжню революцію в NLP за останні роки і дозволили значно покращити якість моделей для багатьох задач.

---

Дякую за увагу! Не забуваємо про вподобайку, поширення та чесний відгук чи запитання в коментарях, до зустрічі!