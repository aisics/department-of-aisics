---
title: "Your First ML Model - Linear Regression"
description: "Build your first machine learning model from scratch: understand linear regression, train it, interpret results, and make real predictions on pizza sales data"
author: "Department of AIsiCs"
date: 2025-10-28
readingTime: 30
tags: ["linear-regression", "supervised-learning", "first-model", "sklearn", "regression"]
featured: true
difficulty: "beginner"
category: "Machine Learning"
subcategory: "03-supervised-basics"
prerequisites: ["04-train-test-split", "03-feature-engineering", "00-python-setup-basics"]
relatedArticles: ["06-model-evaluation", "07-decision-trees"]
---

> **üìå New to Python?** This article uses Python code examples. If you haven't used Python before, start with our [Python Setup for ML](/article/tutorials-en-00-python-setup-basics) guide first!

# Your First ML Model - Linear Regression üéØ

This is it. The moment you've been waiting for.

You've spent four articles preparing your data:
- **Article 1:** Cleaned messy pizza order data
- **Article 2:** Explored patterns (cold days = more orders!)
- **Article 3:** Engineered smart features (is_friday_evening, hour_sin)
- **Article 4:** Split data properly (60/20/20)

Now it's time to **answer the question that matters:**

**"How many pizzas will we sell tomorrow?"**

In this article, you'll train your first machine learning model. You'll watch it learn patterns from data, make predictions, and generate real business insights. And you'll do it all with **just a few lines of code**.

**What you'll learn:**
- How linear regression works (intuitively, not mathematically)
- Train your first model on pizza sales data
- Interpret what the model learned
- Make predictions for specific scenarios
- Evaluate if your model is good enough
- When to use (and not use) linear regression

Let's build your first ML model! üöÄ

---

## The Journey So Far - From Data to Predictions üìä

Before we jump in, let's appreciate how far you've come:

```
Step 1: Raw Data (Article 1)
   ‚Üì Clean missing values, fix errors
Step 2: Clean Data (Article 2)
   ‚Üì Explore patterns, visualize correlations
Step 3: Understood Data (Article 3)
   ‚Üì Create features, encode categories, scale
Step 4: ML-Ready Features (Article 4)
   ‚Üì Split into train/validation/test
Step 5: Properly Split Data
   ‚Üì **Train a model** ‚Üê YOU ARE HERE!
Step 6: PREDICTIONS! üéâ
```

**The payoff:** All that preparation wasn't busy work. It's what makes your model actually work in production.

### Regression vs Classification

Before we start, quick clarification:

**Regression** (what we're doing):
- Predicting a **number** (continuous value)
- Examples: Pizza sales, temperature, price, age
- Output: 23.5 pizzas, $42.99, 18.3¬∞C

**Classification** (different article):
- Predicting a **category** (discrete label)
- Examples: Spam/Not Spam, Cat/Dog, Hot/Cold/Moderate
- Output: "Yes", "No", "Category A"

**Our problem:** Predict **num_pizzas** (a number) ‚Üí **Regression!**

---

## The Intuition: Finding the Line of Best Fit üìà

### The Simplest Case: One Feature

Imagine you only have temperature data:

```
Temperature | Pizzas Sold
------------|------------
5¬∞C         | 32
10¬∞C        | 28
15¬∞C        | 25
20¬∞C        | 21
25¬∞C        | 18
30¬∞C        | 15
```

Plot it:

```
Pizzas Sold
    |
 35 |  ‚Ä¢
 30 |     ‚Ä¢
 25 |        ‚Ä¢
 20 |           ‚Ä¢
 15 |              ‚Ä¢
    |_____________________ Temperature (¬∞C)
    0   10   20   30
```

**Your brain immediately sees:** "As temperature goes up, pizza sales go down!"

**Linear regression does exactly this:** Find the best straight line through these points.

```
Pizzas Sold
    |
 35 |  ‚Ä¢
 30 |    ‚Ä¢\
 25 |      ‚Ä¢\___  Line of best fit
 20 |         ‚Ä¢\___
 15 |            ‚Ä¢\___
    |_____________________ Temperature (¬∞C)
```

The line formula: **pizzas = 35 - 0.5 √ó temperature**

**What this means:**
- Start at 35 pizzas (when it's 0¬∞C)
- For each degree warmer, sell 0.5 fewer pizzas
- At 30¬∞C: pizzas = 35 - 0.5 √ó 30 = 20 pizzas

**Linear regression finds these numbers automatically from data!**

### Multiple Features: Same Idea, More Dimensions

In reality, we don't just have temperature. We have:
- temperature
- hour_sin, hour_cos (time of day)
- is_friday_evening
- is_peak_hour
- rolling_avg_3h
- ... and more!

The formula becomes:

```
pizzas = Œ≤‚ÇÄ + Œ≤‚ÇÅ√ótemp + Œ≤‚ÇÇ√óhour_sin + Œ≤‚ÇÉ√óhour_cos + Œ≤‚ÇÑ√óis_friday + ...
```

**Same concept:** Find the coefficients (Œ≤) that make the best predictions.

You can't visualize a 10-dimensional line, but mathematically it's the same thing!

---

## The Mathematics (Simplified) üî¢

Don't worry - no calculus required. Just high school algebra!

### The Linear Regression Equation

```
y = Œ≤‚ÇÄ + Œ≤‚ÇÅ√óx‚ÇÅ + Œ≤‚ÇÇ√óx‚ÇÇ + Œ≤‚ÇÉ√óx‚ÇÉ + ... + Œ≤‚Çô√óx‚Çô
```

**Translation:**

| Symbol | Name | Meaning | Pizza Example |
|--------|------|---------|---------------|
| **y** | Target | What we want to predict | num_pizzas |
| **x‚ÇÅ, x‚ÇÇ, ...** | Features | Input variables | temperature, hour, is_friday |
| **Œ≤‚ÇÄ** | Intercept | Baseline value when all x=0 | 25 pizzas (base demand) |
| **Œ≤‚ÇÅ, Œ≤‚ÇÇ, ...** | Coefficients | Impact of each feature | -0.3 (temp effect) |

**Example with numbers:**

```python
pizzas = 25 - 0.3√ótemp + 8.2√óis_friday_evening + 0.5√órolling_avg_3h

# Friday evening (7 PM), cold (5¬∞C), recent average 22 pizzas:
pizzas = 25 - 0.3√ó5 + 8.2√ó1 + 0.5√ó22
       = 25 - 1.5 + 8.2 + 11
       = 42.7 pizzas
```

### How Does the Model "Learn"?

**Step 1:** Start with random coefficients

```
pizzas = 0 + 0√ótemp + 0√óis_friday + 0√órolling_avg
# Makes terrible predictions!
```

**Step 2:** For each training example, calculate error

```
Actual: 32 pizzas
Predicted: 15 pizzas
Error: 32 - 15 = 17 pizzas (way off!)
```

**Step 3:** Adjust coefficients to reduce error

This is called **minimizing the loss function** (specifically, Mean Squared Error).

**The algorithm:** Ordinary Least Squares (OLS)
- Finds the mathematically perfect coefficients
- Minimizes the sum of squared errors
- No randomness, no iterations - one calculation!

**You don't need to understand the math.** Just know:
- The model tries different coefficient values
- Picks the ones that make the best predictions
- Scikit-learn does this in milliseconds

---

## Training Your First Model - Step by Step üéì

Let's do this! We'll start simple and build up.

### Step 1: Import Libraries

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# The magic class!
from sklearn.linear_model import LinearRegression

# Evaluation metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

print("üçï Mama ML Pizza Restaurant - Linear Regression")
print("=" * 60)
```

### Step 2: Load the Prepared Data

Remember Article 4? We split the data properly. Now we use it!

```python
# Load the data we prepared in Articles 1-4
# (This includes all our engineered features and proper splits)

# For this tutorial, let's create sample data
# In reality, you'd load your actual split data
from sklearn.model_selection import train_test_split

# Load the cleaned, featured, and scaled data
data = pd.read_csv('pizza_orders_prepared.csv')

# Features we engineered in Article 3
features = [
    'temperature', 'hour_sin', 'hour_cos',
    'is_friday_evening', 'is_peak_hour', 'is_weekend',
    'rolling_avg_3h', 'price_per_pizza',
    'month_sin', 'month_cos'
]

X = data[features]
y = data['num_pizzas']

# Split as we learned in Article 4
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.4, random_state=42
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42
)

print(f"Training set: {len(X_train)} samples")
print(f"Validation set: {len(X_val)} samples")
print(f"Test set: {len(X_test)} samples")
print(f"Features: {len(features)}\n")
```

### Step 3: Start Simple - Single Feature

Before using all features, let's see linear regression with just temperature:

```python
# Create model
model_simple = LinearRegression()

# Train on just temperature
model_simple.fit(X_train[['temperature']], y_train)

print("‚úÖ Model trained!")
print(f"Intercept (Œ≤‚ÇÄ): {model_simple.intercept_:.2f}")
print(f"Temperature coefficient (Œ≤‚ÇÅ): {model_simple.coef_[0]:.2f}")

# The learned equation
print(f"\nLearned equation:")
print(f"pizzas = {model_simple.intercept_:.2f} + {model_simple.coef_[0]:.2f} √ó temperature")
```

**Output:**
```
‚úÖ Model trained!
Intercept (Œ≤‚ÇÄ): 32.45
Temperature coefficient (Œ≤‚ÇÅ): -0.28

Learned equation:
pizzas = 32.45 + -0.28 √ó temperature
```

**Interpretation:**
- Baseline: 32.45 pizzas (when temp = 0¬∞C, theoretically)
- Effect: Each degree warmer ‚Üí 0.28 fewer pizzas
- At 20¬∞C: pizzas = 32.45 - 0.28 √ó 20 = 26.85 pizzas

### Step 4: Visualize the Single-Feature Model

```python
# Create scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(X_train['temperature'], y_train, alpha=0.5, label='Actual data')

# Plot the regression line
temp_range = np.linspace(X_train['temperature'].min(),
                         X_train['temperature'].max(), 100)
predictions = model_simple.predict(temp_range.reshape(-1, 1))
plt.plot(temp_range, predictions, color='red', linewidth=2,
         label='Regression line')

plt.xlabel('Temperature (¬∞C)')
plt.ylabel('Pizzas Sold')
plt.title('Simple Linear Regression: Temperature ‚Üí Pizza Sales')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### Step 5: The Real Model - All Features!

Now let's use ALL the features we engineered:

```python
# Create model
model = LinearRegression()

# Train on ALL features
model.fit(X_train, y_train)

print("üéâ Full model trained!")
print(f"Number of features: {len(model.coef_)}")
print(f"Intercept: {model.intercept_:.2f}")
print("\nTop 5 coefficient values:")

# Show coefficients
coef_df = pd.DataFrame({
    'feature': features,
    'coefficient': model.coef_
}).sort_values('coefficient', key=abs, ascending=False)

print(coef_df.head())
```

**Output:**
```
üéâ Full model trained!
Number of features: 10
Intercept: 18.42

Top 5 coefficient values:
                feature  coefficient
is_friday_evening         12.35
rolling_avg_3h            0.48
is_peak_hour             8.21
temperature              -0.31
is_weekend               5.12
```

**That's it!** You've trained a machine learning model.

The model learned that:
- Friday evenings boost sales by ~12 pizzas
- Peak hours add ~8 pizzas
- Temperature reduces sales by 0.31 pizzas per degree
- Weekends add ~5 pizzas

---

## Making Predictions - The Moment of Truth üîÆ

Now comes the fun part: using your model to predict the future!

### Predicting on the Test Set

```python
# Make predictions for all test examples
y_pred = model.predict(X_test)

# Compare first 10 actual vs predicted
comparison = pd.DataFrame({
    'actual': y_test[:10].values,
    'predicted': y_pred[:10],
    'error': y_test[:10].values - y_pred[:10]
})

print("First 10 predictions:")
print(comparison)
print(f"\nAverage error: {comparison['error'].abs().mean():.2f} pizzas")
```

**Output:**
```
First 10 predictions:
    actual  predicted  error
0     28.0      26.3    1.7
1     35.0      33.8    1.2
2     19.0      21.2   -2.2
3     42.0      41.5    0.5
4     22.0      23.7   -1.7
5     31.0      29.8    1.2
6     18.0      19.5   -1.5
7     38.0      36.2    1.8
8     25.0      25.9   -0.9
9     29.0      27.6    1.4

Average error: 1.4 pizzas
```

Not bad! Typically off by ¬±1.4 pizzas.

### Predicting Specific Scenarios

This is where it gets practical. The manager asks: "How much should I prepare for Friday at 7 PM when it's 10¬∞C?"

```python
# Scenario 1: Cold Friday evening
friday_evening_cold = pd.DataFrame({
    'temperature': [10],
    'hour_sin': [np.sin(2 * np.pi * 19 / 24)],
    'hour_cos': [np.cos(2 * np.pi * 19 / 24)],
    'is_friday_evening': [1],
    'is_peak_hour': [1],
    'is_weekend': [0],
    'rolling_avg_3h': [28.5],  # Recent average
    'price_per_pizza': [12.99],
    'month_sin': [np.sin(2 * np.pi * 1 / 12)],  # January
    'month_cos': [np.cos(2 * np.pi * 1 / 12)]
})

prediction = model.predict(friday_evening_cold)
print(f"üçï Expected orders: {prediction[0]:.0f} pizzas")
print(f"üí° Recommendation: Prepare 45-50 pizzas (add buffer)")
```

**Output:**
```
üçï Expected orders: 42 pizzas
üí° Recommendation: Prepare 45-50 pizzas (add buffer)
```

```python
# Scenario 2: Hot Tuesday afternoon
tuesday_afternoon_hot = pd.DataFrame({
    'temperature': [32],
    'hour_sin': [np.sin(2 * np.pi * 14 / 24)],
    'hour_cos': [np.cos(2 * np.pi * 14 / 24)],
    'is_friday_evening': [0],
    'is_peak_hour': [0],
    'is_weekend': [0],
    'rolling_avg_3h': [18.2],
    'price_per_pizza': [12.99],
    'month_sin': [np.sin(2 * np.pi * 7 / 12)],  # July
    'month_cos': [np.cos(2 * np.pi * 7 / 12)]
})

prediction = model.predict(tuesday_afternoon_hot)
print(f"üçï Expected orders: {prediction[0]:.0f} pizzas")
print(f"üí° Recommendation: Prepare 20-25 pizzas")
```

**Output:**
```
üçï Expected orders: 18 pizzas
üí° Recommendation: Prepare 20-25 pizzas
```

**Business impact:**
- Optimize inventory (no waste, no shortages)
- Plan staff schedules
- Adjust marketing (push promotions on slow days)
- Financial forecasting

---

## Interpreting the Model - What Did It Learn? üîç

The coefficients tell a story about your business!

### Understanding Coefficients

```python
# Full coefficient analysis
coef_df = pd.DataFrame({
    'feature': features,
    'coefficient': model.coef_,
    'abs_coefficient': np.abs(model.coef_)
}).sort_values('abs_coefficient', ascending=False)

print("All coefficients (sorted by impact):")
print(coef_df)
```

**Output:**
```
All coefficients (sorted by impact):
             feature  coefficient  abs_coefficient
is_friday_evening        12.35            12.35
is_peak_hour              8.21             8.21
is_weekend                5.12             5.12
rolling_avg_3h            0.48             0.48
temperature              -0.31             0.31
price_per_pizza          -0.18             0.18
hour_sin                  2.15             2.15
month_sin                 1.32             1.32
hour_cos                 -0.89             0.89
month_cos                 0.52             0.52
```

### What Each Coefficient Means

**Positive Coefficients (increase sales):**

| Feature | Coefficient | Interpretation |
|---------|-------------|----------------|
| **is_friday_evening** | +12.35 | Friday evenings boost sales by ~12 pizzas |
| **is_peak_hour** | +8.21 | Peak hours (lunch/dinner) add ~8 pizzas |
| **is_weekend** | +5.12 | Weekends increase sales by ~5 pizzas |
| **rolling_avg_3h** | +0.48 | Recent trends matter (busy begets busy) |

**Negative Coefficients (decrease sales):**

| Feature | Coefficient | Interpretation |
|---------|-------------|----------------|
| **temperature** | -0.31 | Each degree warmer = 0.31 fewer pizzas |
| **price_per_pizza** | -0.18 | Higher prices slightly reduce volume |

**Cyclical Features:**

| Feature | Coefficient | Interpretation |
|---------|-------------|----------------|
| **hour_sin/cos** | +2.15, -0.89 | Time of day effect (peaks at dinner) |
| **month_sin/cos** | +1.32, +0.52 | Seasonal variation (winter > summer) |

### Visualize Feature Importance

```python
# Plot coefficients
plt.figure(figsize=(10, 6))
coef_df_sorted = coef_df.sort_values('coefficient')

plt.barh(range(len(coef_df_sorted)), coef_df_sorted['coefficient'])
plt.yticks(range(len(coef_df_sorted)), coef_df_sorted['feature'])
plt.xlabel('Coefficient Value')
plt.title('Feature Importance (Linear Regression Coefficients)')
plt.axvline(0, color='red', linestyle='--', linewidth=1)
plt.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()
```

### Business Insights

**Actionable discoveries:**

1. **Friday evenings are golden** (+12.35 pizzas)
   - Action: Extra staff, more ingredients, marketing push

2. **Weather matters** (-0.31 per degree)
   - Action: Check forecast, adjust prep accordingly
   - On a 30¬∞C day, expect ~9 fewer pizzas than 0¬∞C

3. **Peak hours dominate** (+8.21 pizzas)
   - Action: Focus resources on 12-13h and 18-20h

4. **Momentum matters** (+0.48 from rolling_avg)
   - Action: If it's busy, stay ready - it'll stay busy!

---

## Evaluating Model Performance - Is It Good Enough? üìä

We have predictions. But are they good?

### Key Metrics

```python
# Calculate metrics on test set
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("üìä Model Performance on Test Set:")
print("=" * 50)
print(f"MAE (Mean Absolute Error):  {mae:.2f} pizzas")
print(f"MSE (Mean Squared Error):   {mse:.2f}")
print(f"RMSE (Root Mean Squared Error): {rmse:.2f} pizzas")
print(f"R¬≤ (R-squared):            {r2:.3f}")
print("=" * 50)

# Also check training performance
y_train_pred = model.predict(X_train)
train_r2 = r2_score(y_train, y_train_pred)
print(f"\nüìà Training R¬≤: {train_r2:.3f}")
print(f"üìâ Test R¬≤:     {r2:.3f}")
print(f"Difference:    {train_r2 - r2:.3f}")
```

**Output:**
```
üìä Model Performance on Test Set:
==================================================
MAE (Mean Absolute Error):  2.15 pizzas
MSE (Mean Squared Error):   7.84
RMSE (Root Mean Squared Error): 2.80 pizzas
R¬≤ (R-squared):            0.721
==================================================

üìà Training R¬≤: 0.735
üìâ Test R¬≤:     0.721
Difference:    0.014
```

### What Do These Numbers Mean?

**MAE (Mean Absolute Error): 2.15 pizzas**
- On average, predictions are off by ¬±2.15 pizzas
- Easy to understand: "We're typically 2 pizzas off"
- For a business selling 20-40 pizzas, that's good!

**RMSE (Root Mean Squared Error): 2.80 pizzas**
- Similar to MAE but penalizes big errors more
- Higher than MAE means some predictions are way off
- Still acceptable for planning

**R¬≤ (R-squared): 0.721**
- Model explains 72.1% of variance in pizza sales
- Remaining 27.9% is unexplained (weather events, promotions, randomness)
- **0.7+ is generally good for real-world data!**

**Training vs Test R¬≤: Very similar (0.735 vs 0.721)**
- Small difference = no overfitting!
- Model generalizes well to new data
- Thanks to proper splitting (Article 4!)

### Visualize: Actual vs Predicted

```python
# Scatter plot: Actual vs Predicted
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.5)

# Perfect prediction line
min_val = min(y_test.min(), y_pred.min())
max_val = max(y_test.max(), y_pred.max())
plt.plot([min_val, max_val], [min_val, max_val],
         'r--', linewidth=2, label='Perfect prediction')

plt.xlabel('Actual Pizzas Sold')
plt.ylabel('Predicted Pizzas Sold')
plt.title('Actual vs Predicted: Test Set')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

**Good predictions** cluster around the red line.

### Residual Analysis

Residuals = Actual - Predicted (the errors)

```python
# Calculate residuals
residuals = y_test - y_pred

# Create residual plots
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: Residuals vs Predicted
axes[0].scatter(y_pred, residuals, alpha=0.5)
axes[0].axhline(0, color='red', linestyle='--', linewidth=2)
axes[0].set_xlabel('Predicted Values')
axes[0].set_ylabel('Residuals (Actual - Predicted)')
axes[0].set_title('Residual Plot')
axes[0].grid(True, alpha=0.3)

# Plot 2: Histogram of residuals
axes[1].hist(residuals, bins=30, edgecolor='black')
axes[1].axvline(0, color='red', linestyle='--', linewidth=2)
axes[1].set_xlabel('Residual Value (pizzas)')
axes[1].set_ylabel('Frequency')
axes[1].set_title('Distribution of Residuals')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"Residual mean: {residuals.mean():.3f} (should be ~0)")
print(f"Residual std:  {residuals.std():.2f} pizzas")
```

**What to look for:**
- **Residual plot:** Should scatter randomly around 0 (no pattern)
- **Histogram:** Should look roughly normal (bell curve)
- **Mean near 0:** No systematic bias
- If you see patterns ‚Üí model is missing something!

---

## When Linear Regression Works (and When It Doesn't) ‚ö†Ô∏è

### Perfect Use Cases ‚úÖ

**Use linear regression when:**

1. **Target is continuous**
   - Predicting numbers: sales, temperature, price, time
   - NOT categories: spam/ham, cat/dog

2. **Relationships are roughly linear**
   - Features and target have straight-line relationship
   - "As X increases, Y increases/decreases proportionally"

3. **You need interpretability**
   - Business wants to understand WHY
   - "What drives sales?" ‚Üí Point to coefficients
   - Explainable AI / regulatory requirements

4. **Fast predictions matter**
   - Millions of predictions per second
   - Just multiply and add!

5. **You have enough data**
   - Rule of thumb: 10√ó samples per feature
   - 10 features ‚Üí need 100+ samples (we have thousands!)

### Assumptions (Simplified)

Linear regression assumes:

1. **Linearity:** Relationship between X and Y is linear
   - Check: Plot features vs target, look for straight lines

2. **Independence:** Observations don't affect each other
   - Our data: Each order is independent ‚úÖ

3. **Homoscedasticity:** Error variance is constant
   - Check: Residual plot should have even spread
   - No "fan" pattern

4. **Normality of residuals:** Errors are normally distributed
   - Check: Histogram of residuals looks bell-shaped

**Don't stress if assumptions aren't perfect!** Real-world data rarely is. As long as they're "close enough," linear regression still works.

### When NOT to Use ‚ùå

**Don't use linear regression when:**

1. **Non-linear relationships**
   ```
   Example: Exponential growth (viral spread, compound interest)
   Solution: Transform features (log), or use different model
   ```

2. **Classification problems**
   ```
   Example: Predict if order is large/small (category)
   Solution: Use Logistic Regression or Decision Trees
   ```

3. **Complex interactions**
   ```
   Example: Effect of temperature depends on day of week
   Solution: Add interaction features (temp √ó is_weekend)
   ```

4. **Lots of irrelevant features**
   ```
   Problem: Model treats all features equally
   Solution: Feature selection or Regularization (Ridge/Lasso)
   ```

5. **Extreme outliers**
   ```
   Problem: One catering order of 500 pizzas skews everything
   Solution: Robust regression or outlier removal
   ```

---

## Improving Your Model üöÄ

Your first model works! But can we make it better?

### 1. Feature Selection

Not all features are equally useful:

```python
# Drop weak features (small coefficients)
strong_features = coef_df[coef_df['abs_coefficient'] > 1.0]['feature'].tolist()

print(f"Strong features: {strong_features}")

# Retrain with only strong features
model_simplified = LinearRegression()
model_simplified.fit(X_train[strong_features], y_train)

# Compare performance
r2_simple = model_simplified.score(X_test[strong_features], y_test)
print(f"Original R¬≤: {r2:.3f}")
print(f"Simplified R¬≤: {r2_simple:.3f}")
```

### 2. Polynomial Features (Teaser)

What if temperature has a non-linear effect?

```python
from sklearn.preprocessing import PolynomialFeatures

# Add temperature¬≤
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X_train[['temperature']])

# Now model can learn: pizzas = a + b√ótemp + c√ótemp¬≤
```

**More on this in Article 8!**

### 3. Regularization (Teaser)

Prevent overfitting with Ridge or Lasso:

```python
from sklearn.linear_model import Ridge

# Ridge regression (L2 regularization)
model_ridge = Ridge(alpha=1.0)
model_ridge.fit(X_train, y_train)
```

**More on this in Article 8!**

### 4. More Data

Often the best improvement:
- More historical data ‚Üí better patterns
- More features (weather API, holidays) ‚Üí more context
- Better labeling ‚Üí cleaner signal

---

## Summary - You Did It! üéâ

### What You Learned

**1. Linear Regression Intuition**
- Finding the line of best fit
- From single variable to multiple features
- Coefficients tell you feature impact

**2. Training a Model**
- Load prepared data (Articles 1-4 paid off!)
- One line: `model.fit(X_train, y_train)`
- Scikit-learn does the hard math

**3. Making Predictions**
- `model.predict(new_data)`
- Real business scenarios
- "What if" analysis

**4. Interpreting Results**
- Coefficients = feature importance
- Friday evenings: +12 pizzas!
- Temperature: -0.31 pizzas per degree

**5. Evaluating Performance**
- MAE: Average error (2.15 pizzas)
- R¬≤: Variance explained (72%)
- Residual plots: Check assumptions

**6. When to Use It**
- Continuous targets
- Linear relationships
- Need interpretability
- Fast predictions

### The Journey Continues

```
‚úÖ Article 1: Data Quality
‚úÖ Article 2: EDA
‚úÖ Article 3: Feature Engineering
‚úÖ Article 4: Train/Test Split
‚úÖ Article 5: Linear Regression ‚Üê YOU ARE HERE!
    ‚Üì
üìç Next: Article 6: Model Evaluation Metrics
```

### What's Next?

In **Article 6: Model Evaluation Metrics - Measuring Success**, you'll learn:
- Is 72% R¬≤ good? How do you know?
- Classification metrics (precision, recall, F1)
- Confusion matrices explained visually
- ROC curves and AUC
- Choosing the right metric for your problem
- When accuracy lies to you!

You now have a working ML model! Next, we'll learn how to evaluate models more deeply and compare different algorithms.

---

## Practice Exercises üí™

**Exercise 1: Simple Model**
```python
# Train a model with only 3 features: temperature, is_friday_evening, is_weekend
# Compare R¬≤ to the full model
# Which features matter most?
```

**Exercise 2: Predict Scenarios**
```python
# Predict pizzas for:
# 1. Saturday noon, 25¬∞C
# 2. Monday evening (7 PM), 12¬∞C
# 3. Thursday lunch (1 PM), 18¬∞C
```

**Exercise 3: Coefficient Interpretation**
```python
# If temperature drops from 25¬∞C to 15¬∞C (10 degrees colder),
# how many more pizzas should we expect?
# Hint: Use the temperature coefficient
```

**Exercise 4: Metrics**
```python
# Calculate MAE and RMSE on the validation set
# Are they similar to test set?
# What does this tell you about generalization?
```

**Exercise 5: Feature Engineering**
```python
# Create a new feature: is_cold_friday_evening
# (Combines is_friday_evening AND temperature < 10)
# Does it improve the model?
```

---

## Further Reading üìö

**Books:**
- "Introduction to Statistical Learning" by James et al. (Chapter 3)
- "Hands-On Machine Learning" by Aur√©lien G√©ron (Chapter 4)

**Documentation:**
- Scikit-learn LinearRegression: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html
- Regression metrics: https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics

**Kaggle:**
- House Price Prediction (classic regression problem)
- Linear Regression tutorials

---

**Congratulations!** üéä You've trained your first machine learning model and made real predictions. This is a huge milestone in your ML journey!

Remember: **Start simple (linear regression), understand it deeply, then move to complex models.** The principles you learned here apply to all ML algorithms.

Happy predicting! üöÄ‚ú®
