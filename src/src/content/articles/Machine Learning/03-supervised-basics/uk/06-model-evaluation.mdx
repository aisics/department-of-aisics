---
title: "Метрики оцінки моделі - Вимірювання успіху"
description: "Навчіться оцінювати моделі машинного навчання за допомогою ключових метрик: від MAE та RMSE для регресії до точності, повноти та AUC для класифікації"
author: "Кафедра ШІзики"
date: 2025-10-28
readingTime: 35
tags: ["оцінка-моделі", "метрики", "регресія", "класифікація", "навчання-з-учителем"]
featured: false
difficulty: "intermediate"
category: "Machine Learning"
subcategory: "03-supervised-basics"
prerequisites: ["05-linear-regression", "04-train-test-split"]
relatedArticles: ["05-linear-regression", "07-decision-trees"]
---

import MetricCalculator from '../../../../../components/MetricCalculator.astro';
import ConfusionMatrixExplorer from '../../../../../components/ConfusionMatrixExplorer.astro';
import ROCCurveAnimator from '../../../../../components/ROCCurveAnimator.astro';

# Метрики оцінки моделі - Вимірювання успіху

## Вступ

У [Статті 5](/article/machine-learning-uk-05-linear-regression) ми побудували модель лінійної регресії для прогнозування продажу піци на основі температури. Ми отримали модель з такими параметрами:

```
Продаж = 35 - 0.5 × Температура
```

Але як добре працює ця модель? Ми згадали R² (коефіцієнт детермінації), але це лише одна метрика. Насправді існують десятки способів оцінки моделей машинного навчання, і вибір правильних метрик є критично важливим для успіху проекту.

У цій статті ми:
- Вивчимо всі основні метрики для задач регресії
- Зрозуміємо метрики класифікації та матрицю плутанини
- Навчимося інтерпретувати ROC-криві та AUC
- Дізнаємося, як вибрати правильні метрики для бізнес-цілей

## Чому метрики оцінки важливі?

Уявіте, що ви побудували дві моделі для прогнозування продажу піци:

**Модель A:**
- Прогнозує 25 піц, коли фактично продано 24
- Прогнозує 18 піц, коли фактично продано 20
- Прогнозує 15 піц, коли фактично продано 15

**Модель B:**
- Прогнозує 30 піц, коли фактично продано 24
- Прогнозує 22 піц, коли фактично продано 20
- Прогнозує 10 піц, коли фактично продано 15

Яка модель краща? На перший погляд важко сказати. Метрики оцінки дають нам числові показники, які дозволяють:

1. **Об'єктивно порівнювати моделі** - замість суб'єктивних оцінок
2. **Відстежувати прогрес** - під час навчання та налаштування
3. **Виявляти проблеми** - де модель працює погано
4. **Комунікувати з бізнесом** - пояснювати результати зрозумілими термінами

## Метрики регресії - Оцінка безперервних прогнозів

Повернемося до нашої моделі прогнозування продажу піци. Ось наші фактичні дані з Статті 5:

| Температура (°C) | Фактичний продаж | Прогноз моделі |
|------------------|------------------|----------------|
| 5                | 32               | 32.5           |
| 10               | 28               | 30.0           |
| 15               | 25               | 27.5           |
| 20               | 21               | 25.0           |
| 25               | 18               | 22.5           |
| 30               | 15               | 20.0           |

Подивимося на чотири найважливіші метрики регресії.

### 1. MAE (Mean Absolute Error) - Середня абсолютна помилка

MAE - це найпростіша метрика: середнє абсолютних різниць між прогнозами та фактичними значеннями.

**Формула:**

$$
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_{\text{actual}} - y_{\text{predicted}}|
$$

**Приклад розрахунку для наших даних:**

$$
\begin{align*}
|32 - 32.5| &= 0.5 \\
|28 - 30.0| &= 2.0 \\
|25 - 27.5| &= 2.5 \\
|21 - 25.0| &= 4.0 \\
|18 - 22.5| &= 4.5 \\
|15 - 20.0| &= 5.0 \\
\\
\text{MAE} &= \frac{0.5 + 2.0 + 2.5 + 4.0 + 4.5 + 5.0}{6} = \frac{18.5}{6} = 3.08
\end{align*}
$$

**Інтерпретація:** В середньому наші прогнози відрізняються від фактичних значень на 3.08 піци.

**Переваги:**
- ✅ Легко інтерпретувати (у тих самих одиницях, що й цільова змінна)
- ✅ Стійка до викидів (великі помилки не мають надмірного впливу)
- ✅ Лінійна шкала (помилка в 4 піци вдвічі гірше, ніж помилка в 2 піци)

**Недоліки:**
- ❌ Не штрафує великі помилки сильніше за малі
- ❌ Не показує напрямок помилок (завищення чи заниження)

### 2. MSE (Mean Squared Error) - Середня квадратична помилка

MSE підносить помилки до квадрату перед усередненням, що робить великі помилки більш "дорогими".

**Формула:**

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_{\text{actual}} - y_{\text{predicted}})^2
$$

**Приклад розрахунку:**

$$
\begin{align*}
(32 - 32.5)^2 &= 0.25 \\
(28 - 30.0)^2 &= 4.00 \\
(25 - 27.5)^2 &= 6.25 \\
(21 - 25.0)^2 &= 16.00 \\
(18 - 22.5)^2 &= 20.25 \\
(15 - 20.0)^2 &= 25.00 \\
\\
\text{MSE} &= \frac{0.25 + 4.00 + 6.25 + 16.00 + 20.25 + 25.00}{6} = \frac{71.75}{6} = 11.96
\end{align*}
$$

**Інтерпретація:** MSE = 11.96 піци² (квадратні одиниці!)

**Переваги:**
- ✅ Сильно штрафує великі помилки (квадратична функція)
- ✅ Диференційована скрізь (зручно для оптимізації)
- ✅ Часто використовується як функція втрат при навчанні

**Недоліки:**
- ❌ Важко інтерпретувати (квадратні одиниці)
- ❌ Дуже чутлива до викидів

### 3. RMSE (Root Mean Squared Error) - Корінь із середньої квадратичної помилки

RMSE - це просто квадратний корінь із MSE, що повертає метрику до початкових одиниць.

**Формула:**

$$
\text{RMSE} = \sqrt{\text{MSE}} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_{\text{actual}} - y_{\text{predicted}})^2}
$$

**Приклад розрахунку:**

$$
\text{RMSE} = \sqrt{11.96} = 3.46 \text{ піци}
$$

**Інтерпретація:** Типове відхилення наших прогнозів становить близько 3.46 піци.

**Переваги:**
- ✅ У тих самих одиницях, що й цільова змінна
- ✅ Штрафує великі помилки (через квадрат всередині)
- ✅ Найпопулярніша метрика в індустрії

**Недоліки:**
- ❌ Важче інтерпретувати, ніж MAE
- ❌ Чутлива до викидів

**Порівняння MAE vs RMSE:**
- Якщо RMSE ≈ MAE → помилки рівномірно розподілені
- Якщо RMSE >> MAE → є кілька великих викидів
- У нашому випадку: RMSE (3.46) > MAE (3.08) → помилки зростають з температурою

### 4. R² (R-squared) - Коефіцієнт детермінації

R² показує, яку частку варіації в даних пояснює наша модель.

**Формула:**

$$
R^2 = 1 - \frac{SS_{\text{res}}}{SS_{\text{tot}}}
$$

де:
- $SS_{\text{res}} = \sum (y_{\text{actual}} - y_{\text{predicted}})^2$ (сума квадратів залишків)
- $SS_{\text{tot}} = \sum (y_{\text{actual}} - \bar{y})^2$ (загальна сума квадратів)

**Приклад розрахунку:**

Спочатку знайдемо середнє фактичних значень:

$$
\bar{y} = \frac{32 + 28 + 25 + 21 + 18 + 15}{6} = \frac{139}{6} = 23.17
$$

Тепер $SS_{\text{res}}$ (вже розрахували для MSE):

$$
SS_{\text{res}} = 71.75
$$

Тепер $SS_{\text{tot}}$:

$$
\begin{align*}
(32 - 23.17)^2 &= 77.97 \\
(28 - 23.17)^2 &= 23.33 \\
(25 - 23.17)^2 &= 3.35 \\
(21 - 23.17)^2 &= 4.71 \\
(18 - 23.17)^2 &= 26.73 \\
(15 - 23.17)^2 &= 66.73 \\
\\
SS_{\text{tot}} &= 77.97 + 23.33 + 3.35 + 4.71 + 26.73 + 66.73 = 202.82
\end{align*}
$$

Отже:

$$
R^2 = 1 - \frac{71.75}{202.82} = 1 - 0.354 = 0.646 \approx 0.65
$$

**Інтерпретація:** Наша модель пояснює 65% варіації в продажах піци. Решта 35% залежить від інших факторів (день тижня, погода, конкуренти тощо).

**Переваги:**
- ✅ Безрозмірна (від 0 до 1)
- ✅ Легко інтерпретувати як відсоток
- ✅ Дозволяє порівнювати моделі на різних датасетах

**Недоліки:**
- ❌ Завжди зростає з додаванням нових ознак (навіть непотрібних)
- ❌ Може бути негативною для дуже поганих моделей
- ❌ Не показує абсолютний розмір помилок

### Інтерактивний калькулятор метрик

Спробуйте змінити прогнози і подивіться, як змінюються метрики:

<MetricCalculator
  title="Калькулятор метрик регресії"
  initialData={JSON.stringify([
    {actual: 32, predicted: 32.5},
    {actual: 28, predicted: 30.0},
    {actual: 25, predicted: 27.5},
    {actual: 21, predicted: 25.0},
    {actual: 18, predicted: 22.5},
    {actual: 15, predicted: 20.0}
  ])}
/>

**Експерименти для спробування:**
1. Зробіть усі прогнози рівними середньому значенню (23.17) - R² стане 0
2. Зробіть прогнози ідеальними (рівними фактичним) - усі метрики стануть 0, R² = 1
3. Додайте один великий викид - подивіться, як RMSE зростає швидше за MAE

## Метрики класифікації - Оцінка категорійних прогнозів

Тепер перейдемо від регресії (прогнозування чисел) до класифікації (прогнозування категорій).

### Приклад задачі: Діагностика захворювання

Уявімо, що ми будуємо модель для діагностики рідкісного захворювання:
- **Positive (P):** Пацієнт хворий
- **Negative (N):** Пацієнт здоровий

Модель може робити чотири типи прогнозів:

1. **True Positive (TP):** Модель правильно передбачила хворобу ✅
2. **True Negative (TN):** Модель правильно передбачила здоров'я ✅
3. **False Positive (FP):** Модель помилково передбачила хворобу (хибна тривога) ❌
4. **False Negative (FN):** Модель пропустила хворобу (найнебезпечніше!) ❌❌

### Матриця плутанини (Confusion Matrix)

Матриця плутанини - це таблиця, яка показує всі чотири типи прогнозів:

|                    |  **Фактично Positive** | **Фактично Negative** |
|--------------------|:----------------------:|:---------------------:|
| **Прогноз Positive** |          TP            |          FP           |
| **Прогноз Negative** |          FN            |          TN           |

**Приклад:** Модель перевірила 100 пацієнтів:
- 10 насправді хворі, 90 здорові
- Модель правильно знайшла 8 хворих (TP = 8)
- Модель помилково діагностувала 5 здорових як хворих (FP = 5)
- Модель пропустила 2 хворих (FN = 2)
- Модель правильно визначила 85 здорових (TN = 85)

|                      | **Фактично Хворий** | **Фактично Здоровий** | **Всього прогнозів** |
|----------------------|:-------------------:|:---------------------:|:--------------------:|
| **Прогноз Хворий**   |          8          |           5           |          13          |
| **Прогноз Здоровий** |          2          |          85           |          87          |
| **Всього фактично**  |         10          |          90           |         100          |

### Основні метрики класифікації

#### 1. Accuracy (Точність) - Загальна правильність

**Формула:**

$$
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
$$

**Приклад:**

$$
\text{Accuracy} = \frac{8 + 85}{8 + 85 + 5 + 2} = \frac{93}{100} = 0.93 = 93\%
$$

**Інтерпретація:** Модель правильна в 93% випадків.

**Проблема з Accuracy:**

Уявімо іншу модель, яка завжди каже "здоровий":
- TP = 0 (не знайшла жодного хворого)
- TN = 90 (правильно визначила всіх здорових)
- FP = 0 (не було хибних тривог)
- FN = 10 (пропустила всіх хворих!)

$$
\text{Accuracy} = \frac{0 + 90}{100} = 90\%
$$

Ця "ледача" модель має Accuracy 90%, але вона абсолютно марна! Вона пропускає всіх хворих пацієнтів.

**Висновок:** Accuracy погано працює на незбалансованих датасетах (коли один клас набагато частіший за інший).

#### 2. Precision (Точність позитивних прогнозів)

**Формула:**

$$
\text{Precision} = \frac{TP}{TP + FP}
$$

**Питання, на яке відповідає Precision:**
"З усіх пацієнтів, яких модель діагностувала як хворих, скільки насправді хворі?"

**Приклад:**

$$
\text{Precision} = \frac{8}{8 + 5} = \frac{8}{13} = 0.615 = 61.5\%
$$

**Інтерпретація:** З 13 пацієнтів, яких модель діагностувала як хворих, лише 8 (61.5%) насправді хворі. Решта 5 - хибні тривоги.

**Коли Precision важлива:**
- Спам-фільтри (не хочемо втратити важливі листи)
- Рекомендаційні системи (не хочемо дратувати користувачів поганими рекомендаціями)
- Вартісні дії (коли помилковий позитив дорого коштує)

#### 3. Recall (Повнота / Чутливість)

**Формула:**

$$
\text{Recall} = \frac{TP}{TP + FN}
$$

**Питання, на яке відповідає Recall:**
"З усіх хворих пацієнтів, скільки модель знайшла?"

**Приклад:**

$$
\text{Recall} = \frac{8}{8 + 2} = \frac{8}{10} = 0.8 = 80\%
$$

**Інтерпретація:** З 10 хворих пацієнтів модель знайшла 8 (80%). Вона пропустила 2 пацієнтів.

**Коли Recall важлива:**
- Медична діагностика (не хочемо пропустити хворих)
- Виявлення шахрайства (краще перестрахуватися)
- Пошук (краще показати зайве, ніж пропустити релевантне)

#### 4. F1-Score - Гармонічне середнє Precision та Recall

Часто ми хочемо баланс між Precision та Recall. F1-score об'єднує обидві метрики.

**Формула:**

$$
F1 = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
$$

**Приклад:**

$$
\begin{align*}
F1 &= \frac{2 \times (0.615 \times 0.8)}{0.615 + 0.8} \\
&= \frac{2 \times 0.492}{1.415} \\
&= \frac{0.984}{1.415} \\
&= 0.695 = 69.5\%
\end{align*}
$$

**Чому гармонічне середнє?**

Порівняйте з арифметичним середнім:

$$
\text{Арифметичне середнє} = \frac{\text{Precision} + \text{Recall}}{2} = \frac{61.5\% + 80\%}{2} = 70.75\%
$$

Гармонічне середнє (F1 = 69.5%) завжди ближче до нижчого значення. Це означає, що F1 "штрафує" нерівномірні метрики.

**Приклад:**
- Precision = 100%, Recall = 10% → Арифметичне = 55%, F1 = 18.2%
- Модель з такими метриками майже марна, і F1 це відображає!

### Інтерактивна матриця плутанини

Змінюйте значення TP, TN, FP, FN і дивіться, як змінюються метрики:

<ConfusionMatrixExplorer
  title="Інтерактивна матриця плутанини"
  initialTP={8}
  initialTN={85}
  initialFP={5}
  initialFN={2}
/>

**Експерименти для спробування:**
1. Зробіть FN = 0 (максимальний Recall) - подивіться, що станеться з Precision
2. Зробіть FP = 0 (максимальний Precision) - подивіться, що станеться з Recall
3. Збалансуйте FP та FN для максимізації F1-score

## ROC-крива та AUC

### Проблема порогу класифікації

Більшість моделей класифікації насправді виводять імовірності:

```python
# Модель діагностики
пацієнт_1: 0.95 (95% ймовірність хвороби)
пацієнт_2: 0.65 (65% ймовірність хвороби)
пацієнт_3: 0.20 (20% ймовірність хвороби)
```

Для прийняття рішення ми встановлюємо **поріг** (threshold):
- Якщо P(хвороба) ≥ поріг → Діагностуємо "хворий"
- Якщо P(хвороба) < поріг → Діагностуємо "здоровий"

**Звичайно поріг = 0.5, але це не завжди оптимально!**

### Компроміс між Recall та Precision

**Низький поріг (0.2):**
- ✅ Високий Recall (знайдемо майже всіх хворих)
- ❌ Низький Precision (багато хибних тривог)
- Використання: Критичні ситуації (рак, терористичні загрози)

**Високий поріг (0.8):**
- ✅ Високий Precision (мало хибних тривог)
- ❌ Низький Recall (пропустимо багато хворих)
- Використання: Дорогі дії (хірургія, арешти)

**Середній поріг (0.5):**
- ⚖️ Баланс між Recall та Precision
- Використання: Більшість загальних випадків

### ROC-крива (Receiver Operating Characteristic)

ROC-крива показує компроміс між двома метриками:
- **True Positive Rate (TPR)** = Recall = TP / (TP + FN)
- **False Positive Rate (FPR)** = FP / (FP + TN)

Ми будуємо криву, змінюючи поріг від 0 до 1:

```
Поріг = 0.0 → TPR = 100%, FPR = 100% (всі "хворі")
Поріг = 0.2 → TPR = 95%, FPR = 30%
Поріг = 0.5 → TPR = 80%, FPR = 5%
Поріг = 0.8 → TPR = 50%, FPR = 1%
Поріг = 1.0 → TPR = 0%, FPR = 0% (всі "здорові")
```

**Ідеальна крива:** TPR = 100%, FPR = 0% для всіх порогів
**Випадкова модель:** Пряма лінія (TPR = FPR)
**Реальні моделі:** Щось посередині

### AUC (Area Under Curve)

AUC - це площа під ROC-кривою:
- **AUC = 1.0:** Ідеальна модель
- **AUC = 0.5:** Випадкове вгадування
- **AUC < 0.5:** Модель гірша за випадкову (щось пішло не так!)
- **AUC > 0.7:** Прийнятна модель
- **AUC > 0.8:** Хороша модель
- **AUC > 0.9:** Відмінна модель

**Переваги AUC:**
- ✅ Не залежить від порогу класифікації
- ✅ Працює на незбалансованих датасетах
- ✅ Легко інтерпретувати (ймовірність правильного ранжування)

**Інтерпретація AUC:**
AUC = 0.85 означає: "Якщо ви виберете випадкового хворого пацієнта та випадкового здорового пацієнта, модель дасть вищий score хворому в 85% випадків".

### Інтерактивна ROC-крива

Змінюйте поріг і дивіться, як рухається точка на ROC-кривій:

<ROCCurveAnimator
  title="ROC-крива та вибір порогу"
  modelData={JSON.stringify([
    {actual: 1, probability: 0.95},
    {actual: 1, probability: 0.85},
    {actual: 1, probability: 0.75},
    {actual: 1, probability: 0.65},
    {actual: 1, probability: 0.55},
    {actual: 0, probability: 0.45},
    {actual: 0, probability: 0.35},
    {actual: 0, probability: 0.25},
    {actual: 0, probability: 0.15},
    {actual: 0, probability: 0.05}
  ])}
/>

## Вибір правильних метрик

### Матриця вибору метрик

| Тип задачі | Збалансованість | Критичність помилок | Рекомендовані метрики |
|------------|-----------------|---------------------|----------------------|
| Регресія | - | Однакова | MAE, R² |
| Регресія | - | Великі помилки критичні | RMSE, MSE |
| Регресія | - | Бізнес-метрика | Custom (наприклад, прибуток) |
| Класифікація | Збалансована | Однакова | Accuracy, F1-score |
| Класифікація | Незбалансована | Однакова | F1-score, AUC |
| Класифікація | Незбалансована | FN критичні (рак) | Recall, AUC |
| Класифікація | Незбалансована | FP критичні (спам) | Precision, AUC |
| Класифікація | Незбалансована | Потрібен баланс | F1-score, AUC |

### Приклади з реального життя

#### 1. Спам-фільтр
**Метрики:** Precision (головна), Recall (додаткова), F1

**Чому:** Ми не хочемо, щоб важливі листи потрапили в спам (FP дуже дорогі). Краще пропустити трохи спаму, ніж втратити важливий лист.

#### 2. Діагностика раку
**Метрики:** Recall (головна), AUC, F1

**Чому:** Пропустити рак (FN) смертельно небезпечно. Хибна тривога (FP) призведе до додаткових тестів, але це прийнятно.

#### 3. Прогнозування продажів
**Метрики:** MAE, RMSE, бізнес-метрика (прибуток)

**Чому:** Нам потрібна абсолютна точність для планування запасів. RMSE штрафує великі помилки (дефіцит або надлишок інвентарю).

#### 4. Рекомендаційна система
**Метрики:** Precision@K, Recall@K, AUC

**Чому:** Користувачі бачать лише топ-K рекомендацій. Важливо, щоб ці рекомендації були релевантні (Precision@K).

#### 5. Виявлення шахрайства
**Метрики:** Recall (головна), Precision, F1, AUC

**Чому:** Пропущена шахрайська транзакція (FN) коштує багато грошей. Блокування легітимної транзакції (FP) дратує клієнтів, але краще перестрахуватися.

## Підводні камені та best practices

### 1. Не покладайтеся на одну метрику

❌ **Погано:**
```python
model_a_accuracy = 0.95
model_b_accuracy = 0.92
# Вибираємо модель A
```

✅ **Добре:**
```python
model_a = {"accuracy": 0.95, "precision": 0.60, "recall": 0.40, "f1": 0.48}
model_b = {"accuracy": 0.92, "precision": 0.85, "recall": 0.80, "f1": 0.82}
# Вибираємо модель B (набагато кращий F1!)
```

### 2. Враховуйте бізнес-контекст

Метрики повинні відображати реальну вартість помилок:

```python
# Приклад: Кредитний скоринг
# FN (не видали кредит хорошому клієнту): втрачений прибуток = $100
# FP (видали кредит поганому клієнту): втрата = $5000

# Стандартна метрика
f1_score = 2 * (precision * recall) / (precision + recall)

# Бізнес-метрика з ваговими коефіцієнтами
business_score = -5000 * FP - 100 * FN + 500 * TP
```

### 3. Використовуйте валідаційний набір

❌ **Погано:**
```python
# Навчаємо та оцінюємо на тих самих даних
model.fit(X_train, y_train)
score = model.score(X_train, y_train)  # Занадто оптимістично!
```

✅ **Добре:**
```python
# Розділяємо дані
X_train, X_val, X_test = split_data(X)

# Навчаємо на train, налаштовуємо на validation, тестуємо на test
model.fit(X_train, y_train)
val_score = model.score(X_val, y_val)  # Використовуємо для вибору моделі
test_score = model.score(X_test, y_test)  # Остаточна оцінка
```

### 4. Стежте за змінами в розподілі даних

Метрики можуть погіршуватися з часом через **data drift**:

```python
# Модель навчена в 2020 році
model_2020_auc = 0.85

# Та сама модель в 2023 році
model_2023_auc = 0.72  # Погіршення!

# Причини:
# - Змінилася поведінка користувачів
# - Змінилися економічні умови
# - Конкуренти змінили стратегії
```

**Рішення:** Регулярно перенавчайте моделі та моніторьте метрики в production.

### 5. Не оптимізуйте метрику наосліп

```python
# Оптимізація Precision до абсурду
def always_predict_positive_with_high_confidence():
    return [1] * 1  # Predict positive for only 1 sample
    # Precision = 100% (if that 1 sample is TP)
    # But Recall = 1% (missed 99% of actual positives!)
```

## Практичний приклад: Повна оцінка моделі

Повернемося до нашої моделі прогнозування продажу піци та зробимо повну оцінку:

```python
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Наші дані з Article 5
temperatures = np.array([5, 10, 15, 20, 25, 30])
actual_sales = np.array([32, 28, 25, 21, 18, 15])

# Модель: Sales = 35 - 0.5 * Temperature
predicted_sales = 35 - 0.5 * temperatures

# Розрахунок метрик
mae = mean_absolute_error(actual_sales, predicted_sales)
mse = mean_squared_error(actual_sales, predicted_sales)
rmse = np.sqrt(mse)
r2 = r2_score(actual_sales, predicted_sales)

print(f"MAE: {mae:.2f} піц")
print(f"MSE: {mse:.2f} піц²")
print(f"RMSE: {rmse:.2f} піц")
print(f"R²: {r2:.3f} ({r2*100:.1f}%)")

# Аналіз залишків
residuals = actual_sales - predicted_sales
print(f"\nЗалишки: {residuals}")
print(f"Середня помилка: {np.mean(residuals):.2f}")
print(f"Std помилки: {np.std(residuals):.2f}")

# Перевірка припущень
print(f"\nПомилки зростають з температурою?")
print(f"Кореляція (temp, |residuals|): {np.corrcoef(temperatures, np.abs(residuals))[0,1]:.3f}")
```

**Вивід:**
```
MAE: 3.08 піц
MSE: 11.96 піц²
RMSE: 3.46 піц
R²: 0.646 (64.6%)

Залишки: [-0.5 -2.  -2.5 -4.  -4.5 -5. ]
Середня помилка: -3.08
Std помилки: 1.66

Помилки зростають з температурою?
Кореляція (temp, |residuals|): 0.989
```

**Інсайти:**
1. ✅ R² = 0.646 - модель пояснює 65% варіації (непогано для простої лінійної моделі)
2. ✅ MAE = 3.08 - в середньому помиляємося на 3 піци (прийнятно для бізнесу)
3. ⚠️ Залишки негативні та зростають - модель систематично завищує прогноз при високих температурах
4. ⚠️ Кореляція 0.989 - помилки не випадкові, є патерн (потрібна нелінійна модель?)

## Висновки та наступні кроки

### Ключові висновки

1. **Для регресії:**
   - MAE - для інтерпретації (у тих самих одиницях)
   - RMSE - для оптимізації (штрафує великі помилки)
   - R² - для розуміння пояснювальної сили моделі

2. **Для класифікації:**
   - Accuracy - якщо класи збалансовані
   - Precision - якщо FP дорогі (спам-фільтри)
   - Recall - якщо FN дорогі (медична діагностика)
   - F1-score - для балансу
   - AUC - для незбалансованих датасетів

3. **Завжди:**
   - Використовуйте кілька метрик
   - Враховуйте бізнес-контекст
   - Перевіряйте на окремому валідаційному наборі
   - Моніторьте в production

### Що далі?

У наступній статті ми вивчимо **Overfitting та Regularization** - як запобігти тому, щоб модель "запам'ятовувала" навчальні дані замість того, щоб вчитися справжнім патернам.

Ми дізнаємося:
- Що таке overfitting та underfitting
- Як виявити overfitting за допомогою кривих навчання
- Техніки regularization (L1, L2, Elastic Net)
- Cross-validation для надійної оцінки

**Підказка:** Пам'ятаєте кореляцію 0.989 у залишках нашої моделі продажу піци? Це може бути ознакою underfitting (модель занадто проста). У наступній статті ми навчимося це виправляти!

## Додаткові ресурси

1. **Sklearn Metrics Documentation:**
   https://scikit-learn.org/stable/modules/model_evaluation.html

2. **ROC Curve and AUC Explained:**
   Fawcett, T. (2006). "An introduction to ROC analysis"

3. **Choosing the Right Metric:**
   https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/

4. **Business Metrics in ML:**
   "Evaluating Machine Learning Models" by Alice Zheng
